{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyflyt\n!pip uninstall stable-baselines3 sb3_contrib -y\n!pip install stable-baselines3 sb3_contrib\n# !pip install stable-baselines3[extra]","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:25:56.101008Z","iopub.execute_input":"2024-05-18T22:25:56.101357Z","iopub.status.idle":"2024-05-18T22:26:30.583127Z","shell.execute_reply.started":"2024-05-18T22:25:56.101329Z","shell.execute_reply":"2024-05-18T22:26:30.582001Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyflyt\n  Downloading PyFlyt-0.21.0-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from pyflyt) (0.58.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pyflyt) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pyflyt) (1.11.4)\nRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (from pyflyt) (0.29.0)\nRequirement already satisfied: pettingzoo in /opt/conda/lib/python3.10/site-packages (from pyflyt) (1.24.0)\nCollecting pybullet (from pyflyt)\n  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from pyflyt) (6.0.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->pyflyt) (2.2.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium->pyflyt) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium->pyflyt) (0.0.4)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->pyflyt) (0.41.1)\nDownloading PyFlyt-0.21.0-py3-none-any.whl (198 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pybullet, pyflyt\nSuccessfully installed pybullet-3.2.6 pyflyt-0.21.0\nFound existing installation: stable-baselines3 2.1.0\nUninstalling stable-baselines3-2.1.0:\n  Successfully uninstalled stable-baselines3-2.1.0\n\u001b[33mWARNING: Skipping sb3_contrib as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mCollecting stable-baselines3\n  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\nCollecting sb3_contrib\n  Downloading sb3_contrib-2.3.0-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.1.2)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.1.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.7.5)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\nDownloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sb3_contrib-2.3.0-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: stable-baselines3, sb3_contrib\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires stable-baselines3==2.1.0, but you have stable-baselines3 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed sb3_contrib-2.3.0 stable-baselines3-2.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# !rm -r *","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:22:37.417729Z","iopub.execute_input":"2024-05-18T22:22:37.418520Z","iopub.status.idle":"2024-05-18T22:22:38.406525Z","shell.execute_reply.started":"2024-05-18T22:22:37.418478Z","shell.execute_reply":"2024-05-18T22:22:38.405422Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom sb3_contrib import TQC\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\nfrom stable_baselines3.common.callbacks import BaseCallback\n\nimport numpy as np\nimport time\nfrom PyFlyt.core.drones import Rocket\nimport PyFlyt.gym_envs\nimport torch.nn as nn\nimport os\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:31:26.718847Z","iopub.execute_input":"2024-05-18T22:31:26.719264Z","iopub.status.idle":"2024-05-18T22:31:42.947478Z","shell.execute_reply.started":"2024-05-18T22:31:26.719228Z","shell.execute_reply":"2024-05-18T22:31:42.946724Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-18 22:31:32.523025: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-18 22:31:32.523121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-18 22:31:32.664344: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\npybullet build time: Nov 28 2023 23:45:17\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to create a monitored environment\ndef make_env():\n    def _init():\n        env = gym.make(\"PyFlyt/Rocket-Landing-v1\")\n        return Monitor(env, \"./monitor_train_logs\")\n    return _init\n\nclass AggregateRewardLoggingCallback(BaseCallback):\n    def __init__(self, log_dir='custom_tensorboard/',verbose=0, check_freq=1000):\n        super(AggregateRewardLoggingCallback, self).__init__(verbose)\n        self.best_mean_reward = -np.inf\n        self.check_freq = check_freq\n        self.log_dir = log_dir\n        self.writer = SummaryWriter(log_dir=self.log_dir)\n\n    def _on_step(self):\n        if self.n_calls % self.check_freq == 0:\n            all_rewards = []\n            # Loop through all wrapped environments\n            for env in self.training_env.envs:\n                # Check and collect rewards from each environment\n                if hasattr(env, 'get_episode_rewards'):\n                    episode_rewards = env.get_episode_rewards()  # Use if method is available\n                else:\n                    episode_rewards = env.episode_rewards  # Direct attribute access if method is not available\n                all_rewards.extend(episode_rewards)  # Collecting rewards from all environments\n\n            # Compute overall mean reward across all environments\n            mean_reward = np.mean(all_rewards)\n            self.writer.add_scalar(\"Mean Episode Reward\", mean_reward, self.num_timesteps)\n            if mean_reward > self.best_mean_reward:\n                self.best_mean_reward = mean_reward\n                print(f\"New best mean reward across all envs: {self.best_mean_reward}\")\n        return True\n    \n    def _on_training_end(self):\n        self.writer.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:31:52.140503Z","iopub.execute_input":"2024-05-18T22:31:52.141187Z","iopub.status.idle":"2024-05-18T22:31:52.152566Z","shell.execute_reply.started":"2024-05-18T22:31:52.141153Z","shell.execute_reply":"2024-05-18T22:31:52.151507Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# # Set up multiple environments for SubprocVecEnv\n# n_envs = 700\n# train_env = DummyVecEnv([make_env() for _ in range(n_envs)])\n\n# eval_env = DummyVecEnv([make_env()])\n# eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\", eval_freq=500)\n# checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./checkpoints/\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-18T01:04:38.054583Z","iopub.execute_input":"2024-05-18T01:04:38.055462Z","iopub.status.idle":"2024-05-18T01:04:40.087454Z","shell.execute_reply.started":"2024-05-18T01:04:38.055425Z","shell.execute_reply":"2024-05-18T01:04:40.086492Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_env = DummyVecEnv([make_env()])\ncheckpoint_callback = CheckpointCallback(save_freq=1000, save_path=\"./checkpoints/\")\nreward_callback = AggregateRewardLoggingCallback(log_dir='custom_tensorboard/',\n                                                verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:31:57.624279Z","iopub.execute_input":"2024-05-18T22:31:57.624650Z","iopub.status.idle":"2024-05-18T22:31:57.639445Z","shell.execute_reply.started":"2024-05-18T22:31:57.624621Z","shell.execute_reply":"2024-05-18T22:31:57.638503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Policy settings for TQC\npolicy_kwargs = {\n    \"net_arch\": [256, 256, 128],  # Architecture for the policy network\n    \"activation_fn\": nn.ReLU,  # Properly reference activation functions\n}\n\ntqc_params = {\n    \"batch_size\": 256,  # Increased batch size\n    \"learning_rate\": 0.0003,\n    \"gradient_steps\": 64,\n    \"train_freq\": 64,\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:32:01.607806Z","iopub.execute_input":"2024-05-18T22:32:01.608738Z","iopub.status.idle":"2024-05-18T22:32:01.613625Z","shell.execute_reply.started":"2024-05-18T22:32:01.608703Z","shell.execute_reply":"2024-05-18T22:32:01.612639Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# int(1e6), int(3e6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TQC(\"MlpPolicy\", train_env,\n            policy_kwargs=policy_kwargs, verbose=1, device='cuda',\n           tensorboard_log=\"./tensorboard_logs/\", **tqc_params)\nmodel.learn(total_timesteps=int(1e6),\n            callback=[checkpoint_callback, reward_callback])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T22:32:09.107574Z","iopub.execute_input":"2024-05-18T22:32:09.107970Z","iopub.status.idle":"2024-05-19T03:13:44.916244Z","shell.execute_reply.started":"2024-05-18T22:32:09.107941Z","shell.execute_reply":"2024-05-19T03:13:44.915267Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Using cuda device\n\u001b[A                             \u001b[A\nLogging to ./tensorboard_logs/TQC_1\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 141       |\n|    ep_rew_mean     | -2.37e+03 |\n| time/              |           |\n|    episodes        | 4         |\n|    fps             | 41        |\n|    time_elapsed    | 13        |\n|    total_timesteps | 565       |\n| train/             |           |\n|    actor_loss      | 62.8      |\n|    critic_loss     | 3.09      |\n|    ent_coef        | 0.982     |\n|    ent_coef_loss   | -0.0167   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 448       |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2298.7283231033084\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 142       |\n|    ep_rew_mean     | -2.46e+03 |\n| time/              |           |\n|    episodes        | 8         |\n|    fps             | 48        |\n|    time_elapsed    | 23        |\n|    total_timesteps | 1135      |\n| train/             |           |\n|    actor_loss      | 120       |\n|    critic_loss     | 3.62      |\n|    ent_coef        | 0.919     |\n|    ent_coef_loss   | -0.071    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 1024      |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 143       |\n|    ep_rew_mean     | -2.54e+03 |\n| time/              |           |\n|    episodes        | 12        |\n|    fps             | 52        |\n|    time_elapsed    | 32        |\n|    total_timesteps | 1712      |\n| train/             |           |\n|    actor_loss      | 151       |\n|    critic_loss     | 3.26      |\n|    ent_coef        | 0.657     |\n|    ent_coef_loss   | -4.89     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 1600      |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 148       |\n|    ep_rew_mean     | -2.48e+03 |\n| time/              |           |\n|    episodes        | 16        |\n|    fps             | 53        |\n|    time_elapsed    | 44        |\n|    total_timesteps | 2372      |\n| train/             |           |\n|    actor_loss      | 196       |\n|    critic_loss     | 2.76      |\n|    ent_coef        | 0.498     |\n|    ent_coef_loss   | -7.92     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 2304      |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 191       |\n|    ep_rew_mean     | -4.14e+03 |\n| time/              |           |\n|    episodes        | 20        |\n|    fps             | 56        |\n|    time_elapsed    | 67        |\n|    total_timesteps | 3814      |\n| train/             |           |\n|    actor_loss      | 325       |\n|    critic_loss     | 3.59      |\n|    ent_coef        | 0.327     |\n|    ent_coef_loss   | -9.34     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 3712      |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 291       |\n|    ep_rew_mean     | -7.74e+03 |\n| time/              |           |\n|    episodes        | 24        |\n|    fps             | 58        |\n|    time_elapsed    | 120       |\n|    total_timesteps | 6993      |\n| train/             |           |\n|    actor_loss      | 558       |\n|    critic_loss     | 3.61      |\n|    ent_coef        | 0.159     |\n|    ent_coef_loss   | -5.36     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 6912      |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 358       |\n|    ep_rew_mean     | -9.71e+03 |\n| time/              |           |\n|    episodes        | 28        |\n|    fps             | 59        |\n|    time_elapsed    | 169       |\n|    total_timesteps | 10018     |\n| train/             |           |\n|    actor_loss      | 787       |\n|    critic_loss     | 4.88      |\n|    ent_coef        | 0.131     |\n|    ent_coef_loss   | 0.55      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 9920      |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 402       |\n|    ep_rew_mean     | -1.29e+04 |\n| time/              |           |\n|    episodes        | 32        |\n|    fps             | 59        |\n|    time_elapsed    | 215       |\n|    total_timesteps | 12860     |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 8.95      |\n|    ent_coef        | 0.189     |\n|    ent_coef_loss   | 0.462     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 12736     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 425       |\n|    ep_rew_mean     | -1.42e+04 |\n| time/              |           |\n|    episodes        | 36        |\n|    fps             | 59        |\n|    time_elapsed    | 256       |\n|    total_timesteps | 15291     |\n| train/             |           |\n|    actor_loss      | 1.48e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.257     |\n|    ent_coef_loss   | 0.294     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 15168     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 429       |\n|    ep_rew_mean     | -1.43e+04 |\n| time/              |           |\n|    episodes        | 40        |\n|    fps             | 59        |\n|    time_elapsed    | 287       |\n|    total_timesteps | 17155     |\n| train/             |           |\n|    actor_loss      | 1.62e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.33      |\n|    ent_coef_loss   | 0.0513    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 17088     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 399       |\n|    ep_rew_mean     | -1.32e+04 |\n| time/              |           |\n|    episodes        | 44        |\n|    fps             | 59        |\n|    time_elapsed    | 294       |\n|    total_timesteps | 17573     |\n| train/             |           |\n|    actor_loss      | 1.62e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.332     |\n|    ent_coef_loss   | 0.0286    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 17472     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 374       |\n|    ep_rew_mean     | -1.22e+04 |\n| time/              |           |\n|    episodes        | 48        |\n|    fps             | 59        |\n|    time_elapsed    | 300       |\n|    total_timesteps | 17929     |\n| train/             |           |\n|    actor_loss      | 1.61e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.331     |\n|    ent_coef_loss   | 0.0159    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 17856     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 352       |\n|    ep_rew_mean     | -1.14e+04 |\n| time/              |           |\n|    episodes        | 52        |\n|    fps             | 59        |\n|    time_elapsed    | 306       |\n|    total_timesteps | 18298     |\n| train/             |           |\n|    actor_loss      | 1.61e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.333     |\n|    ent_coef_loss   | -0.0435   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 18176     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 333       |\n|    ep_rew_mean     | -1.07e+04 |\n| time/              |           |\n|    episodes        | 56        |\n|    fps             | 59        |\n|    time_elapsed    | 312       |\n|    total_timesteps | 18666     |\n| train/             |           |\n|    actor_loss      | 1.62e+03  |\n|    critic_loss     | 16.2      |\n|    ent_coef        | 0.342     |\n|    ent_coef_loss   | 0.0877    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 18560     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 317       |\n|    ep_rew_mean     | -1.02e+04 |\n| time/              |           |\n|    episodes        | 60        |\n|    fps             | 59        |\n|    time_elapsed    | 319       |\n|    total_timesteps | 19035     |\n| train/             |           |\n|    actor_loss      | 1.63e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.351     |\n|    ent_coef_loss   | 0.0538    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 18944     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 315       |\n|    ep_rew_mean     | -1.02e+04 |\n| time/              |           |\n|    episodes        | 64        |\n|    fps             | 59        |\n|    time_elapsed    | 337       |\n|    total_timesteps | 20170     |\n| train/             |           |\n|    actor_loss      | 1.65e+03  |\n|    critic_loss     | 18.7      |\n|    ent_coef        | 0.372     |\n|    ent_coef_loss   | -0.0436   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 20096     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 302       |\n|    ep_rew_mean     | -9.73e+03 |\n| time/              |           |\n|    episodes        | 68        |\n|    fps             | 59        |\n|    time_elapsed    | 344       |\n|    total_timesteps | 20566     |\n| train/             |           |\n|    actor_loss      | 1.65e+03  |\n|    critic_loss     | 19.9      |\n|    ent_coef        | 0.38      |\n|    ent_coef_loss   | 0.161     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 20480     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 303       |\n|    ep_rew_mean     | -9.79e+03 |\n| time/              |           |\n|    episodes        | 72        |\n|    fps             | 59        |\n|    time_elapsed    | 364       |\n|    total_timesteps | 21786     |\n| train/             |           |\n|    actor_loss      | 1.7e+03   |\n|    critic_loss     | 20        |\n|    ent_coef        | 0.428     |\n|    ent_coef_loss   | 0.0619    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 21696     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 292       |\n|    ep_rew_mean     | -9.39e+03 |\n| time/              |           |\n|    episodes        | 76        |\n|    fps             | 59        |\n|    time_elapsed    | 371       |\n|    total_timesteps | 22184     |\n| train/             |           |\n|    actor_loss      | 1.71e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.441     |\n|    ent_coef_loss   | 0.078     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 22080     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 283       |\n|    ep_rew_mean     | -9.08e+03 |\n| time/              |           |\n|    episodes        | 80        |\n|    fps             | 59        |\n|    time_elapsed    | 378       |\n|    total_timesteps | 22609     |\n| train/             |           |\n|    actor_loss      | 1.72e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.451     |\n|    ent_coef_loss   | 0.0566    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 22528     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 283       |\n|    ep_rew_mean     | -9.09e+03 |\n| time/              |           |\n|    episodes        | 84        |\n|    fps             | 59        |\n|    time_elapsed    | 397       |\n|    total_timesteps | 23747     |\n| train/             |           |\n|    actor_loss      | 1.76e+03  |\n|    critic_loss     | 22.8      |\n|    ent_coef        | 0.469     |\n|    ent_coef_loss   | 0.0122    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 23680     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 308       |\n|    ep_rew_mean     | -9.81e+03 |\n| time/              |           |\n|    episodes        | 88        |\n|    fps             | 60        |\n|    time_elapsed    | 450       |\n|    total_timesteps | 27060     |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 25.3      |\n|    ent_coef        | 0.502     |\n|    ent_coef_loss   | 0.03      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 26944     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 317      |\n|    ep_rew_mean     | -1e+04   |\n| time/              |          |\n|    episodes        | 92       |\n|    fps             | 60       |\n|    time_elapsed    | 484      |\n|    total_timesteps | 29165    |\n| train/             |          |\n|    actor_loss      | 1.96e+03 |\n|    critic_loss     | 24.2     |\n|    ent_coef        | 0.498    |\n|    ent_coef_loss   | 0.0459   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 29056    |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 344       |\n|    ep_rew_mean     | -1.07e+04 |\n| time/              |           |\n|    episodes        | 96        |\n|    fps             | 60        |\n|    time_elapsed    | 546       |\n|    total_timesteps | 33001     |\n| train/             |           |\n|    actor_loss      | 2e+03     |\n|    critic_loss     | 25.1      |\n|    ent_coef        | 0.494     |\n|    ent_coef_loss   | 0.00257   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 32896     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 360       |\n|    ep_rew_mean     | -1.11e+04 |\n| time/              |           |\n|    episodes        | 100       |\n|    fps             | 60        |\n|    time_elapsed    | 595       |\n|    total_timesteps | 36015     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 24.4      |\n|    ent_coef        | 0.483     |\n|    ent_coef_loss   | -0.021    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 35904     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 377       |\n|    ep_rew_mean     | -1.15e+04 |\n| time/              |           |\n|    episodes        | 104       |\n|    fps             | 60        |\n|    time_elapsed    | 632       |\n|    total_timesteps | 38242     |\n| train/             |           |\n|    actor_loss      | 2.01e+03  |\n|    critic_loss     | 23.4      |\n|    ent_coef        | 0.461     |\n|    ent_coef_loss   | -0.0161   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 38144     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 387       |\n|    ep_rew_mean     | -1.18e+04 |\n| time/              |           |\n|    episodes        | 108       |\n|    fps             | 60        |\n|    time_elapsed    | 659       |\n|    total_timesteps | 39816     |\n| train/             |           |\n|    actor_loss      | 2.01e+03  |\n|    critic_loss     | 23.8      |\n|    ent_coef        | 0.453     |\n|    ent_coef_loss   | -0.0522   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 39744     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 409       |\n|    ep_rew_mean     | -1.24e+04 |\n| time/              |           |\n|    episodes        | 112       |\n|    fps             | 60        |\n|    time_elapsed    | 704       |\n|    total_timesteps | 42608     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 23.7      |\n|    ent_coef        | 0.439     |\n|    ent_coef_loss   | -0.0647   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 42496     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 418       |\n|    ep_rew_mean     | -1.27e+04 |\n| time/              |           |\n|    episodes        | 116       |\n|    fps             | 60        |\n|    time_elapsed    | 731       |\n|    total_timesteps | 44184     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 23.8      |\n|    ent_coef        | 0.436     |\n|    ent_coef_loss   | -0.00188  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 44096     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 425       |\n|    ep_rew_mean     | -1.28e+04 |\n| time/              |           |\n|    episodes        | 120       |\n|    fps             | 60        |\n|    time_elapsed    | 766       |\n|    total_timesteps | 46323     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 24.5      |\n|    ent_coef        | 0.43      |\n|    ent_coef_loss   | 0.00946   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 46208     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 430       |\n|    ep_rew_mean     | -1.27e+04 |\n| time/              |           |\n|    episodes        | 124       |\n|    fps             | 60        |\n|    time_elapsed    | 827       |\n|    total_timesteps | 49987     |\n| train/             |           |\n|    actor_loss      | 2.06e+03  |\n|    critic_loss     | 24.6      |\n|    ent_coef        | 0.442     |\n|    ent_coef_loss   | 0.0592    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 49920     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 436       |\n|    ep_rew_mean     | -1.27e+04 |\n| time/              |           |\n|    episodes        | 128       |\n|    fps             | 60        |\n|    time_elapsed    | 886       |\n|    total_timesteps | 53612     |\n| train/             |           |\n|    actor_loss      | 2.06e+03  |\n|    critic_loss     | 23.5      |\n|    ent_coef        | 0.444     |\n|    ent_coef_loss   | 0.0256    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 53504     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 445       |\n|    ep_rew_mean     | -1.22e+04 |\n| time/              |           |\n|    episodes        | 132       |\n|    fps             | 60        |\n|    time_elapsed    | 948       |\n|    total_timesteps | 57352     |\n| train/             |           |\n|    actor_loss      | 2.06e+03  |\n|    critic_loss     | 24        |\n|    ent_coef        | 0.452     |\n|    ent_coef_loss   | -0.056    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 57280     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 454       |\n|    ep_rew_mean     | -1.21e+04 |\n| time/              |           |\n|    episodes        | 136       |\n|    fps             | 60        |\n|    time_elapsed    | 1001      |\n|    total_timesteps | 60658     |\n| train/             |           |\n|    actor_loss      | 2.08e+03  |\n|    critic_loss     | 24.1      |\n|    ent_coef        | 0.444     |\n|    ent_coef_loss   | -0.00329  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 60544     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 468       |\n|    ep_rew_mean     | -1.24e+04 |\n| time/              |           |\n|    episodes        | 140       |\n|    fps             | 60        |\n|    time_elapsed    | 1055      |\n|    total_timesteps | 63997     |\n| train/             |           |\n|    actor_loss      | 2.09e+03  |\n|    critic_loss     | 25.4      |\n|    ent_coef        | 0.441     |\n|    ent_coef_loss   | -0.072    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 63872     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 493       |\n|    ep_rew_mean     | -1.31e+04 |\n| time/              |           |\n|    episodes        | 144       |\n|    fps             | 60        |\n|    time_elapsed    | 1102      |\n|    total_timesteps | 66880     |\n| train/             |           |\n|    actor_loss      | 2.11e+03  |\n|    critic_loss     | 25.3      |\n|    ent_coef        | 0.44      |\n|    ent_coef_loss   | -0.0504   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 66752     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 510       |\n|    ep_rew_mean     | -1.35e+04 |\n| time/              |           |\n|    episodes        | 148       |\n|    fps             | 60        |\n|    time_elapsed    | 1137      |\n|    total_timesteps | 68950     |\n| train/             |           |\n|    actor_loss      | 2.1e+03   |\n|    critic_loss     | 25.5      |\n|    ent_coef        | 0.443     |\n|    ent_coef_loss   | -0.0849   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 68864     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 542       |\n|    ep_rew_mean     | -1.42e+04 |\n| time/              |           |\n|    episodes        | 152       |\n|    fps             | 60        |\n|    time_elapsed    | 1195      |\n|    total_timesteps | 72532     |\n| train/             |           |\n|    actor_loss      | 2.09e+03  |\n|    critic_loss     | 24.5      |\n|    ent_coef        | 0.445     |\n|    ent_coef_loss   | -0.0462   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 72448     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 576      |\n|    ep_rew_mean     | -1.5e+04 |\n| time/              |          |\n|    episodes        | 156      |\n|    fps             | 60       |\n|    time_elapsed    | 1256     |\n|    total_timesteps | 76245    |\n| train/             |          |\n|    actor_loss      | 2.1e+03  |\n|    critic_loss     | 25       |\n|    ent_coef        | 0.443    |\n|    ent_coef_loss   | 0.0282   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 76160    |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 604       |\n|    ep_rew_mean     | -1.56e+04 |\n| time/              |           |\n|    episodes        | 160       |\n|    fps             | 60        |\n|    time_elapsed    | 1309      |\n|    total_timesteps | 79462     |\n| train/             |           |\n|    actor_loss      | 2.08e+03  |\n|    critic_loss     | 24.5      |\n|    ent_coef        | 0.432     |\n|    ent_coef_loss   | -0.0103   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 79360     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 621       |\n|    ep_rew_mean     | -1.58e+04 |\n| time/              |           |\n|    episodes        | 164       |\n|    fps             | 60        |\n|    time_elapsed    | 1357      |\n|    total_timesteps | 82315     |\n| train/             |           |\n|    actor_loss      | 2.05e+03  |\n|    critic_loss     | 24.7      |\n|    ent_coef        | 0.41      |\n|    ent_coef_loss   | -0.0254   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 82240     |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 639       |\n|    ep_rew_mean     | -1.62e+04 |\n| time/              |           |\n|    episodes        | 168       |\n|    fps             | 60        |\n|    time_elapsed    | 1391      |\n|    total_timesteps | 84457     |\n| train/             |           |\n|    actor_loss      | 2.06e+03  |\n|    critic_loss     | 24.1      |\n|    ent_coef        | 0.402     |\n|    ent_coef_loss   | 0.0568    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 84352     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 662       |\n|    ep_rew_mean     | -1.66e+04 |\n| time/              |           |\n|    episodes        | 172       |\n|    fps             | 60        |\n|    time_elapsed    | 1450      |\n|    total_timesteps | 88003     |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 24.7      |\n|    ent_coef        | 0.392     |\n|    ent_coef_loss   | -0.00805  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 87936     |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 690       |\n|    ep_rew_mean     | -1.72e+04 |\n| time/              |           |\n|    episodes        | 176       |\n|    fps             | 60        |\n|    time_elapsed    | 1503      |\n|    total_timesteps | 91222     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 24.8      |\n|    ent_coef        | 0.387     |\n|    ent_coef_loss   | -0.0482   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 91136     |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 720       |\n|    ep_rew_mean     | -1.79e+04 |\n| time/              |           |\n|    episodes        | 180       |\n|    fps             | 60        |\n|    time_elapsed    | 1558      |\n|    total_timesteps | 94568     |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 24.4      |\n|    ent_coef        | 0.386     |\n|    ent_coef_loss   | -0.0396   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 94464     |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 713       |\n|    ep_rew_mean     | -1.76e+04 |\n| time/              |           |\n|    episodes        | 184       |\n|    fps             | 60        |\n|    time_elapsed    | 1566      |\n|    total_timesteps | 95084     |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 24.4      |\n|    ent_coef        | 0.384     |\n|    ent_coef_loss   | -0.0147   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 94976     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 695      |\n|    ep_rew_mean     | -1.7e+04 |\n| time/              |          |\n|    episodes        | 188      |\n|    fps             | 60       |\n|    time_elapsed    | 1591     |\n|    total_timesteps | 96585    |\n| train/             |          |\n|    actor_loss      | 2.05e+03 |\n|    critic_loss     | 24.5     |\n|    ent_coef        | 0.378    |\n|    ent_coef_loss   | 0.0256   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 96512    |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 679       |\n|    ep_rew_mean     | -1.64e+04 |\n| time/              |           |\n|    episodes        | 192       |\n|    fps             | 60        |\n|    time_elapsed    | 1599      |\n|    total_timesteps | 97039     |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 24.6      |\n|    ent_coef        | 0.374     |\n|    ent_coef_loss   | -0.0421   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 96960     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 654       |\n|    ep_rew_mean     | -1.57e+04 |\n| time/              |           |\n|    episodes        | 196       |\n|    fps             | 60        |\n|    time_elapsed    | 1621      |\n|    total_timesteps | 98422     |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 24.9      |\n|    ent_coef        | 0.37      |\n|    ent_coef_loss   | 0.0313    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 98304     |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 630      |\n|    ep_rew_mean     | -1.5e+04 |\n| time/              |          |\n|    episodes        | 200      |\n|    fps             | 60       |\n|    time_elapsed    | 1631     |\n|    total_timesteps | 99010    |\n| train/             |          |\n|    actor_loss      | 2.05e+03 |\n|    critic_loss     | 25.1     |\n|    ent_coef        | 0.37     |\n|    ent_coef_loss   | 0.0757   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 98944    |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 635      |\n|    ep_rew_mean     | -1.5e+04 |\n| time/              |          |\n|    episodes        | 204      |\n|    fps             | 60       |\n|    time_elapsed    | 1675     |\n|    total_timesteps | 101728   |\n| train/             |          |\n|    actor_loss      | 2.03e+03 |\n|    critic_loss     | 24.1     |\n|    ent_coef        | 0.361    |\n|    ent_coef_loss   | 0.0527   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 101632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 629       |\n|    ep_rew_mean     | -1.48e+04 |\n| time/              |           |\n|    episodes        | 208       |\n|    fps             | 60        |\n|    time_elapsed    | 1692      |\n|    total_timesteps | 102730    |\n| train/             |           |\n|    actor_loss      | 2.02e+03  |\n|    critic_loss     | 22.8      |\n|    ent_coef        | 0.359     |\n|    ent_coef_loss   | -0.0198   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 102656    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 631       |\n|    ep_rew_mean     | -1.46e+04 |\n| time/              |           |\n|    episodes        | 212       |\n|    fps             | 60        |\n|    time_elapsed    | 1741      |\n|    total_timesteps | 105729    |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 23.2      |\n|    ent_coef        | 0.348     |\n|    ent_coef_loss   | 0.0281    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 105664    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 647       |\n|    ep_rew_mean     | -1.49e+04 |\n| time/              |           |\n|    episodes        | 216       |\n|    fps             | 60        |\n|    time_elapsed    | 1792      |\n|    total_timesteps | 108879    |\n| train/             |           |\n|    actor_loss      | 2.04e+03  |\n|    critic_loss     | 23.9      |\n|    ent_coef        | 0.345     |\n|    ent_coef_loss   | 0.0737    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 108800    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 649       |\n|    ep_rew_mean     | -1.48e+04 |\n| time/              |           |\n|    episodes        | 220       |\n|    fps             | 60        |\n|    time_elapsed    | 1831      |\n|    total_timesteps | 111191    |\n| train/             |           |\n|    actor_loss      | 2.03e+03  |\n|    critic_loss     | 23.6      |\n|    ent_coef        | 0.347     |\n|    ent_coef_loss   | -0.0339   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 111104    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 648       |\n|    ep_rew_mean     | -1.47e+04 |\n| time/              |           |\n|    episodes        | 224       |\n|    fps             | 60        |\n|    time_elapsed    | 1890      |\n|    total_timesteps | 114765    |\n| train/             |           |\n|    actor_loss      | 2.01e+03  |\n|    critic_loss     | 22.4      |\n|    ent_coef        | 0.345     |\n|    ent_coef_loss   | -0.0595   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 114688    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 648       |\n|    ep_rew_mean     | -1.44e+04 |\n| time/              |           |\n|    episodes        | 228       |\n|    fps             | 60        |\n|    time_elapsed    | 1950      |\n|    total_timesteps | 118423    |\n| train/             |           |\n|    actor_loss      | 1.99e+03  |\n|    critic_loss     | 21.6      |\n|    ent_coef        | 0.335     |\n|    ent_coef_loss   | -0.0577   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 118336    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 645       |\n|    ep_rew_mean     | -1.42e+04 |\n| time/              |           |\n|    episodes        | 232       |\n|    fps             | 60        |\n|    time_elapsed    | 2007      |\n|    total_timesteps | 121861    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.327     |\n|    ent_coef_loss   | -0.0127   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 121792    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 647      |\n|    ep_rew_mean     | -1.4e+04 |\n| time/              |          |\n|    episodes        | 236      |\n|    fps             | 60       |\n|    time_elapsed    | 2065     |\n|    total_timesteps | 125375   |\n| train/             |          |\n|    actor_loss      | 1.93e+03 |\n|    critic_loss     | 20.4     |\n|    ent_coef        | 0.323    |\n|    ent_coef_loss   | 0.0412   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 125248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 636       |\n|    ep_rew_mean     | -1.35e+04 |\n| time/              |           |\n|    episodes        | 240       |\n|    fps             | 60        |\n|    time_elapsed    | 2101      |\n|    total_timesteps | 127587    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.32      |\n|    ent_coef_loss   | -0.0189   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 127488    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 618       |\n|    ep_rew_mean     | -1.31e+04 |\n| time/              |           |\n|    episodes        | 244       |\n|    fps             | 60        |\n|    time_elapsed    | 2121      |\n|    total_timesteps | 128730    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 20.5      |\n|    ent_coef        | 0.324     |\n|    ent_coef_loss   | -0.0732   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 128640    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 602       |\n|    ep_rew_mean     | -1.27e+04 |\n| time/              |           |\n|    episodes        | 248       |\n|    fps             | 60        |\n|    time_elapsed    | 2127      |\n|    total_timesteps | 129127    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.322     |\n|    ent_coef_loss   | -0.0242   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 129024    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 570      |\n|    ep_rew_mean     | -1.2e+04 |\n| time/              |          |\n|    episodes        | 252      |\n|    fps             | 60       |\n|    time_elapsed    | 2134     |\n|    total_timesteps | 129513   |\n| train/             |          |\n|    actor_loss      | 1.9e+03  |\n|    critic_loss     | 20.6     |\n|    ent_coef        | 0.32     |\n|    ent_coef_loss   | 0.00108  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 129408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 537       |\n|    ep_rew_mean     | -1.12e+04 |\n| time/              |           |\n|    episodes        | 256       |\n|    fps             | 60        |\n|    time_elapsed    | 2140      |\n|    total_timesteps | 129906    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.318     |\n|    ent_coef_loss   | -0.0439   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 129792    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 517       |\n|    ep_rew_mean     | -1.08e+04 |\n| time/              |           |\n|    episodes        | 260       |\n|    fps             | 60        |\n|    time_elapsed    | 2162      |\n|    total_timesteps | 131208    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 21.9      |\n|    ent_coef        | 0.321     |\n|    ent_coef_loss   | -0.00807  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 131136    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 494       |\n|    ep_rew_mean     | -1.03e+04 |\n| time/              |           |\n|    episodes        | 264       |\n|    fps             | 60        |\n|    time_elapsed    | 2170      |\n|    total_timesteps | 131690    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.318     |\n|    ent_coef_loss   | -0.16     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 131584    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 477       |\n|    ep_rew_mean     | -9.91e+03 |\n| time/              |           |\n|    episodes        | 268       |\n|    fps             | 60        |\n|    time_elapsed    | 2177      |\n|    total_timesteps | 132160    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.314     |\n|    ent_coef_loss   | 0.0404    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 132032    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 456       |\n|    ep_rew_mean     | -9.41e+03 |\n| time/              |           |\n|    episodes        | 272       |\n|    fps             | 60        |\n|    time_elapsed    | 2201      |\n|    total_timesteps | 133571    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.319     |\n|    ent_coef_loss   | 0.0386    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 133504    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 438       |\n|    ep_rew_mean     | -9.07e+03 |\n| time/              |           |\n|    episodes        | 276       |\n|    fps             | 60        |\n|    time_elapsed    | 2223      |\n|    total_timesteps | 134975    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.32      |\n|    ent_coef_loss   | 0.0134    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 134848    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 441       |\n|    ep_rew_mean     | -9.04e+03 |\n| time/              |           |\n|    episodes        | 280       |\n|    fps             | 60        |\n|    time_elapsed    | 2285      |\n|    total_timesteps | 138717    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 21.5      |\n|    ent_coef        | 0.308     |\n|    ent_coef_loss   | 0.146     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 138624    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 475       |\n|    ep_rew_mean     | -9.64e+03 |\n| time/              |           |\n|    episodes        | 284       |\n|    fps             | 60        |\n|    time_elapsed    | 2349      |\n|    total_timesteps | 142604    |\n| train/             |           |\n|    actor_loss      | 1.9e+03   |\n|    critic_loss     | 20        |\n|    ent_coef        | 0.308     |\n|    ent_coef_loss   | 0.0199    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 142528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 502       |\n|    ep_rew_mean     | -1.04e+04 |\n| time/              |           |\n|    episodes        | 288       |\n|    fps             | 60        |\n|    time_elapsed    | 2418      |\n|    total_timesteps | 146796    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 20.2      |\n|    ent_coef        | 0.299     |\n|    ent_coef_loss   | -0.108    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 146688    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 531      |\n|    ep_rew_mean     | -1.1e+04 |\n| time/              |          |\n|    episodes        | 292      |\n|    fps             | 60       |\n|    time_elapsed    | 2474     |\n|    total_timesteps | 150127   |\n| train/             |          |\n|    actor_loss      | 1.91e+03 |\n|    critic_loss     | 20.6     |\n|    ent_coef        | 0.293    |\n|    ent_coef_loss   | -0.0754  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 150016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 554       |\n|    ep_rew_mean     | -1.15e+04 |\n| time/              |           |\n|    episodes        | 296       |\n|    fps             | 60        |\n|    time_elapsed    | 2536      |\n|    total_timesteps | 153838    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 20.4      |\n|    ent_coef        | 0.284     |\n|    ent_coef_loss   | 0.0444    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 153728    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 588       |\n|    ep_rew_mean     | -1.23e+04 |\n| time/              |           |\n|    episodes        | 300       |\n|    fps             | 60        |\n|    time_elapsed    | 2602      |\n|    total_timesteps | 157790    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 19.9      |\n|    ent_coef        | 0.286     |\n|    ent_coef_loss   | -0.0464   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 157696    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 594       |\n|    ep_rew_mean     | -1.25e+04 |\n| time/              |           |\n|    episodes        | 304       |\n|    fps             | 60        |\n|    time_elapsed    | 2658      |\n|    total_timesteps | 161177    |\n| train/             |           |\n|    actor_loss      | 1.93e+03  |\n|    critic_loss     | 20.4      |\n|    ent_coef        | 0.281     |\n|    ent_coef_loss   | -0.13     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 161088    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 615       |\n|    ep_rew_mean     | -1.32e+04 |\n| time/              |           |\n|    episodes        | 308       |\n|    fps             | 60        |\n|    time_elapsed    | 2709      |\n|    total_timesteps | 164255    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.00861   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 164160    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 601      |\n|    ep_rew_mean     | -1.3e+04 |\n| time/              |          |\n|    episodes        | 312      |\n|    fps             | 60       |\n|    time_elapsed    | 2735     |\n|    total_timesteps | 165854   |\n| train/             |          |\n|    actor_loss      | 1.95e+03 |\n|    critic_loss     | 20.2     |\n|    ent_coef        | 0.279    |\n|    ent_coef_loss   | 0.0791   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 165760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 585       |\n|    ep_rew_mean     | -1.26e+04 |\n| time/              |           |\n|    episodes        | 316       |\n|    fps             | 60        |\n|    time_elapsed    | 2761      |\n|    total_timesteps | 167405    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.282     |\n|    ent_coef_loss   | -0.0441   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 167296    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 576       |\n|    ep_rew_mean     | -1.24e+04 |\n| time/              |           |\n|    episodes        | 320       |\n|    fps             | 60        |\n|    time_elapsed    | 2783      |\n|    total_timesteps | 168795    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.289     |\n|    ent_coef_loss   | -0.0618   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 168704    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 550       |\n|    ep_rew_mean     | -1.18e+04 |\n| time/              |           |\n|    episodes        | 324       |\n|    fps             | 60        |\n|    time_elapsed    | 2799      |\n|    total_timesteps | 169745    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.295     |\n|    ent_coef_loss   | 0.00106   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 169664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 540      |\n|    ep_rew_mean     | -1.2e+04 |\n| time/              |          |\n|    episodes        | 328      |\n|    fps             | 60       |\n|    time_elapsed    | 2842     |\n|    total_timesteps | 172457   |\n| train/             |          |\n|    actor_loss      | 1.97e+03 |\n|    critic_loss     | 23.5     |\n|    ent_coef        | 0.329    |\n|    ent_coef_loss   | 0.153    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 172352   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 519       |\n|    ep_rew_mean     | -1.17e+04 |\n| time/              |           |\n|    episodes        | 332       |\n|    fps             | 60        |\n|    time_elapsed    | 2863      |\n|    total_timesteps | 173718    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 23.7      |\n|    ent_coef        | 0.333     |\n|    ent_coef_loss   | 0.0858    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 173632    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 489      |\n|    ep_rew_mean     | -1.1e+04 |\n| time/              |          |\n|    episodes        | 336      |\n|    fps             | 60       |\n|    time_elapsed    | 2872     |\n|    total_timesteps | 174257   |\n| train/             |          |\n|    actor_loss      | 1.96e+03 |\n|    critic_loss     | 24       |\n|    ent_coef        | 0.332    |\n|    ent_coef_loss   | 0.0155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 174144   |\n---------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 493       |\n|    ep_rew_mean     | -1.14e+04 |\n| time/              |           |\n|    episodes        | 340       |\n|    fps             | 60        |\n|    time_elapsed    | 2915      |\n|    total_timesteps | 176846    |\n| train/             |           |\n|    actor_loss      | 1.98e+03  |\n|    critic_loss     | 23.3      |\n|    ent_coef        | 0.32      |\n|    ent_coef_loss   | -0.045    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 176768    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 521       |\n|    ep_rew_mean     | -1.18e+04 |\n| time/              |           |\n|    episodes        | 344       |\n|    fps             | 60        |\n|    time_elapsed    | 2978      |\n|    total_timesteps | 180799    |\n| train/             |           |\n|    actor_loss      | 2e+03     |\n|    critic_loss     | 23.2      |\n|    ent_coef        | 0.295     |\n|    ent_coef_loss   | -0.0416   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 180672    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 547       |\n|    ep_rew_mean     | -1.24e+04 |\n| time/              |           |\n|    episodes        | 348       |\n|    fps             | 60        |\n|    time_elapsed    | 3029      |\n|    total_timesteps | 183868    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.292     |\n|    ent_coef_loss   | -0.00395  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 183744    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 582       |\n|    ep_rew_mean     | -1.31e+04 |\n| time/              |           |\n|    episodes        | 352       |\n|    fps             | 60        |\n|    time_elapsed    | 3092      |\n|    total_timesteps | 187692    |\n| train/             |           |\n|    actor_loss      | 1.97e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.289     |\n|    ent_coef_loss   | -0.0405   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 187584    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 609       |\n|    ep_rew_mean     | -1.35e+04 |\n| time/              |           |\n|    episodes        | 356       |\n|    fps             | 60        |\n|    time_elapsed    | 3143      |\n|    total_timesteps | 190818    |\n| train/             |           |\n|    actor_loss      | 1.97e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | -0.043    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 190720    |\n----------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 636      |\n|    ep_rew_mean     | -1.4e+04 |\n| time/              |          |\n|    episodes        | 360      |\n|    fps             | 60       |\n|    time_elapsed    | 3209     |\n|    total_timesteps | 194826   |\n| train/             |          |\n|    actor_loss      | 1.95e+03 |\n|    critic_loss     | 21.8     |\n|    ent_coef        | 0.284    |\n|    ent_coef_loss   | 0.0389   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 194752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 648       |\n|    ep_rew_mean     | -1.42e+04 |\n| time/              |           |\n|    episodes        | 364       |\n|    fps             | 60        |\n|    time_elapsed    | 3236      |\n|    total_timesteps | 196479    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.283     |\n|    ent_coef_loss   | -0.0519   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 196352    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 647       |\n|    ep_rew_mean     | -1.42e+04 |\n| time/              |           |\n|    episodes        | 368       |\n|    fps             | 60        |\n|    time_elapsed    | 3243      |\n|    total_timesteps | 196894    |\n| train/             |           |\n|    actor_loss      | 1.96e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.283     |\n|    ent_coef_loss   | 0.103     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 196800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 638       |\n|    ep_rew_mean     | -1.39e+04 |\n| time/              |           |\n|    episodes        | 372       |\n|    fps             | 60        |\n|    time_elapsed    | 3251      |\n|    total_timesteps | 197370    |\n| train/             |           |\n|    actor_loss      | 1.95e+03  |\n|    critic_loss     | 21.5      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | 0.147     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 197248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 627       |\n|    ep_rew_mean     | -1.37e+04 |\n| time/              |           |\n|    episodes        | 376       |\n|    fps             | 60        |\n|    time_elapsed    | 3257      |\n|    total_timesteps | 197705    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.287     |\n|    ent_coef_loss   | -0.0359   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 197632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 593       |\n|    ep_rew_mean     | -1.29e+04 |\n| time/              |           |\n|    episodes        | 380       |\n|    fps             | 60        |\n|    time_elapsed    | 3262      |\n|    total_timesteps | 198013    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.288     |\n|    ent_coef_loss   | -0.0825   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 197888    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 557       |\n|    ep_rew_mean     | -1.23e+04 |\n| time/              |           |\n|    episodes        | 384       |\n|    fps             | 60        |\n|    time_elapsed    | 3267      |\n|    total_timesteps | 198329    |\n| train/             |           |\n|    actor_loss      | 1.95e+03  |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.287     |\n|    ent_coef_loss   | -0.141    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 198208    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 519       |\n|    ep_rew_mean     | -1.12e+04 |\n| time/              |           |\n|    episodes        | 388       |\n|    fps             | 60        |\n|    time_elapsed    | 3273      |\n|    total_timesteps | 198667    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.288     |\n|    ent_coef_loss   | -0.0255   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 198592    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 488       |\n|    ep_rew_mean     | -1.06e+04 |\n| time/              |           |\n|    episodes        | 392       |\n|    fps             | 60        |\n|    time_elapsed    | 3278      |\n|    total_timesteps | 198976    |\n| train/             |           |\n|    actor_loss      | 1.95e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.288     |\n|    ent_coef_loss   | 0.0212    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 198848    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 455       |\n|    ep_rew_mean     | -9.84e+03 |\n| time/              |           |\n|    episodes        | 396       |\n|    fps             | 60        |\n|    time_elapsed    | 3284      |\n|    total_timesteps | 199327    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.291     |\n|    ent_coef_loss   | -0.0292   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 199232    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 418       |\n|    ep_rew_mean     | -8.97e+03 |\n| time/              |           |\n|    episodes        | 400       |\n|    fps             | 60        |\n|    time_elapsed    | 3289      |\n|    total_timesteps | 199634    |\n| train/             |           |\n|    actor_loss      | 1.94e+03  |\n|    critic_loss     | 22.5      |\n|    ent_coef        | 0.295     |\n|    ent_coef_loss   | 0.0892    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 199552    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 388      |\n|    ep_rew_mean     | -8.3e+03 |\n| time/              |          |\n|    episodes        | 404      |\n|    fps             | 60       |\n|    time_elapsed    | 3295     |\n|    total_timesteps | 199954   |\n| train/             |          |\n|    actor_loss      | 1.93e+03 |\n|    critic_loss     | 21.6     |\n|    ent_coef        | 0.299    |\n|    ent_coef_loss   | 0.00861  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 199872   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 360       |\n|    ep_rew_mean     | -7.49e+03 |\n| time/              |           |\n|    episodes        | 408       |\n|    fps             | 60        |\n|    time_elapsed    | 3300      |\n|    total_timesteps | 200260    |\n| train/             |           |\n|    actor_loss      | 1.93e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.302     |\n|    ent_coef_loss   | 0.00171   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 200192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 347       |\n|    ep_rew_mean     | -7.22e+03 |\n| time/              |           |\n|    episodes        | 412       |\n|    fps             | 60        |\n|    time_elapsed    | 3306      |\n|    total_timesteps | 200585    |\n| train/             |           |\n|    actor_loss      | 1.93e+03  |\n|    critic_loss     | 21.6      |\n|    ent_coef        | 0.303     |\n|    ent_coef_loss   | -0.0171   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 200512    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 335       |\n|    ep_rew_mean     | -6.98e+03 |\n| time/              |           |\n|    episodes        | 416       |\n|    fps             | 60        |\n|    time_elapsed    | 3311      |\n|    total_timesteps | 200903    |\n| train/             |           |\n|    actor_loss      | 1.9e+03   |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.304     |\n|    ent_coef_loss   | 0.0246    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 200832    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 324      |\n|    ep_rew_mean     | -6.8e+03 |\n| time/              |          |\n|    episodes        | 420      |\n|    fps             | 60       |\n|    time_elapsed    | 3317     |\n|    total_timesteps | 201226   |\n| train/             |          |\n|    actor_loss      | 1.92e+03 |\n|    critic_loss     | 22.6     |\n|    ent_coef        | 0.304    |\n|    ent_coef_loss   | 0.099    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 201152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 318       |\n|    ep_rew_mean     | -6.64e+03 |\n| time/              |           |\n|    episodes        | 424       |\n|    fps             | 60        |\n|    time_elapsed    | 3322      |\n|    total_timesteps | 201560    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 21.8      |\n|    ent_coef        | 0.308     |\n|    ent_coef_loss   | -0.0461   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 201472    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 294      |\n|    ep_rew_mean     | -5.9e+03 |\n| time/              |          |\n|    episodes        | 428      |\n|    fps             | 60       |\n|    time_elapsed    | 3328     |\n|    total_timesteps | 201906   |\n| train/             |          |\n|    actor_loss      | 1.91e+03 |\n|    critic_loss     | 21.7     |\n|    ent_coef        | 0.31     |\n|    ent_coef_loss   | -0.00159 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 201792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 285       |\n|    ep_rew_mean     | -5.58e+03 |\n| time/              |           |\n|    episodes        | 432       |\n|    fps             | 60        |\n|    time_elapsed    | 3335      |\n|    total_timesteps | 202257    |\n| train/             |           |\n|    actor_loss      | 1.92e+03  |\n|    critic_loss     | 22.6      |\n|    ent_coef        | 0.312     |\n|    ent_coef_loss   | 0.0355    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 202176    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 285       |\n|    ep_rew_mean     | -5.55e+03 |\n| time/              |           |\n|    episodes        | 436       |\n|    fps             | 60        |\n|    time_elapsed    | 3342      |\n|    total_timesteps | 202720    |\n| train/             |           |\n|    actor_loss      | 1.91e+03  |\n|    critic_loss     | 22.4      |\n|    ent_coef        | 0.313     |\n|    ent_coef_loss   | -0.0262   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 202624    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 264       |\n|    ep_rew_mean     | -4.91e+03 |\n| time/              |           |\n|    episodes        | 440       |\n|    fps             | 60        |\n|    time_elapsed    | 3352      |\n|    total_timesteps | 203278    |\n| train/             |           |\n|    actor_loss      | 1.9e+03   |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.309     |\n|    ent_coef_loss   | 0.0412    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 203200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 231       |\n|    ep_rew_mean     | -4.21e+03 |\n| time/              |           |\n|    episodes        | 444       |\n|    fps             | 60        |\n|    time_elapsed    | 3362      |\n|    total_timesteps | 203879    |\n| train/             |           |\n|    actor_loss      | 1.89e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.305     |\n|    ent_coef_loss   | -0.0528   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 203776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 206       |\n|    ep_rew_mean     | -3.62e+03 |\n| time/              |           |\n|    episodes        | 448       |\n|    fps             | 60        |\n|    time_elapsed    | 3373      |\n|    total_timesteps | 204514    |\n| train/             |           |\n|    actor_loss      | 1.9e+03   |\n|    critic_loss     | 22.2      |\n|    ent_coef        | 0.305     |\n|    ent_coef_loss   | -0.0299   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 204416    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 176       |\n|    ep_rew_mean     | -3.04e+03 |\n| time/              |           |\n|    episodes        | 452       |\n|    fps             | 60        |\n|    time_elapsed    | 3386      |\n|    total_timesteps | 205259    |\n| train/             |           |\n|    actor_loss      | 1.88e+03  |\n|    critic_loss     | 22.3      |\n|    ent_coef        | 0.304     |\n|    ent_coef_loss   | -0.0242   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 205184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 160       |\n|    ep_rew_mean     | -2.77e+03 |\n| time/              |           |\n|    episodes        | 456       |\n|    fps             | 60        |\n|    time_elapsed    | 3411      |\n|    total_timesteps | 206824    |\n| train/             |           |\n|    actor_loss      | 1.86e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.295     |\n|    ent_coef_loss   | -0.184    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 206720    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 129       |\n|    ep_rew_mean     | -2.12e+03 |\n| time/              |           |\n|    episodes        | 460       |\n|    fps             | 60        |\n|    time_elapsed    | 3426      |\n|    total_timesteps | 207729    |\n| train/             |           |\n|    actor_loss      | 1.88e+03  |\n|    critic_loss     | 21.5      |\n|    ent_coef        | 0.291     |\n|    ent_coef_loss   | 0.0567    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 207616    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 129       |\n|    ep_rew_mean     | -2.06e+03 |\n| time/              |           |\n|    episodes        | 464       |\n|    fps             | 60        |\n|    time_elapsed    | 3455      |\n|    total_timesteps | 209426    |\n| train/             |           |\n|    actor_loss      | 1.87e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | -0.0332   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 209344    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 157       |\n|    ep_rew_mean     | -2.56e+03 |\n| time/              |           |\n|    episodes        | 468       |\n|    fps             | 60        |\n|    time_elapsed    | 3507      |\n|    total_timesteps | 212625    |\n| train/             |           |\n|    actor_loss      | 1.87e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | -0.064    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 212544    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 165       |\n|    ep_rew_mean     | -2.69e+03 |\n| time/              |           |\n|    episodes        | 472       |\n|    fps             | 60        |\n|    time_elapsed    | 3527      |\n|    total_timesteps | 213870    |\n| train/             |           |\n|    actor_loss      | 1.87e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | -0.119    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 213760    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 169       |\n|    ep_rew_mean     | -2.74e+03 |\n| time/              |           |\n|    episodes        | 476       |\n|    fps             | 60        |\n|    time_elapsed    | 3540      |\n|    total_timesteps | 214618    |\n| train/             |           |\n|    actor_loss      | 1.87e+03  |\n|    critic_loss     | 20.5      |\n|    ent_coef        | 0.268     |\n|    ent_coef_loss   | 0.0408    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 214528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 172       |\n|    ep_rew_mean     | -2.79e+03 |\n| time/              |           |\n|    episodes        | 480       |\n|    fps             | 60        |\n|    time_elapsed    | 3551      |\n|    total_timesteps | 215240    |\n| train/             |           |\n|    actor_loss      | 1.87e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.271     |\n|    ent_coef_loss   | -0.0193   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 215168    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 175       |\n|    ep_rew_mean     | -2.84e+03 |\n| time/              |           |\n|    episodes        | 484       |\n|    fps             | 60        |\n|    time_elapsed    | 3559      |\n|    total_timesteps | 215808    |\n| train/             |           |\n|    actor_loss      | 1.86e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.269     |\n|    ent_coef_loss   | -0.14     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 215680    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 176       |\n|    ep_rew_mean     | -2.86e+03 |\n| time/              |           |\n|    episodes        | 488       |\n|    fps             | 60        |\n|    time_elapsed    | 3568      |\n|    total_timesteps | 216296    |\n| train/             |           |\n|    actor_loss      | 1.86e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.268     |\n|    ent_coef_loss   | 0.00399   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 216192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 180       |\n|    ep_rew_mean     | -2.93e+03 |\n| time/              |           |\n|    episodes        | 492       |\n|    fps             | 60        |\n|    time_elapsed    | 3580      |\n|    total_timesteps | 217023    |\n| train/             |           |\n|    actor_loss      | 1.85e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.268     |\n|    ent_coef_loss   | -0.0684   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 216896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 184      |\n|    ep_rew_mean     | -3e+03   |\n| time/              |          |\n|    episodes        | 496      |\n|    fps             | 60       |\n|    time_elapsed    | 3592     |\n|    total_timesteps | 217730   |\n| train/             |          |\n|    actor_loss      | 1.87e+03 |\n|    critic_loss     | 20.2     |\n|    ent_coef        | 0.274    |\n|    ent_coef_loss   | -0.0476  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 217664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 216       |\n|    ep_rew_mean     | -3.59e+03 |\n| time/              |           |\n|    episodes        | 500       |\n|    fps             | 60        |\n|    time_elapsed    | 3651      |\n|    total_timesteps | 221266    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | 0.036     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 221184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 244       |\n|    ep_rew_mean     | -4.16e+03 |\n| time/              |           |\n|    episodes        | 504       |\n|    fps             | 60        |\n|    time_elapsed    | 3702      |\n|    total_timesteps | 224404    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.277     |\n|    ent_coef_loss   | 0.0479    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 224320    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 261       |\n|    ep_rew_mean     | -4.46e+03 |\n| time/              |           |\n|    episodes        | 508       |\n|    fps             | 60        |\n|    time_elapsed    | 3734      |\n|    total_timesteps | 226315    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 20.5      |\n|    ent_coef        | 0.277     |\n|    ent_coef_loss   | 0.0456    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 226240    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 276       |\n|    ep_rew_mean     | -4.84e+03 |\n| time/              |           |\n|    episodes        | 512       |\n|    fps             | 60        |\n|    time_elapsed    | 3766      |\n|    total_timesteps | 228228    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.279     |\n|    ent_coef_loss   | 0.145     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 228160    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 276       |\n|    ep_rew_mean     | -4.84e+03 |\n| time/              |           |\n|    episodes        | 516       |\n|    fps             | 60        |\n|    time_elapsed    | 3770      |\n|    total_timesteps | 228544    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.121    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 228416    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 277       |\n|    ep_rew_mean     | -4.85e+03 |\n| time/              |           |\n|    episodes        | 520       |\n|    fps             | 60        |\n|    time_elapsed    | 3777      |\n|    total_timesteps | 228912    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 21.9      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.0959   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 228800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 278       |\n|    ep_rew_mean     | -4.86e+03 |\n| time/              |           |\n|    episodes        | 524       |\n|    fps             | 60        |\n|    time_elapsed    | 3784      |\n|    total_timesteps | 229342    |\n| train/             |           |\n|    actor_loss      | 1.82e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | 0.0182    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 229248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 279       |\n|    ep_rew_mean     | -4.89e+03 |\n| time/              |           |\n|    episodes        | 528       |\n|    fps             | 60        |\n|    time_elapsed    | 3792      |\n|    total_timesteps | 229823    |\n| train/             |           |\n|    actor_loss      | 1.83e+03  |\n|    critic_loss     | 21.6      |\n|    ent_coef        | 0.275     |\n|    ent_coef_loss   | 0.123     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 229696    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 281       |\n|    ep_rew_mean     | -4.91e+03 |\n| time/              |           |\n|    episodes        | 532       |\n|    fps             | 60        |\n|    time_elapsed    | 3801      |\n|    total_timesteps | 230352    |\n| train/             |           |\n|    actor_loss      | 1.84e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | 0.0916    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 230272    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 282       |\n|    ep_rew_mean     | -4.93e+03 |\n| time/              |           |\n|    episodes        | 536       |\n|    fps             | 60        |\n|    time_elapsed    | 3811      |\n|    total_timesteps | 230929    |\n| train/             |           |\n|    actor_loss      | 1.82e+03  |\n|    critic_loss     | 22.1      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | 0.012     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 230848    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 283       |\n|    ep_rew_mean     | -4.95e+03 |\n| time/              |           |\n|    episodes        | 540       |\n|    fps             | 60        |\n|    time_elapsed    | 3821      |\n|    total_timesteps | 231553    |\n| train/             |           |\n|    actor_loss      | 1.82e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | -0.0702   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 231488    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 283       |\n|    ep_rew_mean     | -4.99e+03 |\n| time/              |           |\n|    episodes        | 544       |\n|    fps             | 60        |\n|    time_elapsed    | 3832      |\n|    total_timesteps | 232212    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.4      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | -0.00829  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 232128    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 284       |\n|    ep_rew_mean     | -5.04e+03 |\n| time/              |           |\n|    episodes        | 548       |\n|    fps             | 60        |\n|    time_elapsed    | 3844      |\n|    total_timesteps | 232901    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.271     |\n|    ent_coef_loss   | -0.0331   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 232832    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 283       |\n|    ep_rew_mean     | -5.02e+03 |\n| time/              |           |\n|    episodes        | 552       |\n|    fps             | 60        |\n|    time_elapsed    | 3856      |\n|    total_timesteps | 233606    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.268     |\n|    ent_coef_loss   | -0.156    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 233536    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 275       |\n|    ep_rew_mean     | -4.87e+03 |\n| time/              |           |\n|    episodes        | 556       |\n|    fps             | 60        |\n|    time_elapsed    | 3868      |\n|    total_timesteps | 234348    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.271     |\n|    ent_coef_loss   | -0.086    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 234240    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 275       |\n|    ep_rew_mean     | -4.87e+03 |\n| time/              |           |\n|    episodes        | 560       |\n|    fps             | 60        |\n|    time_elapsed    | 3881      |\n|    total_timesteps | 235184    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.3      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | -0.0135   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 235072    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 266       |\n|    ep_rew_mean     | -4.72e+03 |\n| time/              |           |\n|    episodes        | 564       |\n|    fps             | 60        |\n|    time_elapsed    | 3895      |\n|    total_timesteps | 236000    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.3      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | -0.174    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 235904    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 260       |\n|    ep_rew_mean     | -4.67e+03 |\n| time/              |           |\n|    episodes        | 568       |\n|    fps             | 60        |\n|    time_elapsed    | 3938      |\n|    total_timesteps | 238606    |\n| train/             |           |\n|    actor_loss      | 1.8e+03   |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.0188    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 238528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 256       |\n|    ep_rew_mean     | -4.62e+03 |\n| time/              |           |\n|    episodes        | 572       |\n|    fps             | 60        |\n|    time_elapsed    | 3953      |\n|    total_timesteps | 239499    |\n| train/             |           |\n|    actor_loss      | 1.8e+03   |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.075     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 239424    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 266      |\n|    ep_rew_mean     | -4.8e+03 |\n| time/              |          |\n|    episodes        | 576      |\n|    fps             | 60       |\n|    time_elapsed    | 3980     |\n|    total_timesteps | 241168   |\n| train/             |          |\n|    actor_loss      | 1.82e+03 |\n|    critic_loss     | 21.3     |\n|    ent_coef        | 0.281    |\n|    ent_coef_loss   | 0.0871   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 241088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 265       |\n|    ep_rew_mean     | -4.81e+03 |\n| time/              |           |\n|    episodes        | 580       |\n|    fps             | 60        |\n|    time_elapsed    | 3989      |\n|    total_timesteps | 241693    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.288     |\n|    ent_coef_loss   | 0.121     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 241600    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 267       |\n|    ep_rew_mean     | -4.83e+03 |\n| time/              |           |\n|    episodes        | 584       |\n|    fps             | 60        |\n|    time_elapsed    | 4001      |\n|    total_timesteps | 242482    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.291     |\n|    ent_coef_loss   | -0.00436  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 242368    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 266       |\n|    ep_rew_mean     | -4.84e+03 |\n| time/              |           |\n|    episodes        | 588       |\n|    fps             | 60        |\n|    time_elapsed    | 4008      |\n|    total_timesteps | 242847    |\n| train/             |           |\n|    actor_loss      | 1.82e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.291     |\n|    ent_coef_loss   | -0.0531   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 242752    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 275       |\n|    ep_rew_mean     | -5.09e+03 |\n| time/              |           |\n|    episodes        | 592       |\n|    fps             | 60        |\n|    time_elapsed    | 4035      |\n|    total_timesteps | 244526    |\n| train/             |           |\n|    actor_loss      | 1.81e+03  |\n|    critic_loss     | 20.8      |\n|    ent_coef        | 0.287     |\n|    ent_coef_loss   | -0.0274   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 244416    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 272       |\n|    ep_rew_mean     | -5.04e+03 |\n| time/              |           |\n|    episodes        | 596       |\n|    fps             | 60        |\n|    time_elapsed    | 4042      |\n|    total_timesteps | 244913    |\n| train/             |           |\n|    actor_loss      | 1.8e+03   |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | -0.0126   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 244800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 240       |\n|    ep_rew_mean     | -4.48e+03 |\n| time/              |           |\n|    episodes        | 600       |\n|    fps             | 60        |\n|    time_elapsed    | 4048      |\n|    total_timesteps | 245284    |\n| train/             |           |\n|    actor_loss      | 1.8e+03   |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | 0.0199    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 245184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 212       |\n|    ep_rew_mean     | -3.93e+03 |\n| time/              |           |\n|    episodes        | 604       |\n|    fps             | 60        |\n|    time_elapsed    | 4054      |\n|    total_timesteps | 245632    |\n| train/             |           |\n|    actor_loss      | 1.8e+03   |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | -0.0645   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 245504    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 201       |\n|    ep_rew_mean     | -3.72e+03 |\n| time/              |           |\n|    episodes        | 608       |\n|    fps             | 60        |\n|    time_elapsed    | 4066      |\n|    total_timesteps | 246390    |\n| train/             |           |\n|    actor_loss      | 1.79e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.281     |\n|    ent_coef_loss   | -0.0543   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 246272    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 201       |\n|    ep_rew_mean     | -3.65e+03 |\n| time/              |           |\n|    episodes        | 612       |\n|    fps             | 60        |\n|    time_elapsed    | 4100      |\n|    total_timesteps | 248347    |\n| train/             |           |\n|    actor_loss      | 1.78e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.114    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 248256    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 216       |\n|    ep_rew_mean     | -3.84e+03 |\n| time/              |           |\n|    episodes        | 616       |\n|    fps             | 60        |\n|    time_elapsed    | 4129      |\n|    total_timesteps | 250111    |\n| train/             |           |\n|    actor_loss      | 1.77e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | -0.0247   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 249984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 221       |\n|    ep_rew_mean     | -3.93e+03 |\n| time/              |           |\n|    episodes        | 620       |\n|    fps             | 60        |\n|    time_elapsed    | 4145      |\n|    total_timesteps | 251027    |\n| train/             |           |\n|    actor_loss      | 1.76e+03  |\n|    critic_loss     | 20.4      |\n|    ent_coef        | 0.271     |\n|    ent_coef_loss   | -0.0807   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 250944    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 223       |\n|    ep_rew_mean     | -3.96e+03 |\n| time/              |           |\n|    episodes        | 624       |\n|    fps             | 60        |\n|    time_elapsed    | 4155      |\n|    total_timesteps | 251641    |\n| train/             |           |\n|    actor_loss      | 1.76e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.27      |\n|    ent_coef_loss   | -0.0632   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 251520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 223       |\n|    ep_rew_mean     | -3.94e+03 |\n| time/              |           |\n|    episodes        | 628       |\n|    fps             | 60        |\n|    time_elapsed    | 4162      |\n|    total_timesteps | 252087    |\n| train/             |           |\n|    actor_loss      | 1.76e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | -0.0449   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 251968    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 222       |\n|    ep_rew_mean     | -3.95e+03 |\n| time/              |           |\n|    episodes        | 632       |\n|    fps             | 60        |\n|    time_elapsed    | 4170      |\n|    total_timesteps | 252506    |\n| train/             |           |\n|    actor_loss      | 1.77e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.275     |\n|    ent_coef_loss   | 0.0695    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 252416    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 220       |\n|    ep_rew_mean     | -3.93e+03 |\n| time/              |           |\n|    episodes        | 636       |\n|    fps             | 60        |\n|    time_elapsed    | 4177      |\n|    total_timesteps | 252922    |\n| train/             |           |\n|    actor_loss      | 1.74e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.0738    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 252800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 218       |\n|    ep_rew_mean     | -3.91e+03 |\n| time/              |           |\n|    episodes        | 640       |\n|    fps             | 60        |\n|    time_elapsed    | 4184      |\n|    total_timesteps | 253343    |\n| train/             |           |\n|    actor_loss      | 1.77e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.279     |\n|    ent_coef_loss   | 0.0328    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 253248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 215       |\n|    ep_rew_mean     | -3.86e+03 |\n| time/              |           |\n|    episodes        | 644       |\n|    fps             | 60        |\n|    time_elapsed    | 4191      |\n|    total_timesteps | 253734    |\n| train/             |           |\n|    actor_loss      | 1.75e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.28      |\n|    ent_coef_loss   | 0.0215    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 253632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 213       |\n|    ep_rew_mean     | -3.81e+03 |\n| time/              |           |\n|    episodes        | 648       |\n|    fps             | 60        |\n|    time_elapsed    | 4198      |\n|    total_timesteps | 254170    |\n| train/             |           |\n|    actor_loss      | 1.74e+03  |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.00777  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 254080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 210       |\n|    ep_rew_mean     | -3.78e+03 |\n| time/              |           |\n|    episodes        | 652       |\n|    fps             | 60        |\n|    time_elapsed    | 4206      |\n|    total_timesteps | 254598    |\n| train/             |           |\n|    actor_loss      | 1.75e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.28      |\n|    ent_coef_loss   | -0.057    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 254528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 207       |\n|    ep_rew_mean     | -3.72e+03 |\n| time/              |           |\n|    episodes        | 656       |\n|    fps             | 60        |\n|    time_elapsed    | 4213      |\n|    total_timesteps | 255030    |\n| train/             |           |\n|    actor_loss      | 1.75e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.279     |\n|    ent_coef_loss   | -0.0119   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 254912    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 203       |\n|    ep_rew_mean     | -3.66e+03 |\n| time/              |           |\n|    episodes        | 660       |\n|    fps             | 60        |\n|    time_elapsed    | 4221      |\n|    total_timesteps | 255502    |\n| train/             |           |\n|    actor_loss      | 1.75e+03  |\n|    critic_loss     | 22.2      |\n|    ent_coef        | 0.277     |\n|    ent_coef_loss   | -0.179    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 255424    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 200       |\n|    ep_rew_mean     | -3.62e+03 |\n| time/              |           |\n|    episodes        | 664       |\n|    fps             | 60        |\n|    time_elapsed    | 4230      |\n|    total_timesteps | 256014    |\n| train/             |           |\n|    actor_loss      | 1.73e+03  |\n|    critic_loss     | 20.8      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | 0.0446    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 255936    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 178       |\n|    ep_rew_mean     | -3.17e+03 |\n| time/              |           |\n|    episodes        | 668       |\n|    fps             | 60        |\n|    time_elapsed    | 4236      |\n|    total_timesteps | 256431    |\n| train/             |           |\n|    actor_loss      | 1.73e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | -0.099    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 256320    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 174      |\n|    ep_rew_mean     | -3.1e+03 |\n| time/              |          |\n|    episodes        | 672      |\n|    fps             | 60       |\n|    time_elapsed    | 4244     |\n|    total_timesteps | 256883   |\n| train/             |          |\n|    actor_loss      | 1.74e+03 |\n|    critic_loss     | 20.8     |\n|    ent_coef        | 0.273    |\n|    ent_coef_loss   | -0.0378  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 256768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 161       |\n|    ep_rew_mean     | -2.87e+03 |\n| time/              |           |\n|    episodes        | 676       |\n|    fps             | 60        |\n|    time_elapsed    | 4252      |\n|    total_timesteps | 257296    |\n| train/             |           |\n|    actor_loss      | 1.73e+03  |\n|    critic_loss     | 21.8      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | 0.0438    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 257216    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 160       |\n|    ep_rew_mean     | -2.83e+03 |\n| time/              |           |\n|    episodes        | 680       |\n|    fps             | 60        |\n|    time_elapsed    | 4258      |\n|    total_timesteps | 257713    |\n| train/             |           |\n|    actor_loss      | 1.72e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.275     |\n|    ent_coef_loss   | -0.0202   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 257600    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 157       |\n|    ep_rew_mean     | -2.78e+03 |\n| time/              |           |\n|    episodes        | 684       |\n|    fps             | 60        |\n|    time_elapsed    | 4266      |\n|    total_timesteps | 258136    |\n| train/             |           |\n|    actor_loss      | 1.72e+03  |\n|    critic_loss     | 20.5      |\n|    ent_coef        | 0.275     |\n|    ent_coef_loss   | -0.0234   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 258048    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 157       |\n|    ep_rew_mean     | -2.77e+03 |\n| time/              |           |\n|    episodes        | 688       |\n|    fps             | 60        |\n|    time_elapsed    | 4272      |\n|    total_timesteps | 258545    |\n| train/             |           |\n|    actor_loss      | 1.72e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.276     |\n|    ent_coef_loss   | -0.0639   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 258432    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 145       |\n|    ep_rew_mean     | -2.48e+03 |\n| time/              |           |\n|    episodes        | 692       |\n|    fps             | 60        |\n|    time_elapsed    | 4280      |\n|    total_timesteps | 258986    |\n| train/             |           |\n|    actor_loss      | 1.73e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.281     |\n|    ent_coef_loss   | 0.142     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 258880    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 144       |\n|    ep_rew_mean     | -2.46e+03 |\n| time/              |           |\n|    episodes        | 696       |\n|    fps             | 60        |\n|    time_elapsed    | 4286      |\n|    total_timesteps | 259341    |\n| train/             |           |\n|    actor_loss      | 1.71e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.283     |\n|    ent_coef_loss   | -0.0628   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 259264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 145       |\n|    ep_rew_mean     | -2.45e+03 |\n| time/              |           |\n|    episodes        | 700       |\n|    fps             | 60        |\n|    time_elapsed    | 4293      |\n|    total_timesteps | 259770    |\n| train/             |           |\n|    actor_loss      | 1.72e+03  |\n|    critic_loss     | 20.8      |\n|    ent_coef        | 0.282     |\n|    ent_coef_loss   | -0.0505   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 259648    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 146       |\n|    ep_rew_mean     | -2.46e+03 |\n| time/              |           |\n|    episodes        | 704       |\n|    fps             | 60        |\n|    time_elapsed    | 4301      |\n|    total_timesteps | 260182    |\n| train/             |           |\n|    actor_loss      | 1.7e+03   |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.284     |\n|    ent_coef_loss   | -0.0494   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 260096    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 141       |\n|    ep_rew_mean     | -2.37e+03 |\n| time/              |           |\n|    episodes        | 708       |\n|    fps             | 60        |\n|    time_elapsed    | 4305      |\n|    total_timesteps | 260469    |\n| train/             |           |\n|    actor_loss      | 1.69e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.286     |\n|    ent_coef_loss   | -0.035    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 260352    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 125       |\n|    ep_rew_mean     | -2.06e+03 |\n| time/              |           |\n|    episodes        | 712       |\n|    fps             | 60        |\n|    time_elapsed    | 4312      |\n|    total_timesteps | 260825    |\n| train/             |           |\n|    actor_loss      | 1.69e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.291     |\n|    ent_coef_loss   | 0.062     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 260736    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 110       |\n|    ep_rew_mean     | -1.87e+03 |\n| time/              |           |\n|    episodes        | 716       |\n|    fps             | 60        |\n|    time_elapsed    | 4317      |\n|    total_timesteps | 261134    |\n| train/             |           |\n|    actor_loss      | 1.69e+03  |\n|    critic_loss     | 21.8      |\n|    ent_coef        | 0.293     |\n|    ent_coef_loss   | 0.048     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 261056    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 104       |\n|    ep_rew_mean     | -1.77e+03 |\n| time/              |           |\n|    episodes        | 720       |\n|    fps             | 60        |\n|    time_elapsed    | 4322      |\n|    total_timesteps | 261423    |\n| train/             |           |\n|    actor_loss      | 1.68e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.297     |\n|    ent_coef_loss   | 0.00339   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 261312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 101       |\n|    ep_rew_mean     | -1.72e+03 |\n| time/              |           |\n|    episodes        | 724       |\n|    fps             | 60        |\n|    time_elapsed    | 4327      |\n|    total_timesteps | 261703    |\n| train/             |           |\n|    actor_loss      | 1.68e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.301     |\n|    ent_coef_loss   | 0.0938    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 261632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 99       |\n|    ep_rew_mean     | -1.7e+03 |\n| time/              |          |\n|    episodes        | 728      |\n|    fps             | 60       |\n|    time_elapsed    | 4332     |\n|    total_timesteps | 261986   |\n| train/             |          |\n|    actor_loss      | 1.68e+03 |\n|    critic_loss     | 21.3     |\n|    ent_coef        | 0.302    |\n|    ent_coef_loss   | -0.0447  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 261888   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 97.6      |\n|    ep_rew_mean     | -1.66e+03 |\n| time/              |           |\n|    episodes        | 732       |\n|    fps             | 60        |\n|    time_elapsed    | 4336      |\n|    total_timesteps | 262269    |\n| train/             |           |\n|    actor_loss      | 1.68e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.304     |\n|    ent_coef_loss   | 0.0767    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 262144    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 96.2      |\n|    ep_rew_mean     | -1.63e+03 |\n| time/              |           |\n|    episodes        | 736       |\n|    fps             | 60        |\n|    time_elapsed    | 4341      |\n|    total_timesteps | 262547    |\n| train/             |           |\n|    actor_loss      | 1.66e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.309     |\n|    ent_coef_loss   | -0.0749   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 262464    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 94.9      |\n|    ep_rew_mean     | -1.59e+03 |\n| time/              |           |\n|    episodes        | 740       |\n|    fps             | 60        |\n|    time_elapsed    | 4346      |\n|    total_timesteps | 262831    |\n| train/             |           |\n|    actor_loss      | 1.66e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.309     |\n|    ent_coef_loss   | 0.122     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 262720    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 94        |\n|    ep_rew_mean     | -1.58e+03 |\n| time/              |           |\n|    episodes        | 744       |\n|    fps             | 60        |\n|    time_elapsed    | 4351      |\n|    total_timesteps | 263129    |\n| train/             |           |\n|    actor_loss      | 1.66e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.313     |\n|    ent_coef_loss   | 0.13      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 263040    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 92.5      |\n|    ep_rew_mean     | -1.54e+03 |\n| time/              |           |\n|    episodes        | 748       |\n|    fps             | 60        |\n|    time_elapsed    | 4356      |\n|    total_timesteps | 263421    |\n| train/             |           |\n|    actor_loss      | 1.64e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.314     |\n|    ent_coef_loss   | -0.00488  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 263296    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 91.1      |\n|    ep_rew_mean     | -1.51e+03 |\n| time/              |           |\n|    episodes        | 752       |\n|    fps             | 60        |\n|    time_elapsed    | 4361      |\n|    total_timesteps | 263712    |\n| train/             |           |\n|    actor_loss      | 1.65e+03  |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.314     |\n|    ent_coef_loss   | -0.0896   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 263616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 89.8      |\n|    ep_rew_mean     | -1.49e+03 |\n| time/              |           |\n|    episodes        | 756       |\n|    fps             | 60        |\n|    time_elapsed    | 4367      |\n|    total_timesteps | 264011    |\n| train/             |           |\n|    actor_loss      | 1.63e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.312     |\n|    ent_coef_loss   | -0.0223   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 263936    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 88.1      |\n|    ep_rew_mean     | -1.46e+03 |\n| time/              |           |\n|    episodes        | 760       |\n|    fps             | 60        |\n|    time_elapsed    | 4371      |\n|    total_timesteps | 264311    |\n| train/             |           |\n|    actor_loss      | 1.62e+03  |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.311     |\n|    ent_coef_loss   | -0.0424   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 264192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 86        |\n|    ep_rew_mean     | -1.42e+03 |\n| time/              |           |\n|    episodes        | 764       |\n|    fps             | 60        |\n|    time_elapsed    | 4377      |\n|    total_timesteps | 264619    |\n| train/             |           |\n|    actor_loss      | 1.61e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.309     |\n|    ent_coef_loss   | 0.0618    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 264512    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 85        |\n|    ep_rew_mean     | -1.39e+03 |\n| time/              |           |\n|    episodes        | 768       |\n|    fps             | 60        |\n|    time_elapsed    | 4382      |\n|    total_timesteps | 264929    |\n| train/             |           |\n|    actor_loss      | 1.61e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.304     |\n|    ent_coef_loss   | -0.055    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 264832    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 83.6      |\n|    ep_rew_mean     | -1.36e+03 |\n| time/              |           |\n|    episodes        | 772       |\n|    fps             | 60        |\n|    time_elapsed    | 4388      |\n|    total_timesteps | 265240    |\n| train/             |           |\n|    actor_loss      | 1.6e+03   |\n|    critic_loss     | 20.8      |\n|    ent_coef        | 0.303     |\n|    ent_coef_loss   | -0.0494   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 265152    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 82.4      |\n|    ep_rew_mean     | -1.34e+03 |\n| time/              |           |\n|    episodes        | 776       |\n|    fps             | 60        |\n|    time_elapsed    | 4393      |\n|    total_timesteps | 265537    |\n| train/             |           |\n|    actor_loss      | 1.61e+03  |\n|    critic_loss     | 21.2      |\n|    ent_coef        | 0.303     |\n|    ent_coef_loss   | -0.0448   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 265472    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 81.3      |\n|    ep_rew_mean     | -1.33e+03 |\n| time/              |           |\n|    episodes        | 780       |\n|    fps             | 60        |\n|    time_elapsed    | 4398      |\n|    total_timesteps | 265846    |\n| train/             |           |\n|    actor_loss      | 1.6e+03   |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.3       |\n|    ent_coef_loss   | -0.00424  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 265728    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 80.1      |\n|    ep_rew_mean     | -1.31e+03 |\n| time/              |           |\n|    episodes        | 784       |\n|    fps             | 60        |\n|    time_elapsed    | 4403      |\n|    total_timesteps | 266144    |\n| train/             |           |\n|    actor_loss      | 1.6e+03   |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.301     |\n|    ent_coef_loss   | 0.0084    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 266048    |\n----------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 78.8      |\n|    ep_rew_mean     | -1.27e+03 |\n| time/              |           |\n|    episodes        | 788       |\n|    fps             | 60        |\n|    time_elapsed    | 4408      |\n|    total_timesteps | 266424    |\n| train/             |           |\n|    actor_loss      | 1.6e+03   |\n|    critic_loss     | 22.1      |\n|    ent_coef        | 0.301     |\n|    ent_coef_loss   | 0.075     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 266304    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 77.3      |\n|    ep_rew_mean     | -1.24e+03 |\n| time/              |           |\n|    episodes        | 792       |\n|    fps             | 60        |\n|    time_elapsed    | 4413      |\n|    total_timesteps | 266712    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.298     |\n|    ent_coef_loss   | -0.0444   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 266624    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 76.6      |\n|    ep_rew_mean     | -1.21e+03 |\n| time/              |           |\n|    episodes        | 796       |\n|    fps             | 60        |\n|    time_elapsed    | 4417      |\n|    total_timesteps | 267001    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 21.8      |\n|    ent_coef        | 0.295     |\n|    ent_coef_loss   | -0.0474   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 266880    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 75        |\n|    ep_rew_mean     | -1.18e+03 |\n| time/              |           |\n|    episodes        | 800       |\n|    fps             | 60        |\n|    time_elapsed    | 4423      |\n|    total_timesteps | 267271    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.292     |\n|    ent_coef_loss   | -0.0227   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 267200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 73.6      |\n|    ep_rew_mean     | -1.15e+03 |\n| time/              |           |\n|    episodes        | 804       |\n|    fps             | 60        |\n|    time_elapsed    | 4427      |\n|    total_timesteps | 267539    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 21.7      |\n|    ent_coef        | 0.29      |\n|    ent_coef_loss   | 0.00682   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 267456    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 73.4      |\n|    ep_rew_mean     | -1.14e+03 |\n| time/              |           |\n|    episodes        | 808       |\n|    fps             | 60        |\n|    time_elapsed    | 4432      |\n|    total_timesteps | 267806    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 22        |\n|    ent_coef        | 0.29      |\n|    ent_coef_loss   | -0.109    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 267712    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.6      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 812       |\n|    fps             | 60        |\n|    time_elapsed    | 4436      |\n|    total_timesteps | 268081    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 21.5      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | -0.101    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 267968    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.2      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 816       |\n|    fps             | 60        |\n|    time_elapsed    | 4441      |\n|    total_timesteps | 268358    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 21.9      |\n|    ent_coef        | 0.284     |\n|    ent_coef_loss   | 0.0852    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 268288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.2      |\n|    ep_rew_mean     | -1.14e+03 |\n| time/              |           |\n|    episodes        | 820       |\n|    fps             | 60        |\n|    time_elapsed    | 4446      |\n|    total_timesteps | 268641    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 21.8      |\n|    ent_coef        | 0.283     |\n|    ent_coef_loss   | -0.0554   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 268544    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.1      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 824       |\n|    fps             | 60        |\n|    time_elapsed    | 4450      |\n|    total_timesteps | 268915    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.0154    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 268800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.1      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 828       |\n|    fps             | 60        |\n|    time_elapsed    | 4456      |\n|    total_timesteps | 269197    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | 0.0199    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 269120    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.1      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 832       |\n|    fps             | 60        |\n|    time_elapsed    | 4460      |\n|    total_timesteps | 269478    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 21        |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.146    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 269376    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.1      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 836       |\n|    fps             | 60        |\n|    time_elapsed    | 4465      |\n|    total_timesteps | 269754    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 21.5      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | 0.0553    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 269632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72        |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 840       |\n|    fps             | 60        |\n|    time_elapsed    | 4470      |\n|    total_timesteps | 270034    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.274     |\n|    ent_coef_loss   | -0.0197   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 269952    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71.8      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 844       |\n|    fps             | 60        |\n|    time_elapsed    | 4474      |\n|    total_timesteps | 270314    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | -0.0657   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 270208    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71.8      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 848       |\n|    fps             | 60        |\n|    time_elapsed    | 4480      |\n|    total_timesteps | 270600    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.272     |\n|    ent_coef_loss   | 0.0511    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 270528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71.8      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 852       |\n|    fps             | 60        |\n|    time_elapsed    | 4484      |\n|    total_timesteps | 270893    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 21.3      |\n|    ent_coef        | 0.271     |\n|    ent_coef_loss   | 0.0962    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 270784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71.9      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 856       |\n|    fps             | 60        |\n|    time_elapsed    | 4490      |\n|    total_timesteps | 271205    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.269     |\n|    ent_coef_loss   | -0.14     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 271104    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72        |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 860       |\n|    fps             | 60        |\n|    time_elapsed    | 4495      |\n|    total_timesteps | 271508    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 21.4      |\n|    ent_coef        | 0.269     |\n|    ent_coef_loss   | 0.13      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 271424    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.2      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 864       |\n|    fps             | 60        |\n|    time_elapsed    | 4500      |\n|    total_timesteps | 271837    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 20.3      |\n|    ent_coef        | 0.27      |\n|    ent_coef_loss   | 0.00343   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 271744    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.8      |\n|    ep_rew_mean     | -1.15e+03 |\n| time/              |           |\n|    episodes        | 868       |\n|    fps             | 60        |\n|    time_elapsed    | 4507      |\n|    total_timesteps | 272209    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.27      |\n|    ent_coef_loss   | 0.0232    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 272128    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 73.7      |\n|    ep_rew_mean     | -1.16e+03 |\n| time/              |           |\n|    episodes        | 872       |\n|    fps             | 60        |\n|    time_elapsed    | 4513      |\n|    total_timesteps | 272614    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 20.9      |\n|    ent_coef        | 0.269     |\n|    ent_coef_loss   | -0.0715   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 272512    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 74.4      |\n|    ep_rew_mean     | -1.17e+03 |\n| time/              |           |\n|    episodes        | 876       |\n|    fps             | 60        |\n|    time_elapsed    | 4519      |\n|    total_timesteps | 272976    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 21.1      |\n|    ent_coef        | 0.269     |\n|    ent_coef_loss   | -0.0712   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 272896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 75.8     |\n|    ep_rew_mean     | -1.2e+03 |\n| time/              |          |\n|    episodes        | 880      |\n|    fps             | 60       |\n|    time_elapsed    | 4527     |\n|    total_timesteps | 273430   |\n| train/             |          |\n|    actor_loss      | 1.58e+03 |\n|    critic_loss     | 20.5     |\n|    ent_coef        | 0.267    |\n|    ent_coef_loss   | 0.0232   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 273344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 76.7      |\n|    ep_rew_mean     | -1.21e+03 |\n| time/              |           |\n|    episodes        | 884       |\n|    fps             | 60        |\n|    time_elapsed    | 4533      |\n|    total_timesteps | 273812    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 20.7      |\n|    ent_coef        | 0.267     |\n|    ent_coef_loss   | -0.0415   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 273728    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 80.1      |\n|    ep_rew_mean     | -1.26e+03 |\n| time/              |           |\n|    episodes        | 888       |\n|    fps             | 60        |\n|    time_elapsed    | 4544      |\n|    total_timesteps | 274438    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 20.6      |\n|    ent_coef        | 0.265     |\n|    ent_coef_loss   | -0.00861  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 274368    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 86.8      |\n|    ep_rew_mean     | -1.37e+03 |\n| time/              |           |\n|    episodes        | 892       |\n|    fps             | 60        |\n|    time_elapsed    | 4559      |\n|    total_timesteps | 275392    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 19.8      |\n|    ent_coef        | 0.265     |\n|    ent_coef_loss   | -0.00186  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 275264    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 93.6      |\n|    ep_rew_mean     | -1.51e+03 |\n| time/              |           |\n|    episodes        | 896       |\n|    fps             | 60        |\n|    time_elapsed    | 4576      |\n|    total_timesteps | 276357    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.9      |\n|    ent_coef        | 0.26      |\n|    ent_coef_loss   | -0.144    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 276288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 102       |\n|    ep_rew_mean     | -1.67e+03 |\n| time/              |           |\n|    episodes        | 900       |\n|    fps             | 60        |\n|    time_elapsed    | 4594      |\n|    total_timesteps | 277498    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.7      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | -0.135    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 277376    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 113       |\n|    ep_rew_mean     | -1.83e+03 |\n| time/              |           |\n|    episodes        | 904       |\n|    fps             | 60        |\n|    time_elapsed    | 4616      |\n|    total_timesteps | 278832    |\n| train/             |           |\n|    actor_loss      | 1.6e+03   |\n|    critic_loss     | 20.3      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | 0.143     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 278720    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 121       |\n|    ep_rew_mean     | -1.96e+03 |\n| time/              |           |\n|    episodes        | 908       |\n|    fps             | 60        |\n|    time_elapsed    | 4633      |\n|    total_timesteps | 279867    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.252     |\n|    ent_coef_loss   | 0.0552    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 279744    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 129       |\n|    ep_rew_mean     | -2.07e+03 |\n| time/              |           |\n|    episodes        | 912       |\n|    fps             | 60        |\n|    time_elapsed    | 4652      |\n|    total_timesteps | 280986    |\n| train/             |           |\n|    actor_loss      | 1.59e+03  |\n|    critic_loss     | 19.8      |\n|    ent_coef        | 0.255     |\n|    ent_coef_loss   | 0.0629    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 280896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 132       |\n|    ep_rew_mean     | -2.12e+03 |\n| time/              |           |\n|    episodes        | 916       |\n|    fps             | 60        |\n|    time_elapsed    | 4663      |\n|    total_timesteps | 281602    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.255     |\n|    ent_coef_loss   | -0.063    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 281536    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 140       |\n|    ep_rew_mean     | -2.23e+03 |\n| time/              |           |\n|    episodes        | 920       |\n|    fps             | 60        |\n|    time_elapsed    | 4680      |\n|    total_timesteps | 282641    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.8      |\n|    ent_coef        | 0.254     |\n|    ent_coef_loss   | 0.0751    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 282560    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 145       |\n|    ep_rew_mean     | -2.33e+03 |\n| time/              |           |\n|    episodes        | 924       |\n|    fps             | 60        |\n|    time_elapsed    | 4693      |\n|    total_timesteps | 283453    |\n| train/             |           |\n|    actor_loss      | 1.58e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.255     |\n|    ent_coef_loss   | 0.0976    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 283328    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 160       |\n|    ep_rew_mean     | -2.55e+03 |\n| time/              |           |\n|    episodes        | 928       |\n|    fps             | 60        |\n|    time_elapsed    | 4722      |\n|    total_timesteps | 285238    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 19.4      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | -0.000891 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 285120    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 165       |\n|    ep_rew_mean     | -2.63e+03 |\n| time/              |           |\n|    episodes        | 932       |\n|    fps             | 60        |\n|    time_elapsed    | 4735      |\n|    total_timesteps | 285960    |\n| train/             |           |\n|    actor_loss      | 1.57e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | -0.0672   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 285888    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 173       |\n|    ep_rew_mean     | -2.74e+03 |\n| time/              |           |\n|    episodes        | 936       |\n|    fps             | 60        |\n|    time_elapsed    | 4752      |\n|    total_timesteps | 287018    |\n| train/             |           |\n|    actor_loss      | 1.56e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.259     |\n|    ent_coef_loss   | -0.152    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 286912    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 176      |\n|    ep_rew_mean     | -2.8e+03 |\n| time/              |          |\n|    episodes        | 940      |\n|    fps             | 60       |\n|    time_elapsed    | 4763     |\n|    total_timesteps | 287679   |\n| train/             |          |\n|    actor_loss      | 1.55e+03 |\n|    critic_loss     | 18.7     |\n|    ent_coef        | 0.26     |\n|    ent_coef_loss   | 0.0175   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 287552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 183      |\n|    ep_rew_mean     | -2.9e+03 |\n| time/              |          |\n|    episodes        | 944      |\n|    fps             | 60       |\n|    time_elapsed    | 4780     |\n|    total_timesteps | 288659   |\n| train/             |          |\n|    actor_loss      | 1.53e+03 |\n|    critic_loss     | 19       |\n|    ent_coef        | 0.268    |\n|    ent_coef_loss   | -0.159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 288576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 187       |\n|    ep_rew_mean     | -2.95e+03 |\n| time/              |           |\n|    episodes        | 948       |\n|    fps             | 60        |\n|    time_elapsed    | 4791      |\n|    total_timesteps | 289328    |\n| train/             |           |\n|    actor_loss      | 1.54e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | 0.104     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 289216    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 187       |\n|    ep_rew_mean     | -2.96e+03 |\n| time/              |           |\n|    episodes        | 952       |\n|    fps             | 60        |\n|    time_elapsed    | 4797      |\n|    total_timesteps | 289627    |\n| train/             |           |\n|    actor_loss      | 1.54e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.278     |\n|    ent_coef_loss   | -0.0575   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 289536    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 187       |\n|    ep_rew_mean     | -2.95e+03 |\n| time/              |           |\n|    episodes        | 956       |\n|    fps             | 60        |\n|    time_elapsed    | 4801      |\n|    total_timesteps | 289891    |\n| train/             |           |\n|    actor_loss      | 1.54e+03  |\n|    critic_loss     | 18.5      |\n|    ent_coef        | 0.279     |\n|    ent_coef_loss   | 0.123     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 289792    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 186       |\n|    ep_rew_mean     | -2.94e+03 |\n| time/              |           |\n|    episodes        | 960       |\n|    fps             | 60        |\n|    time_elapsed    | 4806      |\n|    total_timesteps | 290153    |\n| train/             |           |\n|    actor_loss      | 1.52e+03  |\n|    critic_loss     | 19.4      |\n|    ent_coef        | 0.282     |\n|    ent_coef_loss   | 0.0862    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 290048    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 186       |\n|    ep_rew_mean     | -2.94e+03 |\n| time/              |           |\n|    episodes        | 964       |\n|    fps             | 60        |\n|    time_elapsed    | 4810      |\n|    total_timesteps | 290417    |\n| train/             |           |\n|    actor_loss      | 1.53e+03  |\n|    critic_loss     | 20        |\n|    ent_coef        | 0.288     |\n|    ent_coef_loss   | 0.0308    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 290304    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 185       |\n|    ep_rew_mean     | -2.91e+03 |\n| time/              |           |\n|    episodes        | 968       |\n|    fps             | 60        |\n|    time_elapsed    | 4815      |\n|    total_timesteps | 290680    |\n| train/             |           |\n|    actor_loss      | 1.53e+03  |\n|    critic_loss     | 18.9      |\n|    ent_coef        | 0.292     |\n|    ent_coef_loss   | 0.0486    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 290560    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 183       |\n|    ep_rew_mean     | -2.89e+03 |\n| time/              |           |\n|    episodes        | 972       |\n|    fps             | 60        |\n|    time_elapsed    | 4820      |\n|    total_timesteps | 290949    |\n| train/             |           |\n|    actor_loss      | 1.52e+03  |\n|    critic_loss     | 19.2      |\n|    ent_coef        | 0.296     |\n|    ent_coef_loss   | 0.0795    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 290880    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 182       |\n|    ep_rew_mean     | -2.88e+03 |\n| time/              |           |\n|    episodes        | 976       |\n|    fps             | 60        |\n|    time_elapsed    | 4825      |\n|    total_timesteps | 291218    |\n| train/             |           |\n|    actor_loss      | 1.51e+03  |\n|    critic_loss     | 19.2      |\n|    ent_coef        | 0.297     |\n|    ent_coef_loss   | 0.0466    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 291136    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 181       |\n|    ep_rew_mean     | -2.86e+03 |\n| time/              |           |\n|    episodes        | 980       |\n|    fps             | 60        |\n|    time_elapsed    | 4829      |\n|    total_timesteps | 291494    |\n| train/             |           |\n|    actor_loss      | 1.5e+03   |\n|    critic_loss     | 19.8      |\n|    ent_coef        | 0.298     |\n|    ent_coef_loss   | -0.000703 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 291392    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 180       |\n|    ep_rew_mean     | -2.83e+03 |\n| time/              |           |\n|    episodes        | 984       |\n|    fps             | 60        |\n|    time_elapsed    | 4834      |\n|    total_timesteps | 291765    |\n| train/             |           |\n|    actor_loss      | 1.48e+03  |\n|    critic_loss     | 19.5      |\n|    ent_coef        | 0.299     |\n|    ent_coef_loss   | -0.0447   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 291648    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 176       |\n|    ep_rew_mean     | -2.78e+03 |\n| time/              |           |\n|    episodes        | 988       |\n|    fps             | 60        |\n|    time_elapsed    | 4839      |\n|    total_timesteps | 292037    |\n| train/             |           |\n|    actor_loss      | 1.48e+03  |\n|    critic_loss     | 19        |\n|    ent_coef        | 0.298     |\n|    ent_coef_loss   | -0.0229   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 291968    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 169       |\n|    ep_rew_mean     | -2.67e+03 |\n| time/              |           |\n|    episodes        | 992       |\n|    fps             | 60        |\n|    time_elapsed    | 4844      |\n|    total_timesteps | 292309    |\n| train/             |           |\n|    actor_loss      | 1.49e+03  |\n|    critic_loss     | 19.7      |\n|    ent_coef        | 0.294     |\n|    ent_coef_loss   | -0.0583   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 292224    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 162       |\n|    ep_rew_mean     | -2.53e+03 |\n| time/              |           |\n|    episodes        | 996       |\n|    fps             | 60        |\n|    time_elapsed    | 4848      |\n|    total_timesteps | 292581    |\n| train/             |           |\n|    actor_loss      | 1.48e+03  |\n|    critic_loss     | 19.7      |\n|    ent_coef        | 0.29      |\n|    ent_coef_loss   | -0.0146   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 292480    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 154       |\n|    ep_rew_mean     | -2.36e+03 |\n| time/              |           |\n|    episodes        | 1000      |\n|    fps             | 60        |\n|    time_elapsed    | 4853      |\n|    total_timesteps | 292849    |\n| train/             |           |\n|    actor_loss      | 1.47e+03  |\n|    critic_loss     | 19.4      |\n|    ent_coef        | 0.287     |\n|    ent_coef_loss   | -0.0044   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 292736    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 143      |\n|    ep_rew_mean     | -2.2e+03 |\n| time/              |          |\n|    episodes        | 1004     |\n|    fps             | 60       |\n|    time_elapsed    | 4857     |\n|    total_timesteps | 293117   |\n| train/             |          |\n|    actor_loss      | 1.48e+03 |\n|    critic_loss     | 19.8     |\n|    ent_coef        | 0.286    |\n|    ent_coef_loss   | 0.00399  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 292992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 135       |\n|    ep_rew_mean     | -2.08e+03 |\n| time/              |           |\n|    episodes        | 1008      |\n|    fps             | 60        |\n|    time_elapsed    | 4862      |\n|    total_timesteps | 293385    |\n| train/             |           |\n|    actor_loss      | 1.47e+03  |\n|    critic_loss     | 20.3      |\n|    ent_coef        | 0.285     |\n|    ent_coef_loss   | 0.00601   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 293312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 127       |\n|    ep_rew_mean     | -1.97e+03 |\n| time/              |           |\n|    episodes        | 1012      |\n|    fps             | 60        |\n|    time_elapsed    | 4867      |\n|    total_timesteps | 293656    |\n| train/             |           |\n|    actor_loss      | 1.47e+03  |\n|    critic_loss     | 19.9      |\n|    ent_coef        | 0.281     |\n|    ent_coef_loss   | -0.151    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 293568    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 123       |\n|    ep_rew_mean     | -1.92e+03 |\n| time/              |           |\n|    episodes        | 1016      |\n|    fps             | 60        |\n|    time_elapsed    | 4871      |\n|    total_timesteps | 293925    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.5      |\n|    ent_coef        | 0.277     |\n|    ent_coef_loss   | 0.00894   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 293824    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 115       |\n|    ep_rew_mean     | -1.81e+03 |\n| time/              |           |\n|    episodes        | 1020      |\n|    fps             | 60        |\n|    time_elapsed    | 4876      |\n|    total_timesteps | 294190    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.7      |\n|    ent_coef        | 0.277     |\n|    ent_coef_loss   | -0.201    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 294080    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 110       |\n|    ep_rew_mean     | -1.72e+03 |\n| time/              |           |\n|    episodes        | 1024      |\n|    fps             | 60        |\n|    time_elapsed    | 4880      |\n|    total_timesteps | 294459    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.273     |\n|    ent_coef_loss   | 0.0118    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 294336    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 94.9      |\n|    ep_rew_mean     | -1.49e+03 |\n| time/              |           |\n|    episodes        | 1028      |\n|    fps             | 60        |\n|    time_elapsed    | 4886      |\n|    total_timesteps | 294726    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.27      |\n|    ent_coef_loss   | -0.181    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 294656    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 90.3      |\n|    ep_rew_mean     | -1.41e+03 |\n| time/              |           |\n|    episodes        | 1032      |\n|    fps             | 60        |\n|    time_elapsed    | 4890      |\n|    total_timesteps | 294991    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 20.2      |\n|    ent_coef        | 0.268     |\n|    ent_coef_loss   | -0.0774   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 294912    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 82.5      |\n|    ep_rew_mean     | -1.29e+03 |\n| time/              |           |\n|    episodes        | 1036      |\n|    fps             | 60        |\n|    time_elapsed    | 4894      |\n|    total_timesteps | 295263    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19        |\n|    ent_coef        | 0.263     |\n|    ent_coef_loss   | -0.0802   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 295168    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 78.5      |\n|    ep_rew_mean     | -1.23e+03 |\n| time/              |           |\n|    episodes        | 1040      |\n|    fps             | 60        |\n|    time_elapsed    | 4899      |\n|    total_timesteps | 295533    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.5      |\n|    ent_coef        | 0.26      |\n|    ent_coef_loss   | -0.124    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 295424    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71.4      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 1044      |\n|    fps             | 60        |\n|    time_elapsed    | 4903      |\n|    total_timesteps | 295803    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 19.4      |\n|    ent_coef        | 0.257     |\n|    ent_coef_loss   | -0.0814   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 295680    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1048      |\n|    fps             | 60        |\n|    time_elapsed    | 4909      |\n|    total_timesteps | 296084    |\n| train/             |           |\n|    actor_loss      | 1.47e+03  |\n|    critic_loss     | 19.6      |\n|    ent_coef        | 0.249     |\n|    ent_coef_loss   | 0.216     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 296000    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.4      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1052      |\n|    fps             | 60        |\n|    time_elapsed    | 4913      |\n|    total_timesteps | 296368    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 18.8      |\n|    ent_coef        | 0.253     |\n|    ent_coef_loss   | 0.0461    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 296256    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1056      |\n|    fps             | 60        |\n|    time_elapsed    | 4919      |\n|    total_timesteps | 296648    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.6      |\n|    ent_coef        | 0.252     |\n|    ent_coef_loss   | -0.0848   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 296576    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.8      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1060      |\n|    fps             | 60        |\n|    time_elapsed    | 4923      |\n|    total_timesteps | 296928    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.7      |\n|    ent_coef        | 0.248     |\n|    ent_coef_loss   | -0.102    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 296832    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.9      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1064      |\n|    fps             | 60        |\n|    time_elapsed    | 4928      |\n|    total_timesteps | 297204    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.245     |\n|    ent_coef_loss   | -0.0693   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 297088    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68        |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1068      |\n|    fps             | 60        |\n|    time_elapsed    | 4933      |\n|    total_timesteps | 297483    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.7      |\n|    ent_coef        | 0.243     |\n|    ent_coef_loss   | -0.00582  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 297408    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1072      |\n|    fps             | 60        |\n|    time_elapsed    | 4937      |\n|    total_timesteps | 297760    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.24      |\n|    ent_coef_loss   | -0.0787   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 297664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.3      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1076      |\n|    fps             | 60        |\n|    time_elapsed    | 4942      |\n|    total_timesteps | 298045    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.238     |\n|    ent_coef_loss   | 0.0823    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 297920    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.3      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 1080      |\n|    fps             | 60        |\n|    time_elapsed    | 4947      |\n|    total_timesteps | 298324    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.9      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.0245    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 298240    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.4      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 1084      |\n|    fps             | 60        |\n|    time_elapsed    | 4952      |\n|    total_timesteps | 298602    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | -0.187    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 298496    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.5      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 1088      |\n|    fps             | 60        |\n|    time_elapsed    | 4957      |\n|    total_timesteps | 298885    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 18.9      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.136     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 298816    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.5      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 1092      |\n|    fps             | 60        |\n|    time_elapsed    | 4962      |\n|    total_timesteps | 299156    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 18.6      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.00733   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 299072    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 1096      |\n|    fps             | 60        |\n|    time_elapsed    | 4966      |\n|    total_timesteps | 299439    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.2      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.0217    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 299328    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 1100      |\n|    fps             | 60        |\n|    time_elapsed    | 4971      |\n|    total_timesteps | 299712    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.1      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.0743    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 299584    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 1104      |\n|    fps             | 60        |\n|    time_elapsed    | 4976      |\n|    total_timesteps | 299984    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 19        |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.142     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 299904    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 1108      |\n|    fps             | 60        |\n|    time_elapsed    | 4982      |\n|    total_timesteps | 300296    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.105     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 300224    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 1112      |\n|    fps             | 60        |\n|    time_elapsed    | 4986      |\n|    total_timesteps | 300567    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 18.8      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | 0.0698    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 300480    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 1116      |\n|    fps             | 60        |\n|    time_elapsed    | 4991      |\n|    total_timesteps | 300845    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19        |\n|    ent_coef        | 0.238     |\n|    ent_coef_loss   | 0.136     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 300736    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1120      |\n|    fps             | 60        |\n|    time_elapsed    | 4995      |\n|    total_timesteps | 301103    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.6      |\n|    ent_coef        | 0.24      |\n|    ent_coef_loss   | 0.0486    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 300992    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1124      |\n|    fps             | 60        |\n|    time_elapsed    | 5000      |\n|    total_timesteps | 301369    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.242     |\n|    ent_coef_loss   | 0.104     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 301248    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1128      |\n|    fps             | 60        |\n|    time_elapsed    | 5005      |\n|    total_timesteps | 301634    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.6      |\n|    ent_coef        | 0.244     |\n|    ent_coef_loss   | 0.0103    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 301568    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69        |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1132      |\n|    fps             | 60        |\n|    time_elapsed    | 5010      |\n|    total_timesteps | 301892    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 18.2      |\n|    ent_coef        | 0.247     |\n|    ent_coef_loss   | 0.139     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 301824    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1136      |\n|    fps             | 60        |\n|    time_elapsed    | 5014      |\n|    total_timesteps | 302152    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.8      |\n|    ent_coef        | 0.252     |\n|    ent_coef_loss   | 0.061     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 302080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.8      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1140      |\n|    fps             | 60        |\n|    time_elapsed    | 5018      |\n|    total_timesteps | 302408    |\n| train/             |           |\n|    actor_loss      | 1.46e+03  |\n|    critic_loss     | 19.1      |\n|    ent_coef        | 0.255     |\n|    ent_coef_loss   | 0.0642    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 302336    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.6      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1144      |\n|    fps             | 60        |\n|    time_elapsed    | 5023      |\n|    total_timesteps | 302667    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 19.3      |\n|    ent_coef        | 0.257     |\n|    ent_coef_loss   | 0.0136    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 302592    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1148      |\n|    fps             | 60        |\n|    time_elapsed    | 5028      |\n|    total_timesteps | 302923    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.6      |\n|    ent_coef        | 0.257     |\n|    ent_coef_loss   | -0.0104   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 302848    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1152      |\n|    fps             | 60        |\n|    time_elapsed    | 5032      |\n|    total_timesteps | 303180    |\n| train/             |           |\n|    actor_loss      | 1.43e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | -0.0871   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 303104    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.9      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1156      |\n|    fps             | 60        |\n|    time_elapsed    | 5037      |\n|    total_timesteps | 303436    |\n| train/             |           |\n|    actor_loss      | 1.43e+03  |\n|    critic_loss     | 17.7      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | 0.0897    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 303360    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.7      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1160      |\n|    fps             | 60        |\n|    time_elapsed    | 5041      |\n|    total_timesteps | 303693    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.4      |\n|    ent_coef        | 0.261     |\n|    ent_coef_loss   | 0.127     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 303616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1164      |\n|    fps             | 60        |\n|    time_elapsed    | 5046      |\n|    total_timesteps | 303949    |\n| train/             |           |\n|    actor_loss      | 1.45e+03  |\n|    critic_loss     | 18.5      |\n|    ent_coef        | 0.264     |\n|    ent_coef_loss   | -0.0265   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 303872    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1168      |\n|    fps             | 60        |\n|    time_elapsed    | 5050      |\n|    total_timesteps | 304206    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.263     |\n|    ent_coef_loss   | 0.00275   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 304128    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67        |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1172      |\n|    fps             | 60        |\n|    time_elapsed    | 5054      |\n|    total_timesteps | 304464    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.6      |\n|    ent_coef        | 0.264     |\n|    ent_coef_loss   | 0.0873    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 304384    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.8      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1176      |\n|    fps             | 60        |\n|    time_elapsed    | 5059      |\n|    total_timesteps | 304720    |\n| train/             |           |\n|    actor_loss      | 1.44e+03  |\n|    critic_loss     | 18.5      |\n|    ent_coef        | 0.263     |\n|    ent_coef_loss   | -0.0518   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 304640    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1180      |\n|    fps             | 60        |\n|    time_elapsed    | 5063      |\n|    total_timesteps | 304979    |\n| train/             |           |\n|    actor_loss      | 1.42e+03  |\n|    critic_loss     | 18.5      |\n|    ent_coef        | 0.261     |\n|    ent_coef_loss   | -0.0597   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 304896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1184      |\n|    fps             | 60        |\n|    time_elapsed    | 5068      |\n|    total_timesteps | 305236    |\n| train/             |           |\n|    actor_loss      | 1.43e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.26      |\n|    ent_coef_loss   | -0.00991  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 305152    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1188      |\n|    fps             | 60        |\n|    time_elapsed    | 5072      |\n|    total_timesteps | 305493    |\n| train/             |           |\n|    actor_loss      | 1.41e+03  |\n|    critic_loss     | 18.5      |\n|    ent_coef        | 0.26      |\n|    ent_coef_loss   | -0.0942   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 305408    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.9      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1192      |\n|    fps             | 60        |\n|    time_elapsed    | 5077      |\n|    total_timesteps | 305750    |\n| train/             |           |\n|    actor_loss      | 1.42e+03  |\n|    critic_loss     | 18.4      |\n|    ent_coef        | 0.259     |\n|    ent_coef_loss   | -0.0494   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 305664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 1196     |\n|    fps             | 60       |\n|    time_elapsed    | 5081     |\n|    total_timesteps | 306009   |\n| train/             |          |\n|    actor_loss      | 1.43e+03 |\n|    critic_loss     | 18.1     |\n|    ent_coef        | 0.26     |\n|    ent_coef_loss   | 0.00442  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 305920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 1200     |\n|    fps             | 60       |\n|    time_elapsed    | 5086     |\n|    total_timesteps | 306266   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.258    |\n|    ent_coef_loss   | -0.0361  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 306176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 1204     |\n|    fps             | 60       |\n|    time_elapsed    | 5090     |\n|    total_timesteps | 306522   |\n| train/             |          |\n|    actor_loss      | 1.43e+03 |\n|    critic_loss     | 18.6     |\n|    ent_coef        | 0.26     |\n|    ent_coef_loss   | 0.206    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 306432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 1208     |\n|    fps             | 60       |\n|    time_elapsed    | 5095     |\n|    total_timesteps | 306778   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 18.6     |\n|    ent_coef        | 0.259    |\n|    ent_coef_loss   | -0.0986  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 306688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1212     |\n|    fps             | 60       |\n|    time_elapsed    | 5099     |\n|    total_timesteps | 307034   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 18.7     |\n|    ent_coef        | 0.257    |\n|    ent_coef_loss   | -0.00797 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 306944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1216      |\n|    fps             | 60        |\n|    time_elapsed    | 5103      |\n|    total_timesteps | 307294    |\n| train/             |           |\n|    actor_loss      | 1.42e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.257     |\n|    ent_coef_loss   | 0.0792    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 307200    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1220      |\n|    fps             | 60        |\n|    time_elapsed    | 5108      |\n|    total_timesteps | 307551    |\n| train/             |           |\n|    actor_loss      | 1.41e+03  |\n|    critic_loss     | 18.8      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | -0.123    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 307456    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1224     |\n|    fps             | 60       |\n|    time_elapsed    | 5112     |\n|    total_timesteps | 307807   |\n| train/             |          |\n|    actor_loss      | 1.43e+03 |\n|    critic_loss     | 18.9     |\n|    ent_coef        | 0.256    |\n|    ent_coef_loss   | -0.0699  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 307712   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 1228     |\n|    fps             | 60       |\n|    time_elapsed    | 5116     |\n|    total_timesteps | 308065   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.1     |\n|    ent_coef        | 0.253    |\n|    ent_coef_loss   | -0.169   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 307968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -992     |\n| time/              |          |\n|    episodes        | 1232     |\n|    fps             | 60       |\n|    time_elapsed    | 5121     |\n|    total_timesteps | 308322   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | -0.0607  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 308224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -991     |\n| time/              |          |\n|    episodes        | 1236     |\n|    fps             | 60       |\n|    time_elapsed    | 5125     |\n|    total_timesteps | 308581   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 18.7     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | 0.073    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 308480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -992     |\n| time/              |          |\n|    episodes        | 1240     |\n|    fps             | 60       |\n|    time_elapsed    | 5129     |\n|    total_timesteps | 308840   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.3     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | 0.00768  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 308736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -987     |\n| time/              |          |\n|    episodes        | 1244     |\n|    fps             | 60       |\n|    time_elapsed    | 5134     |\n|    total_timesteps | 309101   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.8     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | -0.0756  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 308992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 1248     |\n|    fps             | 60       |\n|    time_elapsed    | 5138     |\n|    total_timesteps | 309359   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.6     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | -0.0261  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 309248   |\n---------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 1252     |\n|    fps             | 60       |\n|    time_elapsed    | 5143     |\n|    total_timesteps | 309620   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 18.1     |\n|    ent_coef        | 0.249    |\n|    ent_coef_loss   | -0.0209  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 309504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -988     |\n| time/              |          |\n|    episodes        | 1256     |\n|    fps             | 60       |\n|    time_elapsed    | 5147     |\n|    total_timesteps | 309879   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 18.3     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | 0.0147   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 309760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 1260     |\n|    fps             | 60       |\n|    time_elapsed    | 5151     |\n|    total_timesteps | 310140   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.3     |\n|    ent_coef        | 0.249    |\n|    ent_coef_loss   | -0.181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 310016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 1264     |\n|    fps             | 60       |\n|    time_elapsed    | 5156     |\n|    total_timesteps | 310399   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.246    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 310272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 1268     |\n|    fps             | 60       |\n|    time_elapsed    | 5161     |\n|    total_timesteps | 310659   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 18.3     |\n|    ent_coef        | 0.249    |\n|    ent_coef_loss   | -0.0725  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 310592   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 1272     |\n|    fps             | 60       |\n|    time_elapsed    | 5165     |\n|    total_timesteps | 310918   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.2     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | -0.00175 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 310848   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 1276     |\n|    fps             | 60       |\n|    time_elapsed    | 5170     |\n|    total_timesteps | 311180   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18       |\n|    ent_coef        | 0.246    |\n|    ent_coef_loss   | -0.0429  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 311104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -984     |\n| time/              |          |\n|    episodes        | 1280     |\n|    fps             | 60       |\n|    time_elapsed    | 5174     |\n|    total_timesteps | 311441   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 18.3     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 311360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 1284     |\n|    fps             | 60       |\n|    time_elapsed    | 5179     |\n|    total_timesteps | 311701   |\n| train/             |          |\n|    actor_loss      | 1.42e+03 |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | 0.0267   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 311616   |\n---------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 1288     |\n|    fps             | 60       |\n|    time_elapsed    | 5183     |\n|    total_timesteps | 311961   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 18.2     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | -0.0754  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 311872   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 1292     |\n|    fps             | 60       |\n|    time_elapsed    | 5187     |\n|    total_timesteps | 312225   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 312128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -970     |\n| time/              |          |\n|    episodes        | 1296     |\n|    fps             | 60       |\n|    time_elapsed    | 5192     |\n|    total_timesteps | 312488   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.245    |\n|    ent_coef_loss   | -0.026   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 312384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 1300     |\n|    fps             | 60       |\n|    time_elapsed    | 5196     |\n|    total_timesteps | 312749   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 18       |\n|    ent_coef        | 0.246    |\n|    ent_coef_loss   | 0.0808   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 312640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 1304     |\n|    fps             | 60       |\n|    time_elapsed    | 5201     |\n|    total_timesteps | 313011   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 18       |\n|    ent_coef        | 0.249    |\n|    ent_coef_loss   | 0.132    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 312896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 1308     |\n|    fps             | 60       |\n|    time_elapsed    | 5205     |\n|    total_timesteps | 313274   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | -0.0535  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 313152   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 1312     |\n|    fps             | 60       |\n|    time_elapsed    | 5210     |\n|    total_timesteps | 313538   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.1     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | -0.136   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 313472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 1316     |\n|    fps             | 60       |\n|    time_elapsed    | 5215     |\n|    total_timesteps | 313799   |\n| train/             |          |\n|    actor_loss      | 1.41e+03 |\n|    critic_loss     | 18.2     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | 0.215    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 313728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 1320     |\n|    fps             | 60       |\n|    time_elapsed    | 5219     |\n|    total_timesteps | 314060   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.6     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | 0.00345  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 313984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 1324     |\n|    fps             | 60       |\n|    time_elapsed    | 5223     |\n|    total_timesteps | 314322   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | 0.0292   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 314240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -988     |\n| time/              |          |\n|    episodes        | 1328     |\n|    fps             | 60       |\n|    time_elapsed    | 5228     |\n|    total_timesteps | 314585   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.6     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | 0.0941   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 314496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -988     |\n| time/              |          |\n|    episodes        | 1332     |\n|    fps             | 60       |\n|    time_elapsed    | 5232     |\n|    total_timesteps | 314846   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | 0.0448   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 314752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -984     |\n| time/              |          |\n|    episodes        | 1336     |\n|    fps             | 60       |\n|    time_elapsed    | 5236     |\n|    total_timesteps | 315106   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | -0.0412  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 315008   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -984     |\n| time/              |          |\n|    episodes        | 1340     |\n|    fps             | 60       |\n|    time_elapsed    | 5241     |\n|    total_timesteps | 315370   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | -0.00419 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 315264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -987     |\n| time/              |          |\n|    episodes        | 1344     |\n|    fps             | 60       |\n|    time_elapsed    | 5245     |\n|    total_timesteps | 315634   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.247    |\n|    ent_coef_loss   | -0.0394  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 315520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 1348     |\n|    fps             | 60       |\n|    time_elapsed    | 5249     |\n|    total_timesteps | 315896   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 18.1     |\n|    ent_coef        | 0.246    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 315776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -984     |\n| time/              |          |\n|    episodes        | 1352     |\n|    fps             | 60       |\n|    time_elapsed    | 5254     |\n|    total_timesteps | 316156   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | 0.0405   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 316032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 1356     |\n|    fps             | 60       |\n|    time_elapsed    | 5258     |\n|    total_timesteps | 316415   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.7     |\n|    ent_coef        | 0.245    |\n|    ent_coef_loss   | -0.0416  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 316288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 1360     |\n|    fps             | 60       |\n|    time_elapsed    | 5263     |\n|    total_timesteps | 316674   |\n| train/             |          |\n|    actor_loss      | 1.4e+03  |\n|    critic_loss     | 17.6     |\n|    ent_coef        | 0.244    |\n|    ent_coef_loss   | 0.00168  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 316608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 1364     |\n|    fps             | 60       |\n|    time_elapsed    | 5268     |\n|    total_timesteps | 316933   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.245    |\n|    ent_coef_loss   | 0.126    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 316864   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 1368     |\n|    fps             | 60       |\n|    time_elapsed    | 5272     |\n|    total_timesteps | 317190   |\n| train/             |          |\n|    actor_loss      | 1.38e+03 |\n|    critic_loss     | 17.3     |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | 0.00608  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 317120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 1372     |\n|    fps             | 60       |\n|    time_elapsed    | 5277     |\n|    total_timesteps | 317454   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 18       |\n|    ent_coef        | 0.248    |\n|    ent_coef_loss   | -0.0771  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 317376   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 1376     |\n|    fps             | 60       |\n|    time_elapsed    | 5281     |\n|    total_timesteps | 317714   |\n| train/             |          |\n|    actor_loss      | 1.38e+03 |\n|    critic_loss     | 17.3     |\n|    ent_coef        | 0.25     |\n|    ent_coef_loss   | 0.0667   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 317632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 1380     |\n|    fps             | 60       |\n|    time_elapsed    | 5285     |\n|    total_timesteps | 317973   |\n| train/             |          |\n|    actor_loss      | 1.39e+03 |\n|    critic_loss     | 17.9     |\n|    ent_coef        | 0.251    |\n|    ent_coef_loss   | 0.143    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 317888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 1384     |\n|    fps             | 60       |\n|    time_elapsed    | 5290     |\n|    total_timesteps | 318230   |\n| train/             |          |\n|    actor_loss      | 1.38e+03 |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.252    |\n|    ent_coef_loss   | 0.0861   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 318144   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1388     |\n|    fps             | 60       |\n|    time_elapsed    | 5294     |\n|    total_timesteps | 318489   |\n| train/             |          |\n|    actor_loss      | 1.38e+03 |\n|    critic_loss     | 17.8     |\n|    ent_coef        | 0.253    |\n|    ent_coef_loss   | 0.0159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 318400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1392      |\n|    fps             | 60        |\n|    time_elapsed    | 5299      |\n|    total_timesteps | 318747    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.1      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | -0.106    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 318656    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1396      |\n|    fps             | 60        |\n|    time_elapsed    | 5303      |\n|    total_timesteps | 319006    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 17.5      |\n|    ent_coef        | 0.253     |\n|    ent_coef_loss   | 0.0423    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 318912    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1400      |\n|    fps             | 60        |\n|    time_elapsed    | 5308      |\n|    total_timesteps | 319263    |\n| train/             |           |\n|    actor_loss      | 1.38e+03  |\n|    critic_loss     | 17.9      |\n|    ent_coef        | 0.253     |\n|    ent_coef_loss   | -0.0273   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 319168    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1404      |\n|    fps             | 60        |\n|    time_elapsed    | 5312      |\n|    total_timesteps | 319519    |\n| train/             |           |\n|    actor_loss      | 1.38e+03  |\n|    critic_loss     | 18.3      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | 0.249     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 319424    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1408      |\n|    fps             | 60        |\n|    time_elapsed    | 5316      |\n|    total_timesteps | 319775    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | -0.0514   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 319680    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1412     |\n|    fps             | 60       |\n|    time_elapsed    | 5321     |\n|    total_timesteps | 320031   |\n| train/             |          |\n|    actor_loss      | 1.37e+03 |\n|    critic_loss     | 17.5     |\n|    ent_coef        | 0.256    |\n|    ent_coef_loss   | 0.163    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 319936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.9      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1416      |\n|    fps             | 60        |\n|    time_elapsed    | 5325      |\n|    total_timesteps | 320289    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 17.5      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | 0.148     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 320192    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.9      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1420      |\n|    fps             | 60        |\n|    time_elapsed    | 5330      |\n|    total_timesteps | 320546    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | 0.0575    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 320448    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.8      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1424      |\n|    fps             | 60        |\n|    time_elapsed    | 5334      |\n|    total_timesteps | 320803    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.6      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | -0.0898   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 320704    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1428      |\n|    fps             | 60        |\n|    time_elapsed    | 5339      |\n|    total_timesteps | 321059    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.8      |\n|    ent_coef        | 0.256     |\n|    ent_coef_loss   | 0.0382    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 320960    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1432      |\n|    fps             | 60        |\n|    time_elapsed    | 5343      |\n|    total_timesteps | 321316    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.9      |\n|    ent_coef        | 0.259     |\n|    ent_coef_loss   | -0.0113   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 321216    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1436      |\n|    fps             | 60        |\n|    time_elapsed    | 5347      |\n|    total_timesteps | 321573    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 17.8      |\n|    ent_coef        | 0.259     |\n|    ent_coef_loss   | 0.0427    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 321472    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1440      |\n|    fps             | 60        |\n|    time_elapsed    | 5352      |\n|    total_timesteps | 321830    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.1      |\n|    ent_coef        | 0.259     |\n|    ent_coef_loss   | -0.106    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 321728    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1444      |\n|    fps             | 60        |\n|    time_elapsed    | 5356      |\n|    total_timesteps | 322086    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.8      |\n|    ent_coef        | 0.258     |\n|    ent_coef_loss   | -0.102    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 321984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1448      |\n|    fps             | 60        |\n|    time_elapsed    | 5361      |\n|    total_timesteps | 322343    |\n| train/             |           |\n|    actor_loss      | 1.37e+03  |\n|    critic_loss     | 18.1      |\n|    ent_coef        | 0.255     |\n|    ent_coef_loss   | -0.0164   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 322240    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1452      |\n|    fps             | 60        |\n|    time_elapsed    | 5365      |\n|    total_timesteps | 322601    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.252     |\n|    ent_coef_loss   | -0.102    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 322496    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1456      |\n|    fps             | 60        |\n|    time_elapsed    | 5370      |\n|    total_timesteps | 322857    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 18        |\n|    ent_coef        | 0.248     |\n|    ent_coef_loss   | -0.0837   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 322752    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1460      |\n|    fps             | 60        |\n|    time_elapsed    | 5374      |\n|    total_timesteps | 323113    |\n| train/             |           |\n|    actor_loss      | 1.36e+03  |\n|    critic_loss     | 17.3      |\n|    ent_coef        | 0.246     |\n|    ent_coef_loss   | 0.0939    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 323008    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1464      |\n|    fps             | 60        |\n|    time_elapsed    | 5379      |\n|    total_timesteps | 323370    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.2      |\n|    ent_coef        | 0.247     |\n|    ent_coef_loss   | 0.0104    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 323264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1468      |\n|    fps             | 60        |\n|    time_elapsed    | 5383      |\n|    total_timesteps | 323628    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.245     |\n|    ent_coef_loss   | -0.259    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 323520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1472      |\n|    fps             | 60        |\n|    time_elapsed    | 5387      |\n|    total_timesteps | 323886    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.5      |\n|    ent_coef        | 0.243     |\n|    ent_coef_loss   | 0.00995   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 323776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1476      |\n|    fps             | 60        |\n|    time_elapsed    | 5392      |\n|    total_timesteps | 324143    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.5      |\n|    ent_coef        | 0.244     |\n|    ent_coef_loss   | 0.0156    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 324032    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1480      |\n|    fps             | 60        |\n|    time_elapsed    | 5396      |\n|    total_timesteps | 324403    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.8      |\n|    ent_coef        | 0.242     |\n|    ent_coef_loss   | 0.0546    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 324288    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1484      |\n|    fps             | 60        |\n|    time_elapsed    | 5401      |\n|    total_timesteps | 324661    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.2      |\n|    ent_coef        | 0.239     |\n|    ent_coef_loss   | -0.0193   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 324544    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1488      |\n|    fps             | 60        |\n|    time_elapsed    | 5405      |\n|    total_timesteps | 324920    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | -0.0963   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 324800    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1492      |\n|    fps             | 60        |\n|    time_elapsed    | 5410      |\n|    total_timesteps | 325178    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.6      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | 0.0467    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 325056    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1496      |\n|    fps             | 60        |\n|    time_elapsed    | 5414      |\n|    total_timesteps | 325438    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17.2      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | -0.042    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 325312    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1500      |\n|    fps             | 60        |\n|    time_elapsed    | 5419      |\n|    total_timesteps | 325698    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | -0.0266   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 325632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1504      |\n|    fps             | 60        |\n|    time_elapsed    | 5424      |\n|    total_timesteps | 325956    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | -0.0872   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 325888    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1508      |\n|    fps             | 60        |\n|    time_elapsed    | 5428      |\n|    total_timesteps | 326214    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.0897    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 326144    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1512      |\n|    fps             | 60        |\n|    time_elapsed    | 5433      |\n|    total_timesteps | 326472    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | -0.106    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 326400    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1516      |\n|    fps             | 60        |\n|    time_elapsed    | 5437      |\n|    total_timesteps | 326732    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 18        |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.0565   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 326656    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1520      |\n|    fps             | 60        |\n|    time_elapsed    | 5442      |\n|    total_timesteps | 326992    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17.2      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.00569  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 326912    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1524      |\n|    fps             | 60        |\n|    time_elapsed    | 5446      |\n|    total_timesteps | 327251    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0687   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 327168    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1528      |\n|    fps             | 60        |\n|    time_elapsed    | 5451      |\n|    total_timesteps | 327509    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 16.9      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.0436    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 327424    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1532      |\n|    fps             | 60        |\n|    time_elapsed    | 5455      |\n|    total_timesteps | 327768    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.0643   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 327680    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1536      |\n|    fps             | 60        |\n|    time_elapsed    | 5460      |\n|    total_timesteps | 328027    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 16.7      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.147     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 327936    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1540      |\n|    fps             | 60        |\n|    time_elapsed    | 5464      |\n|    total_timesteps | 328286    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | 0.00092   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 328192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1544      |\n|    fps             | 60        |\n|    time_elapsed    | 5469      |\n|    total_timesteps | 328545    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 17.4      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.0135   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 328448    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1548      |\n|    fps             | 60        |\n|    time_elapsed    | 5473      |\n|    total_timesteps | 328804    |\n| train/             |           |\n|    actor_loss      | 1.34e+03  |\n|    critic_loss     | 17.3      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.172     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 328704    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1552      |\n|    fps             | 60        |\n|    time_elapsed    | 5478      |\n|    total_timesteps | 329065    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 17.1      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.193    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 328960    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1556      |\n|    fps             | 60        |\n|    time_elapsed    | 5482      |\n|    total_timesteps | 329322    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | 0.0564    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 329216    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1560      |\n|    fps             | 60        |\n|    time_elapsed    | 5487      |\n|    total_timesteps | 329582    |\n| train/             |           |\n|    actor_loss      | 1.33e+03  |\n|    critic_loss     | 17.5      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.0264    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 329472    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1564      |\n|    fps             | 60        |\n|    time_elapsed    | 5491      |\n|    total_timesteps | 329840    |\n| train/             |           |\n|    actor_loss      | 1.32e+03  |\n|    critic_loss     | 16.5      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | 0.0974    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 329728    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1568      |\n|    fps             | 60        |\n|    time_elapsed    | 5496      |\n|    total_timesteps | 330098    |\n| train/             |           |\n|    actor_loss      | 1.35e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.0807    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 329984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1572      |\n|    fps             | 60        |\n|    time_elapsed    | 5500      |\n|    total_timesteps | 330355    |\n| train/             |           |\n|    actor_loss      | 1.32e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.0254   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 330240    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1576     |\n|    fps             | 60       |\n|    time_elapsed    | 5505     |\n|    total_timesteps | 330613   |\n| train/             |          |\n|    actor_loss      | 1.31e+03 |\n|    critic_loss     | 16.3     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 330496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1580     |\n|    fps             | 60       |\n|    time_elapsed    | 5510     |\n|    total_timesteps | 330872   |\n| train/             |          |\n|    actor_loss      | 1.32e+03 |\n|    critic_loss     | 17.3     |\n|    ent_coef        | 0.231    |\n|    ent_coef_loss   | -0.0282  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 330752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1584      |\n|    fps             | 60        |\n|    time_elapsed    | 5515      |\n|    total_timesteps | 331131    |\n| train/             |           |\n|    actor_loss      | 1.32e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0276   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 331008    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1588      |\n|    fps             | 60        |\n|    time_elapsed    | 5519      |\n|    total_timesteps | 331388    |\n| train/             |           |\n|    actor_loss      | 1.3e+03   |\n|    critic_loss     | 16.2      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.0508   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 331264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1592      |\n|    fps             | 60        |\n|    time_elapsed    | 5524      |\n|    total_timesteps | 331646    |\n| train/             |           |\n|    actor_loss      | 1.3e+03   |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.127    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 331520    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1596      |\n|    fps             | 60        |\n|    time_elapsed    | 5529      |\n|    total_timesteps | 331905    |\n| train/             |           |\n|    actor_loss      | 1.31e+03  |\n|    critic_loss     | 16.4      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | 0.0373    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 331840    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1600      |\n|    fps             | 60        |\n|    time_elapsed    | 5533      |\n|    total_timesteps | 332164    |\n| train/             |           |\n|    actor_loss      | 1.32e+03  |\n|    critic_loss     | 16.4      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | 0.0228    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 332096    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1604      |\n|    fps             | 60        |\n|    time_elapsed    | 5538      |\n|    total_timesteps | 332422    |\n| train/             |           |\n|    actor_loss      | 1.3e+03   |\n|    critic_loss     | 16.3      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.167    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 332352    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.7      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1608      |\n|    fps             | 60        |\n|    time_elapsed    | 5542      |\n|    total_timesteps | 332681    |\n| train/             |           |\n|    actor_loss      | 1.31e+03  |\n|    critic_loss     | 16.8      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.08     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 332608    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1612     |\n|    fps             | 60       |\n|    time_elapsed    | 5547     |\n|    total_timesteps | 332939   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 17.2     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | 0.0284   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 332864   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 1616     |\n|    fps             | 60       |\n|    time_elapsed    | 5551     |\n|    total_timesteps | 333197   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.2     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.0717   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 333120   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 1620     |\n|    fps             | 60       |\n|    time_elapsed    | 5556     |\n|    total_timesteps | 333454   |\n| train/             |          |\n|    actor_loss      | 1.31e+03 |\n|    critic_loss     | 16.9     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.259    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 333376   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -991     |\n| time/              |          |\n|    episodes        | 1624     |\n|    fps             | 60       |\n|    time_elapsed    | 5560     |\n|    total_timesteps | 333711   |\n| train/             |          |\n|    actor_loss      | 1.31e+03 |\n|    critic_loss     | 17.2     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.0572  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 333632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 1628     |\n|    fps             | 60       |\n|    time_elapsed    | 5565     |\n|    total_timesteps | 333970   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.4     |\n|    ent_coef        | 0.231    |\n|    ent_coef_loss   | -0.16    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 333888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 1632     |\n|    fps             | 60       |\n|    time_elapsed    | 5569     |\n|    total_timesteps | 334227   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 17       |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | -0.0509  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 334144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 1636     |\n|    fps             | 60       |\n|    time_elapsed    | 5574     |\n|    total_timesteps | 334484   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.8     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.0204   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 334400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 1640     |\n|    fps             | 59       |\n|    time_elapsed    | 5579     |\n|    total_timesteps | 334742   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.2     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.0969   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 334656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 1644     |\n|    fps             | 59       |\n|    time_elapsed    | 5583     |\n|    total_timesteps | 335001   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.2     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.1     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 334912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 1648     |\n|    fps             | 59       |\n|    time_elapsed    | 5588     |\n|    total_timesteps | 335259   |\n| train/             |          |\n|    actor_loss      | 1.3e+03  |\n|    critic_loss     | 16.8     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | -0.106   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 335168   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1652     |\n|    fps             | 59       |\n|    time_elapsed    | 5592     |\n|    total_timesteps | 335517   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.7     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.0822   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 335424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1656      |\n|    fps             | 59        |\n|    time_elapsed    | 5597      |\n|    total_timesteps | 335777    |\n| train/             |           |\n|    actor_loss      | 1.28e+03  |\n|    critic_loss     | 16.3      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.148    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 335680    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1660     |\n|    fps             | 59       |\n|    time_elapsed    | 5601     |\n|    total_timesteps | 336034   |\n| train/             |          |\n|    actor_loss      | 1.31e+03 |\n|    critic_loss     | 16.7     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 335936   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 1664     |\n|    fps             | 59       |\n|    time_elapsed    | 5606     |\n|    total_timesteps | 336290   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 16.1     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.246   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 336192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 1668     |\n|    fps             | 59       |\n|    time_elapsed    | 5611     |\n|    total_timesteps | 336548   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.3     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.0493  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 336448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 1672     |\n|    fps             | 59       |\n|    time_elapsed    | 5615     |\n|    total_timesteps | 336805   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.3     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 336704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1676     |\n|    fps             | 59       |\n|    time_elapsed    | 5620     |\n|    total_timesteps | 337063   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.4     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.000987 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 336960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 1680     |\n|    fps             | 59       |\n|    time_elapsed    | 5624     |\n|    total_timesteps | 337322   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 16.4     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | -0.187   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 337216   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 1684     |\n|    fps             | 59       |\n|    time_elapsed    | 5629     |\n|    total_timesteps | 337581   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 16.2     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | -0.0525  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 337472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 1688     |\n|    fps             | 59       |\n|    time_elapsed    | 5633     |\n|    total_timesteps | 337837   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 16.4     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.145    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 337728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1692     |\n|    fps             | 59       |\n|    time_elapsed    | 5638     |\n|    total_timesteps | 338093   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.3     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.171   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 337984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 1696     |\n|    fps             | 59       |\n|    time_elapsed    | 5642     |\n|    total_timesteps | 338351   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 15.8     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.0783  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 338240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1700     |\n|    fps             | 59       |\n|    time_elapsed    | 5647     |\n|    total_timesteps | 338608   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 15.9     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.0936  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 338496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 1704     |\n|    fps             | 59       |\n|    time_elapsed    | 5651     |\n|    total_timesteps | 338867   |\n| train/             |          |\n|    actor_loss      | 1.29e+03 |\n|    critic_loss     | 16.5     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | -0.0547  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 338752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 1708     |\n|    fps             | 59       |\n|    time_elapsed    | 5656     |\n|    total_timesteps | 339123   |\n| train/             |          |\n|    actor_loss      | 1.28e+03 |\n|    critic_loss     | 16.5     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.0352   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 339008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1712      |\n|    fps             | 59        |\n|    time_elapsed    | 5660      |\n|    total_timesteps | 339381    |\n| train/             |           |\n|    actor_loss      | 1.29e+03  |\n|    critic_loss     | 16.4      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.032    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 339264    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1716      |\n|    fps             | 59        |\n|    time_elapsed    | 5665      |\n|    total_timesteps | 339638    |\n| train/             |           |\n|    actor_loss      | 1.29e+03  |\n|    critic_loss     | 17        |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.169     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 339520    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1720      |\n|    fps             | 59        |\n|    time_elapsed    | 5670      |\n|    total_timesteps | 339894    |\n| train/             |           |\n|    actor_loss      | 1.3e+03   |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.122     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 339776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1724      |\n|    fps             | 59        |\n|    time_elapsed    | 5674      |\n|    total_timesteps | 340154    |\n| train/             |           |\n|    actor_loss      | 1.29e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0198   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 340032    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1728      |\n|    fps             | 59        |\n|    time_elapsed    | 5678      |\n|    total_timesteps | 340410    |\n| train/             |           |\n|    actor_loss      | 1.29e+03  |\n|    critic_loss     | 16.4      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.0153    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 340288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1732      |\n|    fps             | 59        |\n|    time_elapsed    | 5683      |\n|    total_timesteps | 340666    |\n| train/             |           |\n|    actor_loss      | 1.27e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.0233    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 340544    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1736      |\n|    fps             | 59        |\n|    time_elapsed    | 5687      |\n|    total_timesteps | 340924    |\n| train/             |           |\n|    actor_loss      | 1.28e+03  |\n|    critic_loss     | 16.5      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.00815   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 340800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1740      |\n|    fps             | 59        |\n|    time_elapsed    | 5692      |\n|    total_timesteps | 341180    |\n| train/             |           |\n|    actor_loss      | 1.27e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.177    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 341056    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1744      |\n|    fps             | 59        |\n|    time_elapsed    | 5696      |\n|    total_timesteps | 341437    |\n| train/             |           |\n|    actor_loss      | 1.27e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0257   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 341312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1748      |\n|    fps             | 59        |\n|    time_elapsed    | 5701      |\n|    total_timesteps | 341693    |\n| train/             |           |\n|    actor_loss      | 1.28e+03  |\n|    critic_loss     | 16.3      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.022    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 341568    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1752     |\n|    fps             | 59       |\n|    time_elapsed    | 5705     |\n|    total_timesteps | 341949   |\n| train/             |          |\n|    actor_loss      | 1.27e+03 |\n|    critic_loss     | 15.5     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | -0.147   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 341824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 1756     |\n|    fps             | 59       |\n|    time_elapsed    | 5709     |\n|    total_timesteps | 342206   |\n| train/             |          |\n|    actor_loss      | 1.27e+03 |\n|    critic_loss     | 16       |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | 0.0471   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 342080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1760      |\n|    fps             | 59        |\n|    time_elapsed    | 5714      |\n|    total_timesteps | 342462    |\n| train/             |           |\n|    actor_loss      | 1.27e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.0571   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 342336    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 1764     |\n|    fps             | 59       |\n|    time_elapsed    | 5718     |\n|    total_timesteps | 342718   |\n| train/             |          |\n|    actor_loss      | 1.26e+03 |\n|    critic_loss     | 16       |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | 0.121    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 342592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1768      |\n|    fps             | 59        |\n|    time_elapsed    | 5723      |\n|    total_timesteps | 342975    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.065    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 342848    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1772      |\n|    fps             | 59        |\n|    time_elapsed    | 5727      |\n|    total_timesteps | 343232    |\n| train/             |           |\n|    actor_loss      | 1.28e+03  |\n|    critic_loss     | 16.4      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.00235  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 343104    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1776      |\n|    fps             | 59        |\n|    time_elapsed    | 5732      |\n|    total_timesteps | 343489    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.0863    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 343424    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1780      |\n|    fps             | 59        |\n|    time_elapsed    | 5736      |\n|    total_timesteps | 343745    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0865   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 343680    |\n----------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1784      |\n|    fps             | 59        |\n|    time_elapsed    | 5741      |\n|    total_timesteps | 344002    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 16.3      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | 0.0912    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 343936    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1788      |\n|    fps             | 59        |\n|    time_elapsed    | 5745      |\n|    total_timesteps | 344259    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.0631   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 344192    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1792      |\n|    fps             | 59        |\n|    time_elapsed    | 5750      |\n|    total_timesteps | 344515    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0722   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 344448    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1796      |\n|    fps             | 59        |\n|    time_elapsed    | 5754      |\n|    total_timesteps | 344773    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0445   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 344704    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1800      |\n|    fps             | 59        |\n|    time_elapsed    | 5758      |\n|    total_timesteps | 345031    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.11     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 344960    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1804      |\n|    fps             | 59        |\n|    time_elapsed    | 5763      |\n|    total_timesteps | 345287    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.00519  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 345216    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1808      |\n|    fps             | 59        |\n|    time_elapsed    | 5767      |\n|    total_timesteps | 345544    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.00701   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 345472    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1812      |\n|    fps             | 59        |\n|    time_elapsed    | 5771      |\n|    total_timesteps | 345802    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.143     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 345728    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1816      |\n|    fps             | 59        |\n|    time_elapsed    | 5776      |\n|    total_timesteps | 346060    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.147     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 345984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1820      |\n|    fps             | 59        |\n|    time_elapsed    | 5780      |\n|    total_timesteps | 346318    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.0912   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 346240    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1824      |\n|    fps             | 59        |\n|    time_elapsed    | 5784      |\n|    total_timesteps | 346576    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.132    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 346496    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1828      |\n|    fps             | 59        |\n|    time_elapsed    | 5789      |\n|    total_timesteps | 346832    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.108    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 346752    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1832      |\n|    fps             | 59        |\n|    time_elapsed    | 5793      |\n|    total_timesteps | 347090    |\n| train/             |           |\n|    actor_loss      | 1.27e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0372    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 347008    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1836      |\n|    fps             | 59        |\n|    time_elapsed    | 5798      |\n|    total_timesteps | 347348    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | 0.114     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 347264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1840      |\n|    fps             | 59        |\n|    time_elapsed    | 5802      |\n|    total_timesteps | 347604    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0434   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 347520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1844      |\n|    fps             | 59        |\n|    time_elapsed    | 5806      |\n|    total_timesteps | 347861    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.0442    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 347776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1848      |\n|    fps             | 59        |\n|    time_elapsed    | 5811      |\n|    total_timesteps | 348120    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0825   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 348032    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1852      |\n|    fps             | 59        |\n|    time_elapsed    | 5815      |\n|    total_timesteps | 348376    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0672   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 348288    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1856      |\n|    fps             | 59        |\n|    time_elapsed    | 5819      |\n|    total_timesteps | 348633    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.084    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 348544    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1860      |\n|    fps             | 59        |\n|    time_elapsed    | 5824      |\n|    total_timesteps | 348889    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.246     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 348800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1864      |\n|    fps             | 59        |\n|    time_elapsed    | 5828      |\n|    total_timesteps | 349148    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0597   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 349056    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1868      |\n|    fps             | 59        |\n|    time_elapsed    | 5833      |\n|    total_timesteps | 349407    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.0522   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 349312    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1872      |\n|    fps             | 59        |\n|    time_elapsed    | 5837      |\n|    total_timesteps | 349666    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.00291  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 349568    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1876      |\n|    fps             | 59        |\n|    time_elapsed    | 5841      |\n|    total_timesteps | 349923    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.174    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 349824    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1880      |\n|    fps             | 59        |\n|    time_elapsed    | 5846      |\n|    total_timesteps | 350181    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.167     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 350080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1884      |\n|    fps             | 59        |\n|    time_elapsed    | 5850      |\n|    total_timesteps | 350440    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.0216    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 350336    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1888      |\n|    fps             | 59        |\n|    time_elapsed    | 5855      |\n|    total_timesteps | 350697    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.128     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 350592    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1892      |\n|    fps             | 59        |\n|    time_elapsed    | 5859      |\n|    total_timesteps | 350954    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | 0.0439    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 350848    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1896      |\n|    fps             | 59        |\n|    time_elapsed    | 5863      |\n|    total_timesteps | 351214    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.00214   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 351104    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1900      |\n|    fps             | 59        |\n|    time_elapsed    | 5868      |\n|    total_timesteps | 351472    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.0394   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 351360    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1904      |\n|    fps             | 59        |\n|    time_elapsed    | 5872      |\n|    total_timesteps | 351730    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16.2      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.188    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 351616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 1908      |\n|    fps             | 59        |\n|    time_elapsed    | 5877      |\n|    total_timesteps | 351988    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0392    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 351872    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1912      |\n|    fps             | 59        |\n|    time_elapsed    | 5881      |\n|    total_timesteps | 352244    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16        |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0465    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 352128    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1916      |\n|    fps             | 59        |\n|    time_elapsed    | 5886      |\n|    total_timesteps | 352501    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | -0.0342   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 352384    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1920      |\n|    fps             | 59        |\n|    time_elapsed    | 5890      |\n|    total_timesteps | 352758    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.0471    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 352640    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 1924      |\n|    fps             | 59        |\n|    time_elapsed    | 5894      |\n|    total_timesteps | 353019    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0407    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 352896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1928      |\n|    fps             | 59        |\n|    time_elapsed    | 5899      |\n|    total_timesteps | 353275    |\n| train/             |           |\n|    actor_loss      | 1.26e+03  |\n|    critic_loss     | 16.6      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.252     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 353152    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1932      |\n|    fps             | 59        |\n|    time_elapsed    | 5903      |\n|    total_timesteps | 353531    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | 0.0605    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 353408    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 1936      |\n|    fps             | 59        |\n|    time_elapsed    | 5907      |\n|    total_timesteps | 353788    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | 0.0727    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 353664    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1940      |\n|    fps             | 59        |\n|    time_elapsed    | 5912      |\n|    total_timesteps | 354046    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 16.2      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.138     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 353920    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1944      |\n|    fps             | 59        |\n|    time_elapsed    | 5917      |\n|    total_timesteps | 354305    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | -0.0343   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 354240    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1948      |\n|    fps             | 59        |\n|    time_elapsed    | 5921      |\n|    total_timesteps | 354562    |\n| train/             |           |\n|    actor_loss      | 1.25e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.0539   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 354496    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1952      |\n|    fps             | 59        |\n|    time_elapsed    | 5926      |\n|    total_timesteps | 354819    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0409   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 354752    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1956      |\n|    fps             | 59        |\n|    time_elapsed    | 5930      |\n|    total_timesteps | 355076    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.155     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 355008    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1960      |\n|    fps             | 59        |\n|    time_elapsed    | 5935      |\n|    total_timesteps | 355336    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0435   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 355264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1964      |\n|    fps             | 59        |\n|    time_elapsed    | 5939      |\n|    total_timesteps | 355594    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.147     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 355520    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1968      |\n|    fps             | 59        |\n|    time_elapsed    | 5944      |\n|    total_timesteps | 355853    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | -0.0148   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 355776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1972      |\n|    fps             | 59        |\n|    time_elapsed    | 5948      |\n|    total_timesteps | 356111    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0122    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 356032    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1976      |\n|    fps             | 59        |\n|    time_elapsed    | 5952      |\n|    total_timesteps | 356367    |\n| train/             |           |\n|    actor_loss      | 1.24e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0271    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 356288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1980      |\n|    fps             | 59        |\n|    time_elapsed    | 5957      |\n|    total_timesteps | 356624    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.123     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 356544    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1984      |\n|    fps             | 59        |\n|    time_elapsed    | 5961      |\n|    total_timesteps | 356880    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.1      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.219    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 356800    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1988      |\n|    fps             | 59        |\n|    time_elapsed    | 5966      |\n|    total_timesteps | 357137    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.222     |\n|    ent_coef_loss   | 0.0493    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 357056    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 1992      |\n|    fps             | 59        |\n|    time_elapsed    | 5970      |\n|    total_timesteps | 357394    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.222     |\n|    ent_coef_loss   | -0.13     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 357312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 1996      |\n|    fps             | 59        |\n|    time_elapsed    | 5975      |\n|    total_timesteps | 357651    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.132     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 357568    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2000      |\n|    fps             | 59        |\n|    time_elapsed    | 5979      |\n|    total_timesteps | 357907    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.0441    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 357824    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2004     |\n|    fps             | 59       |\n|    time_elapsed    | 5983     |\n|    total_timesteps | 358165   |\n| train/             |          |\n|    actor_loss      | 1.23e+03 |\n|    critic_loss     | 15.7     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.04     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 358080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2008      |\n|    fps             | 59        |\n|    time_elapsed    | 5988      |\n|    total_timesteps | 358422    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0254   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 358336    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2012      |\n|    fps             | 59        |\n|    time_elapsed    | 5992      |\n|    total_timesteps | 358682    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.036    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 358592    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2016     |\n|    fps             | 59       |\n|    time_elapsed    | 5997     |\n|    total_timesteps | 358940   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.223    |\n|    ent_coef_loss   | 0.0482   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 358848   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2020     |\n|    fps             | 59       |\n|    time_elapsed    | 6001     |\n|    total_timesteps | 359196   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.0487   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 359104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2024      |\n|    fps             | 59        |\n|    time_elapsed    | 6006      |\n|    total_timesteps | 359453    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.0356    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 359360    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2028      |\n|    fps             | 59        |\n|    time_elapsed    | 6010      |\n|    total_timesteps | 359709    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0572    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 359616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2032      |\n|    fps             | 59        |\n|    time_elapsed    | 6014      |\n|    total_timesteps | 359965    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.00281  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 359872    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2036     |\n|    fps             | 59       |\n|    time_elapsed    | 6019     |\n|    total_timesteps | 360221   |\n| train/             |          |\n|    actor_loss      | 1.23e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.0459   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 360128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2040      |\n|    fps             | 59        |\n|    time_elapsed    | 6023      |\n|    total_timesteps | 360478    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0334   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 360384    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2044      |\n|    fps             | 59        |\n|    time_elapsed    | 6028      |\n|    total_timesteps | 360735    |\n| train/             |           |\n|    actor_loss      | 1.23e+03  |\n|    critic_loss     | 16.1      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.0286    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 360640    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2048      |\n|    fps             | 59        |\n|    time_elapsed    | 6032      |\n|    total_timesteps | 360991    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0764    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 360896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2052      |\n|    fps             | 59        |\n|    time_elapsed    | 6037      |\n|    total_timesteps | 361249    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.00973   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 361152    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2056      |\n|    fps             | 59        |\n|    time_elapsed    | 6041      |\n|    total_timesteps | 361505    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0214   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 361408    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2060      |\n|    fps             | 59        |\n|    time_elapsed    | 6046      |\n|    total_timesteps | 361763    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.105    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 361664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2064     |\n|    fps             | 59       |\n|    time_elapsed    | 6050     |\n|    total_timesteps | 362019   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | -0.0428  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 361920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -992     |\n| time/              |          |\n|    episodes        | 2068     |\n|    fps             | 59       |\n|    time_elapsed    | 6055     |\n|    total_timesteps | 362277   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.4     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | 0.0283   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 362176   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 2072     |\n|    fps             | 59       |\n|    time_elapsed    | 6059     |\n|    total_timesteps | 362535   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | 0.0556   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 362432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -988     |\n| time/              |          |\n|    episodes        | 2076     |\n|    fps             | 59       |\n|    time_elapsed    | 6063     |\n|    total_timesteps | 362792   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 362688   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 2080     |\n|    fps             | 59       |\n|    time_elapsed    | 6068     |\n|    total_timesteps | 363050   |\n| train/             |          |\n|    actor_loss      | 1.22e+03 |\n|    critic_loss     | 15.3     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.0529  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 362944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 2084     |\n|    fps             | 59       |\n|    time_elapsed    | 6072     |\n|    total_timesteps | 363308   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.4     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.0773  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 363200   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 2088     |\n|    fps             | 59       |\n|    time_elapsed    | 6077     |\n|    total_timesteps | 363566   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.5     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 363456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 2092     |\n|    fps             | 59       |\n|    time_elapsed    | 6081     |\n|    total_timesteps | 363825   |\n| train/             |          |\n|    actor_loss      | 1.2e+03  |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.0558  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 363712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -987     |\n| time/              |          |\n|    episodes        | 2096     |\n|    fps             | 59       |\n|    time_elapsed    | 6086     |\n|    total_timesteps | 364084   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.7     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.0364   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 363968   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 2100     |\n|    fps             | 59       |\n|    time_elapsed    | 6090     |\n|    total_timesteps | 364340   |\n| train/             |          |\n|    actor_loss      | 1.2e+03  |\n|    critic_loss     | 15.5     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.196    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 364224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 2104     |\n|    fps             | 59       |\n|    time_elapsed    | 6095     |\n|    total_timesteps | 364598   |\n| train/             |          |\n|    actor_loss      | 1.2e+03  |\n|    critic_loss     | 16.3     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | -0.0919  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 364480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 2108     |\n|    fps             | 59       |\n|    time_elapsed    | 6099     |\n|    total_timesteps | 364856   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.0297  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 364736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 2112     |\n|    fps             | 59       |\n|    time_elapsed    | 6104     |\n|    total_timesteps | 365114   |\n| train/             |          |\n|    actor_loss      | 1.2e+03  |\n|    critic_loss     | 15.7     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.0957   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 364992   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 2116     |\n|    fps             | 59       |\n|    time_elapsed    | 6108     |\n|    total_timesteps | 365373   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.5     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | 0.0885   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 365248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 2120     |\n|    fps             | 59       |\n|    time_elapsed    | 6113     |\n|    total_timesteps | 365629   |\n| train/             |          |\n|    actor_loss      | 1.2e+03  |\n|    critic_loss     | 15.7     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.0171  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 365504   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -991     |\n| time/              |          |\n|    episodes        | 2124     |\n|    fps             | 59       |\n|    time_elapsed    | 6117     |\n|    total_timesteps | 365888   |\n| train/             |          |\n|    actor_loss      | 1.19e+03 |\n|    critic_loss     | 15.6     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.0506  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 365760   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2128     |\n|    fps             | 59       |\n|    time_elapsed    | 6122     |\n|    total_timesteps | 366146   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.7     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.115    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 366080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2132     |\n|    fps             | 59       |\n|    time_elapsed    | 6127     |\n|    total_timesteps | 366404   |\n| train/             |          |\n|    actor_loss      | 1.21e+03 |\n|    critic_loss     | 15.5     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | 0.00092  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 366336   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2136      |\n|    fps             | 59        |\n|    time_elapsed    | 6131      |\n|    total_timesteps | 366664    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.12      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 366592    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2140      |\n|    fps             | 59        |\n|    time_elapsed    | 6136      |\n|    total_timesteps | 366921    |\n| train/             |           |\n|    actor_loss      | 1.21e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | 0.0452    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 366848    |\n----------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2144      |\n|    fps             | 59        |\n|    time_elapsed    | 6140      |\n|    total_timesteps | 367181    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15        |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0568   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 367104    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2148      |\n|    fps             | 59        |\n|    time_elapsed    | 6145      |\n|    total_timesteps | 367441    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0883   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 367360    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2152      |\n|    fps             | 59        |\n|    time_elapsed    | 6149      |\n|    total_timesteps | 367702    |\n| train/             |           |\n|    actor_loss      | 1.22e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | 0.183     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 367616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2156      |\n|    fps             | 59        |\n|    time_elapsed    | 6154      |\n|    total_timesteps | 367962    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | 0.0578    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 367872    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2160      |\n|    fps             | 59        |\n|    time_elapsed    | 6158      |\n|    total_timesteps | 368221    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | 0.0713    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 368128    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2164      |\n|    fps             | 59        |\n|    time_elapsed    | 6163      |\n|    total_timesteps | 368479    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | -0.0594   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 368384    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2168      |\n|    fps             | 59        |\n|    time_elapsed    | 6167      |\n|    total_timesteps | 368736    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.9      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | 0.054     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 368640    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2172      |\n|    fps             | 59        |\n|    time_elapsed    | 6172      |\n|    total_timesteps | 368995    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.1      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.0486    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 368896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2176      |\n|    fps             | 59        |\n|    time_elapsed    | 6176      |\n|    total_timesteps | 369254    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | -0.00917  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 369152    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2180      |\n|    fps             | 59        |\n|    time_elapsed    | 6181      |\n|    total_timesteps | 369512    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.0296    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 369408    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2184      |\n|    fps             | 59        |\n|    time_elapsed    | 6185      |\n|    total_timesteps | 369771    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.0544    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 369664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2188      |\n|    fps             | 59        |\n|    time_elapsed    | 6189      |\n|    total_timesteps | 370029    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.144     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 369920    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2192      |\n|    fps             | 59        |\n|    time_elapsed    | 6194      |\n|    total_timesteps | 370288    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.0609    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 370176    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2196      |\n|    fps             | 59        |\n|    time_elapsed    | 6198      |\n|    total_timesteps | 370544    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | -0.0704   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 370432    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2200      |\n|    fps             | 59        |\n|    time_elapsed    | 6203      |\n|    total_timesteps | 370801    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.237     |\n|    ent_coef_loss   | 0.00817   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 370688    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2204      |\n|    fps             | 59        |\n|    time_elapsed    | 6207      |\n|    total_timesteps | 371059    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.0222    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 370944    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2208      |\n|    fps             | 59        |\n|    time_elapsed    | 6212      |\n|    total_timesteps | 371317    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.0681   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 371200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2212      |\n|    fps             | 59        |\n|    time_elapsed    | 6216      |\n|    total_timesteps | 371574    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.033     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 371456    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2216      |\n|    fps             | 59        |\n|    time_elapsed    | 6221      |\n|    total_timesteps | 371830    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | -0.104    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 371712    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2220      |\n|    fps             | 59        |\n|    time_elapsed    | 6225      |\n|    total_timesteps | 372086    |\n| train/             |           |\n|    actor_loss      | 1.2e+03   |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.0599   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 371968    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2224      |\n|    fps             | 59        |\n|    time_elapsed    | 6230      |\n|    total_timesteps | 372343    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 14.9      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.0445    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 372224    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2228      |\n|    fps             | 59        |\n|    time_elapsed    | 6234      |\n|    total_timesteps | 372599    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.7      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.0108   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 372480    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2232      |\n|    fps             | 59        |\n|    time_elapsed    | 6239      |\n|    total_timesteps | 372855    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.0351   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 372736    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2236      |\n|    fps             | 59        |\n|    time_elapsed    | 6243      |\n|    total_timesteps | 373112    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.0179    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 372992    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2240      |\n|    fps             | 59        |\n|    time_elapsed    | 6248      |\n|    total_timesteps | 373369    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.8      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.0383    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 373248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2244      |\n|    fps             | 59        |\n|    time_elapsed    | 6252      |\n|    total_timesteps | 373625    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.0502    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 373504    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2248      |\n|    fps             | 59        |\n|    time_elapsed    | 6257      |\n|    total_timesteps | 373884    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.6      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.0387   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 373760    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2252      |\n|    fps             | 59        |\n|    time_elapsed    | 6261      |\n|    total_timesteps | 374140    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.5      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | 0.0221    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 374016    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2256      |\n|    fps             | 59        |\n|    time_elapsed    | 6266      |\n|    total_timesteps | 374396    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.4      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.242    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 374272    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2260      |\n|    fps             | 59        |\n|    time_elapsed    | 6270      |\n|    total_timesteps | 374653    |\n| train/             |           |\n|    actor_loss      | 1.19e+03  |\n|    critic_loss     | 15.2      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.0383    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 374528    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2264      |\n|    fps             | 59        |\n|    time_elapsed    | 6274      |\n|    total_timesteps | 374910    |\n| train/             |           |\n|    actor_loss      | 1.18e+03  |\n|    critic_loss     | 15.3      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.0157   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 374784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2268     |\n|    fps             | 59       |\n|    time_elapsed    | 6279     |\n|    total_timesteps | 375167   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 15.3     |\n|    ent_coef        | 0.231    |\n|    ent_coef_loss   | -0.0718  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 375040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2272      |\n|    fps             | 59        |\n|    time_elapsed    | 6283      |\n|    total_timesteps | 375423    |\n| train/             |           |\n|    actor_loss      | 1.17e+03  |\n|    critic_loss     | 15        |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | -0.0814   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 375296    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 2276     |\n|    fps             | 59       |\n|    time_elapsed    | 6287     |\n|    total_timesteps | 375680   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15.2     |\n|    ent_coef        | 0.23     |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 375552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 2280     |\n|    fps             | 59       |\n|    time_elapsed    | 6292     |\n|    total_timesteps | 375936   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.229    |\n|    ent_coef_loss   | -0.0963  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 375808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -981     |\n| time/              |          |\n|    episodes        | 2284     |\n|    fps             | 59       |\n|    time_elapsed    | 6296     |\n|    total_timesteps | 376192   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15.3     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | -0.0375  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 376064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 2288     |\n|    fps             | 59       |\n|    time_elapsed    | 6301     |\n|    total_timesteps | 376450   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.00661 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 376384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 2292     |\n|    fps             | 59       |\n|    time_elapsed    | 6306     |\n|    total_timesteps | 376707   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | -0.185   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 376640   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 2296     |\n|    fps             | 59       |\n|    time_elapsed    | 6310     |\n|    total_timesteps | 376964   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15.2     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | -0.0312  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 376896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 2300     |\n|    fps             | 59       |\n|    time_elapsed    | 6314     |\n|    total_timesteps | 377220   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 15.3     |\n|    ent_coef        | 0.224    |\n|    ent_coef_loss   | 0.0247   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 377152   |\n---------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 2304     |\n|    fps             | 59       |\n|    time_elapsed    | 6319     |\n|    total_timesteps | 377477   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15.2     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0462  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 377408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 2308     |\n|    fps             | 59       |\n|    time_elapsed    | 6323     |\n|    total_timesteps | 377735   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15.1     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 377664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 2312     |\n|    fps             | 59       |\n|    time_elapsed    | 6328     |\n|    total_timesteps | 377995   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0189  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 377920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 2316     |\n|    fps             | 59       |\n|    time_elapsed    | 6332     |\n|    total_timesteps | 378253   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.6     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0136  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 378176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 2320     |\n|    fps             | 59       |\n|    time_elapsed    | 6336     |\n|    total_timesteps | 378511   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.215   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 378432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 2324     |\n|    fps             | 59       |\n|    time_elapsed    | 6341     |\n|    total_timesteps | 378769   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.138    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 378688   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 2328     |\n|    fps             | 59       |\n|    time_elapsed    | 6345     |\n|    total_timesteps | 379029   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0507   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 378944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -955     |\n| time/              |          |\n|    episodes        | 2332     |\n|    fps             | 59       |\n|    time_elapsed    | 6350     |\n|    total_timesteps | 379285   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.105   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 379200   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -948     |\n| time/              |          |\n|    episodes        | 2336     |\n|    fps             | 59       |\n|    time_elapsed    | 6354     |\n|    total_timesteps | 379545   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.164   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 379456   |\n---------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 2340     |\n|    fps             | 59       |\n|    time_elapsed    | 6358     |\n|    total_timesteps | 379803   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 15.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.171   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 379712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 2344     |\n|    fps             | 59       |\n|    time_elapsed    | 6363     |\n|    total_timesteps | 380062   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 15       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0359  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 379968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 2348     |\n|    fps             | 59       |\n|    time_elapsed    | 6367     |\n|    total_timesteps | 380321   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0785  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 380224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 2352     |\n|    fps             | 59       |\n|    time_elapsed    | 6371     |\n|    total_timesteps | 380580   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.152    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 380480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 2356     |\n|    fps             | 59       |\n|    time_elapsed    | 6376     |\n|    total_timesteps | 380838   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.132    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 380736   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 2360     |\n|    fps             | 59       |\n|    time_elapsed    | 6380     |\n|    total_timesteps | 381098   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.6     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0418   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 380992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 2364     |\n|    fps             | 59       |\n|    time_elapsed    | 6385     |\n|    total_timesteps | 381355   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.4     |\n|    ent_coef        | 0.223    |\n|    ent_coef_loss   | -0.158   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 381248   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -970     |\n| time/              |          |\n|    episodes        | 2368     |\n|    fps             | 59       |\n|    time_elapsed    | 6389     |\n|    total_timesteps | 381612   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.7     |\n|    ent_coef        | 0.223    |\n|    ent_coef_loss   | -0.093   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 381504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 2372     |\n|    fps             | 59       |\n|    time_elapsed    | 6394     |\n|    total_timesteps | 381872   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 15       |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | -0.045   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 381760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 2376     |\n|    fps             | 59       |\n|    time_elapsed    | 6398     |\n|    total_timesteps | 382129   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 15       |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0315   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 382016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 2380     |\n|    fps             | 59       |\n|    time_elapsed    | 6402     |\n|    total_timesteps | 382387   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.9     |\n|    ent_coef        | 0.224    |\n|    ent_coef_loss   | 0.0219   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 382272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 2384     |\n|    fps             | 59       |\n|    time_elapsed    | 6407     |\n|    total_timesteps | 382644   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.6     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | -0.223   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 382528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 2388     |\n|    fps             | 59       |\n|    time_elapsed    | 6411     |\n|    total_timesteps | 382903   |\n| train/             |          |\n|    actor_loss      | 1.15e+03 |\n|    critic_loss     | 14.5     |\n|    ent_coef        | 0.224    |\n|    ent_coef_loss   | -0.0735  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 382784   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -987     |\n| time/              |          |\n|    episodes        | 2392     |\n|    fps             | 59       |\n|    time_elapsed    | 6416     |\n|    total_timesteps | 383159   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.225    |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 383040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 2396     |\n|    fps             | 59       |\n|    time_elapsed    | 6420     |\n|    total_timesteps | 383415   |\n| train/             |          |\n|    actor_loss      | 1.18e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.226    |\n|    ent_coef_loss   | 0.0349   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 383296   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -984     |\n| time/              |          |\n|    episodes        | 2400     |\n|    fps             | 59       |\n|    time_elapsed    | 6424     |\n|    total_timesteps | 383671   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.7     |\n|    ent_coef        | 0.228    |\n|    ent_coef_loss   | 0.0662   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 383552   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 2404     |\n|    fps             | 59       |\n|    time_elapsed    | 6429     |\n|    total_timesteps | 383927   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.8     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | -0.0451  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 383808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 2408     |\n|    fps             | 59       |\n|    time_elapsed    | 6433     |\n|    total_timesteps | 384183   |\n| train/             |          |\n|    actor_loss      | 1.15e+03 |\n|    critic_loss     | 14.1     |\n|    ent_coef        | 0.227    |\n|    ent_coef_loss   | 0.176    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 384064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 2412     |\n|    fps             | 59       |\n|    time_elapsed    | 6437     |\n|    total_timesteps | 384439   |\n| train/             |          |\n|    actor_loss      | 1.16e+03 |\n|    critic_loss     | 14.3     |\n|    ent_coef        | 0.233    |\n|    ent_coef_loss   | 0.0892   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 384320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2416     |\n|    fps             | 59       |\n|    time_elapsed    | 6442     |\n|    total_timesteps | 384696   |\n| train/             |          |\n|    actor_loss      | 1.17e+03 |\n|    critic_loss     | 14.5     |\n|    ent_coef        | 0.231    |\n|    ent_coef_loss   | 0.0726   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 384576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2420      |\n|    fps             | 59        |\n|    time_elapsed    | 6446      |\n|    total_timesteps | 384952    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 15.1      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.142     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 384832    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2424      |\n|    fps             | 59        |\n|    time_elapsed    | 6451      |\n|    total_timesteps | 385208    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.236     |\n|    ent_coef_loss   | -0.166    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 385088    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2428      |\n|    fps             | 59        |\n|    time_elapsed    | 6455      |\n|    total_timesteps | 385464    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.9      |\n|    ent_coef        | 0.235     |\n|    ent_coef_loss   | -0.0639   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 385344    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2432      |\n|    fps             | 59        |\n|    time_elapsed    | 6459      |\n|    total_timesteps | 385720    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.8      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.0186    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 385600    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2436      |\n|    fps             | 59        |\n|    time_elapsed    | 6464      |\n|    total_timesteps | 385977    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | -0.138    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 385856    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2440      |\n|    fps             | 59        |\n|    time_elapsed    | 6468      |\n|    total_timesteps | 386235    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | 0.0526    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 386112    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2444      |\n|    fps             | 59        |\n|    time_elapsed    | 6473      |\n|    total_timesteps | 386491    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.0243   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 386368    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2448      |\n|    fps             | 59        |\n|    time_elapsed    | 6477      |\n|    total_timesteps | 386747    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.00995  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 386624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2452      |\n|    fps             | 59        |\n|    time_elapsed    | 6481      |\n|    total_timesteps | 387003    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | 0.147     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 386880    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2456      |\n|    fps             | 59        |\n|    time_elapsed    | 6486      |\n|    total_timesteps | 387260    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.0668   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 387136    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2460      |\n|    fps             | 59        |\n|    time_elapsed    | 6490      |\n|    total_timesteps | 387516    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.3      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | 0.0861    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 387392    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2464      |\n|    fps             | 59        |\n|    time_elapsed    | 6495      |\n|    total_timesteps | 387772    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.076    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 387648    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2468      |\n|    fps             | 59        |\n|    time_elapsed    | 6499      |\n|    total_timesteps | 388028    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.234     |\n|    ent_coef_loss   | 0.0412    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 387904    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2472      |\n|    fps             | 59        |\n|    time_elapsed    | 6503      |\n|    total_timesteps | 388284    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.232     |\n|    ent_coef_loss   | -0.0977   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 388160    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2476      |\n|    fps             | 59        |\n|    time_elapsed    | 6508      |\n|    total_timesteps | 388540    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.231     |\n|    ent_coef_loss   | 0.16      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 388416    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2480      |\n|    fps             | 59        |\n|    time_elapsed    | 6512      |\n|    total_timesteps | 388798    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.233     |\n|    ent_coef_loss   | -0.0539   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 388672    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2484      |\n|    fps             | 59        |\n|    time_elapsed    | 6517      |\n|    total_timesteps | 389055    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.23      |\n|    ent_coef_loss   | -0.157    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 388928    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2488      |\n|    fps             | 59        |\n|    time_elapsed    | 6521      |\n|    total_timesteps | 389311    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | 0.0295    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 389184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2492      |\n|    fps             | 59        |\n|    time_elapsed    | 6526      |\n|    total_timesteps | 389567    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.228     |\n|    ent_coef_loss   | -0.0845   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 389440    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2496      |\n|    fps             | 59        |\n|    time_elapsed    | 6530      |\n|    total_timesteps | 389824    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | 0.0556    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 389696    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2500      |\n|    fps             | 59        |\n|    time_elapsed    | 6536      |\n|    total_timesteps | 390081    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | -0.0164   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 390016    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 2504      |\n|    fps             | 59        |\n|    time_elapsed    | 6540      |\n|    total_timesteps | 390337    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0834   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 390272    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 2508      |\n|    fps             | 59        |\n|    time_elapsed    | 6544      |\n|    total_timesteps | 390595    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.9      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0568   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 390528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 2512      |\n|    fps             | 59        |\n|    time_elapsed    | 6549      |\n|    total_timesteps | 390852    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.164     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 390784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2516      |\n|    fps             | 59        |\n|    time_elapsed    | 6553      |\n|    total_timesteps | 391108    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.0432    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 391040    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2520      |\n|    fps             | 59        |\n|    time_elapsed    | 6558      |\n|    total_timesteps | 391366    |\n| train/             |           |\n|    actor_loss      | 1.16e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.165     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 391296    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2524      |\n|    fps             | 59        |\n|    time_elapsed    | 6562      |\n|    total_timesteps | 391623    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.221     |\n|    ent_coef_loss   | -0.198    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 391552    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2528      |\n|    fps             | 59        |\n|    time_elapsed    | 6567      |\n|    total_timesteps | 391881    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.219     |\n|    ent_coef_loss   | 0.0901    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 391808    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 2532      |\n|    fps             | 59        |\n|    time_elapsed    | 6571      |\n|    total_timesteps | 392138    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | -0.0786   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 392064    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 2536      |\n|    fps             | 59        |\n|    time_elapsed    | 6576      |\n|    total_timesteps | 392395    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0409   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 392320    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2540      |\n|    fps             | 59        |\n|    time_elapsed    | 6580      |\n|    total_timesteps | 392652    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.14      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 392576    |\n----------------------------------\nargv[0]=\nargv[0]=\n\u001b[A                             \u001b[A\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2544      |\n|    fps             | 59        |\n|    time_elapsed    | 6585      |\n|    total_timesteps | 392909    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.199    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 392832    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2548      |\n|    fps             | 59        |\n|    time_elapsed    | 6589      |\n|    total_timesteps | 393166    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.00813  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 393088    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 2552      |\n|    fps             | 59        |\n|    time_elapsed    | 6593      |\n|    total_timesteps | 393425    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.0042   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 393344    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2556      |\n|    fps             | 59        |\n|    time_elapsed    | 6598      |\n|    total_timesteps | 393684    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0571   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 393600    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2560      |\n|    fps             | 59        |\n|    time_elapsed    | 6602      |\n|    total_timesteps | 393943    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.00422  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 393856    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2564      |\n|    fps             | 59        |\n|    time_elapsed    | 6607      |\n|    total_timesteps | 394204    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.0757    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 394112    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2568      |\n|    fps             | 59        |\n|    time_elapsed    | 6611      |\n|    total_timesteps | 394465    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.109    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 394368    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2572      |\n|    fps             | 59        |\n|    time_elapsed    | 6616      |\n|    total_timesteps | 394721    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.283    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 394624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2576      |\n|    fps             | 59        |\n|    time_elapsed    | 6620      |\n|    total_timesteps | 394980    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0528    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 394880    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2580      |\n|    fps             | 59        |\n|    time_elapsed    | 6624      |\n|    total_timesteps | 395236    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0669   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 395136    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2584      |\n|    fps             | 59        |\n|    time_elapsed    | 6629      |\n|    total_timesteps | 395495    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0264   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 395392    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2588      |\n|    fps             | 59        |\n|    time_elapsed    | 6633      |\n|    total_timesteps | 395752    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.0611    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 395648    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2592      |\n|    fps             | 59        |\n|    time_elapsed    | 6638      |\n|    total_timesteps | 396009    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0352   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 395904    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2596      |\n|    fps             | 59        |\n|    time_elapsed    | 6642      |\n|    total_timesteps | 396268    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.013    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 396160    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2600      |\n|    fps             | 59        |\n|    time_elapsed    | 6647      |\n|    total_timesteps | 396528    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0352    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 396416    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2604      |\n|    fps             | 59        |\n|    time_elapsed    | 6651      |\n|    total_timesteps | 396785    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.168     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 396672    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2608      |\n|    fps             | 59        |\n|    time_elapsed    | 6655      |\n|    total_timesteps | 397041    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0643    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 396928    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2612      |\n|    fps             | 59        |\n|    time_elapsed    | 6660      |\n|    total_timesteps | 397297    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.144     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 397184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2616      |\n|    fps             | 59        |\n|    time_elapsed    | 6664      |\n|    total_timesteps | 397554    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0236   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 397440    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2620      |\n|    fps             | 59        |\n|    time_elapsed    | 6668      |\n|    total_timesteps | 397812    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.118    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 397696    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2624      |\n|    fps             | 59        |\n|    time_elapsed    | 6673      |\n|    total_timesteps | 398069    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0714    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 397952    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2628      |\n|    fps             | 59        |\n|    time_elapsed    | 6677      |\n|    total_timesteps | 398326    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0539   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 398208    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2632      |\n|    fps             | 59        |\n|    time_elapsed    | 6682      |\n|    total_timesteps | 398583    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0642   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 398464    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2636      |\n|    fps             | 59        |\n|    time_elapsed    | 6686      |\n|    total_timesteps | 398841    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.3      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.121     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 398720    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2640      |\n|    fps             | 59        |\n|    time_elapsed    | 6690      |\n|    total_timesteps | 399097    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.00483  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 398976    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2644      |\n|    fps             | 59        |\n|    time_elapsed    | 6695      |\n|    total_timesteps | 399354    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0563   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 399232    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2648     |\n|    fps             | 59       |\n|    time_elapsed    | 6699     |\n|    total_timesteps | 399614   |\n| train/             |          |\n|    actor_loss      | 1.13e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0888   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 399488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2652      |\n|    fps             | 59        |\n|    time_elapsed    | 6704      |\n|    total_timesteps | 399871    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.171     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 399744    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2656      |\n|    fps             | 59        |\n|    time_elapsed    | 6708      |\n|    total_timesteps | 400126    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0396   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 400000    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2660     |\n|    fps             | 59       |\n|    time_elapsed    | 6712     |\n|    total_timesteps | 400384   |\n| train/             |          |\n|    actor_loss      | 1.14e+03 |\n|    critic_loss     | 14.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0234  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 400256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2664      |\n|    fps             | 59        |\n|    time_elapsed    | 6717      |\n|    total_timesteps | 400640    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0109    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 400512    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2668     |\n|    fps             | 59       |\n|    time_elapsed    | 6721     |\n|    total_timesteps | 400896   |\n| train/             |          |\n|    actor_loss      | 1.12e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0699  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 400768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2672      |\n|    fps             | 59        |\n|    time_elapsed    | 6726      |\n|    total_timesteps | 401154    |\n| train/             |           |\n|    actor_loss      | 1.14e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0114   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 401088    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2676      |\n|    fps             | 59        |\n|    time_elapsed    | 6731      |\n|    total_timesteps | 401413    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0399   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 401344    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2680      |\n|    fps             | 59        |\n|    time_elapsed    | 6735      |\n|    total_timesteps | 401669    |\n| train/             |           |\n|    actor_loss      | 1.15e+03  |\n|    critic_loss     | 14.3      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.146     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 401600    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2684      |\n|    fps             | 59        |\n|    time_elapsed    | 6740      |\n|    total_timesteps | 401925    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.188    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 401856    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2688      |\n|    fps             | 59        |\n|    time_elapsed    | 6744      |\n|    total_timesteps | 402181    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0208    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 402112    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2692      |\n|    fps             | 59        |\n|    time_elapsed    | 6748      |\n|    total_timesteps | 402438    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.151     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 402368    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2696      |\n|    fps             | 59        |\n|    time_elapsed    | 6753      |\n|    total_timesteps | 402694    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | -0.035    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 402624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2700      |\n|    fps             | 59        |\n|    time_elapsed    | 6757      |\n|    total_timesteps | 402951    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0164    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 402880    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2704      |\n|    fps             | 59        |\n|    time_elapsed    | 6762      |\n|    total_timesteps | 403208    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | -0.062    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 403136    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2708      |\n|    fps             | 59        |\n|    time_elapsed    | 6766      |\n|    total_timesteps | 403464    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.0682   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 403392    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2712      |\n|    fps             | 59        |\n|    time_elapsed    | 6771      |\n|    total_timesteps | 403721    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0494   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 403648    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2716      |\n|    fps             | 59        |\n|    time_elapsed    | 6775      |\n|    total_timesteps | 403977    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.11     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 403904    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2720      |\n|    fps             | 59        |\n|    time_elapsed    | 6779      |\n|    total_timesteps | 404233    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0297   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 404160    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2724      |\n|    fps             | 59        |\n|    time_elapsed    | 6784      |\n|    total_timesteps | 404489    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0658    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 404416    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2728      |\n|    fps             | 59        |\n|    time_elapsed    | 6788      |\n|    total_timesteps | 404747    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.0407   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 404672    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2732      |\n|    fps             | 59        |\n|    time_elapsed    | 6793      |\n|    total_timesteps | 405003    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.05      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 404928    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2736      |\n|    fps             | 59        |\n|    time_elapsed    | 6797      |\n|    total_timesteps | 405261    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.26      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 405184    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2740      |\n|    fps             | 59        |\n|    time_elapsed    | 6802      |\n|    total_timesteps | 405517    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0473    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 405440    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2744      |\n|    fps             | 59        |\n|    time_elapsed    | 6806      |\n|    total_timesteps | 405773    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.186    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 405696    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2748      |\n|    fps             | 59        |\n|    time_elapsed    | 6810      |\n|    total_timesteps | 406030    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.00028   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 405952    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2752      |\n|    fps             | 59        |\n|    time_elapsed    | 6815      |\n|    total_timesteps | 406286    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.119     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 406208    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2756      |\n|    fps             | 59        |\n|    time_elapsed    | 6819      |\n|    total_timesteps | 406542    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.142    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 406464    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2760      |\n|    fps             | 59        |\n|    time_elapsed    | 6824      |\n|    total_timesteps | 406801    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0319   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 406720    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2764      |\n|    fps             | 59        |\n|    time_elapsed    | 6828      |\n|    total_timesteps | 407057    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.112     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 406976    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2768      |\n|    fps             | 59        |\n|    time_elapsed    | 6833      |\n|    total_timesteps | 407313    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.115     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 407232    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2772      |\n|    fps             | 59        |\n|    time_elapsed    | 6837      |\n|    total_timesteps | 407571    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.123    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 407488    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2776      |\n|    fps             | 59        |\n|    time_elapsed    | 6842      |\n|    total_timesteps | 407827    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0788   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 407744    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2780      |\n|    fps             | 59        |\n|    time_elapsed    | 6846      |\n|    total_timesteps | 408083    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0147   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 408000    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2784      |\n|    fps             | 59        |\n|    time_elapsed    | 6851      |\n|    total_timesteps | 408339    |\n| train/             |           |\n|    actor_loss      | 1.13e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.0321    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 408256    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2788      |\n|    fps             | 59        |\n|    time_elapsed    | 6855      |\n|    total_timesteps | 408595    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.152     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 408512    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2792     |\n|    fps             | 59       |\n|    time_elapsed    | 6860     |\n|    total_timesteps | 408852   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 408768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2796     |\n|    fps             | 59       |\n|    time_elapsed    | 6864     |\n|    total_timesteps | 409108   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 14       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.11     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 409024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2800     |\n|    fps             | 59       |\n|    time_elapsed    | 6868     |\n|    total_timesteps | 409365   |\n| train/             |          |\n|    actor_loss      | 1.13e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0722   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 409280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 2804     |\n|    fps             | 59       |\n|    time_elapsed    | 6873     |\n|    total_timesteps | 409621   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.168   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 409536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 2808     |\n|    fps             | 59       |\n|    time_elapsed    | 6877     |\n|    total_timesteps | 409877   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0691   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 409792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 2812     |\n|    fps             | 59       |\n|    time_elapsed    | 6881     |\n|    total_timesteps | 410133   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0489  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 410048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2816     |\n|    fps             | 59       |\n|    time_elapsed    | 6886     |\n|    total_timesteps | 410389   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 14.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.143    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 410304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2820     |\n|    fps             | 59       |\n|    time_elapsed    | 6890     |\n|    total_timesteps | 410648   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0563  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 410560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2824      |\n|    fps             | 59        |\n|    time_elapsed    | 6895      |\n|    total_timesteps | 410905    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0133    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 410816    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2828     |\n|    fps             | 59       |\n|    time_elapsed    | 6899     |\n|    total_timesteps | 411163   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 14.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.162    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 411072   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2832      |\n|    fps             | 59        |\n|    time_elapsed    | 6903      |\n|    total_timesteps | 411420    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.0708   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 411328    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2836      |\n|    fps             | 59        |\n|    time_elapsed    | 6908      |\n|    total_timesteps | 411676    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0953    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 411584    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2840      |\n|    fps             | 59        |\n|    time_elapsed    | 6912      |\n|    total_timesteps | 411932    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.0836   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 411840    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2844     |\n|    fps             | 59       |\n|    time_elapsed    | 6917     |\n|    total_timesteps | 412188   |\n| train/             |          |\n|    actor_loss      | 1.12e+03 |\n|    critic_loss     | 14       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.134   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 412096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 2848     |\n|    fps             | 59       |\n|    time_elapsed    | 6921     |\n|    total_timesteps | 412444   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00635 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 412352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2852     |\n|    fps             | 59       |\n|    time_elapsed    | 6925     |\n|    total_timesteps | 412701   |\n| train/             |          |\n|    actor_loss      | 1.12e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0532   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 412608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2856      |\n|    fps             | 59        |\n|    time_elapsed    | 6930      |\n|    total_timesteps | 412957    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.187     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 412864    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 2860     |\n|    fps             | 59       |\n|    time_elapsed    | 6934     |\n|    total_timesteps | 413212   |\n| train/             |          |\n|    actor_loss      | 1.11e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0572  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 413120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2864      |\n|    fps             | 59        |\n|    time_elapsed    | 6938      |\n|    total_timesteps | 413469    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0571   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 413376    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2868      |\n|    fps             | 59        |\n|    time_elapsed    | 6943      |\n|    total_timesteps | 413725    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.207    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 413632    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2872      |\n|    fps             | 59        |\n|    time_elapsed    | 6947      |\n|    total_timesteps | 413981    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.014    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 413888    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2876      |\n|    fps             | 59        |\n|    time_elapsed    | 6952      |\n|    total_timesteps | 414237    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0426   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 414144    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2880      |\n|    fps             | 59        |\n|    time_elapsed    | 6956      |\n|    total_timesteps | 414495    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0953   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 414400    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2884      |\n|    fps             | 59        |\n|    time_elapsed    | 6960      |\n|    total_timesteps | 414751    |\n| train/             |           |\n|    actor_loss      | 1.12e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0959    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 414656    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2888      |\n|    fps             | 59        |\n|    time_elapsed    | 6965      |\n|    total_timesteps | 415009    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0769    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 414912    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2892      |\n|    fps             | 59        |\n|    time_elapsed    | 6969      |\n|    total_timesteps | 415266    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0327   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 415168    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2896      |\n|    fps             | 59        |\n|    time_elapsed    | 6973      |\n|    total_timesteps | 415522    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.106    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 415424    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2900      |\n|    fps             | 59        |\n|    time_elapsed    | 6978      |\n|    total_timesteps | 415779    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.00922   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 415680    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2904      |\n|    fps             | 59        |\n|    time_elapsed    | 6982      |\n|    total_timesteps | 416035    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.0265    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 415936    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2908      |\n|    fps             | 59        |\n|    time_elapsed    | 6987      |\n|    total_timesteps | 416291    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0214    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 416192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2912      |\n|    fps             | 59        |\n|    time_elapsed    | 6991      |\n|    total_timesteps | 416548    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.172    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 416448    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2916      |\n|    fps             | 59        |\n|    time_elapsed    | 6995      |\n|    total_timesteps | 416804    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.0696    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 416704    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2920      |\n|    fps             | 59        |\n|    time_elapsed    | 7000      |\n|    total_timesteps | 417060    |\n| train/             |           |\n|    actor_loss      | 1.11e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.0171    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 416960    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2924      |\n|    fps             | 59        |\n|    time_elapsed    | 7004      |\n|    total_timesteps | 417317    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.0477   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 417216    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2928      |\n|    fps             | 59        |\n|    time_elapsed    | 7009      |\n|    total_timesteps | 417573    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.224    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 417472    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2932      |\n|    fps             | 59        |\n|    time_elapsed    | 7013      |\n|    total_timesteps | 417830    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.0925   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 417728    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2936      |\n|    fps             | 59        |\n|    time_elapsed    | 7017      |\n|    total_timesteps | 418086    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.199     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 417984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2940      |\n|    fps             | 59        |\n|    time_elapsed    | 7022      |\n|    total_timesteps | 418344    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.0347   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 418240    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2944      |\n|    fps             | 59        |\n|    time_elapsed    | 7026      |\n|    total_timesteps | 418600    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 14.3      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.0494   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 418496    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2948      |\n|    fps             | 59        |\n|    time_elapsed    | 7030      |\n|    total_timesteps | 418856    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | -0.0366   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 418752    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2952      |\n|    fps             | 59        |\n|    time_elapsed    | 7035      |\n|    total_timesteps | 419112    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.031    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 419008    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 2956      |\n|    fps             | 59        |\n|    time_elapsed    | 7039      |\n|    total_timesteps | 419369    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | -0.0785   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 419264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2960      |\n|    fps             | 59        |\n|    time_elapsed    | 7043      |\n|    total_timesteps | 419625    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.124     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 419520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 2964      |\n|    fps             | 59        |\n|    time_elapsed    | 7048      |\n|    total_timesteps | 419882    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.0165   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 419776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 2968      |\n|    fps             | 59        |\n|    time_elapsed    | 7052      |\n|    total_timesteps | 420139    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.0389    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 420032    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2972      |\n|    fps             | 59        |\n|    time_elapsed    | 7056      |\n|    total_timesteps | 420395    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.145    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 420288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2976      |\n|    fps             | 59        |\n|    time_elapsed    | 7061      |\n|    total_timesteps | 420651    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.206     |\n|    ent_coef_loss   | -0.0164   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 420544    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2980      |\n|    fps             | 59        |\n|    time_elapsed    | 7065      |\n|    total_timesteps | 420907    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.0367    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 420800    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2984      |\n|    fps             | 59        |\n|    time_elapsed    | 7069      |\n|    total_timesteps | 421163    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.0704    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 421056    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 2988      |\n|    fps             | 59        |\n|    time_elapsed    | 7074      |\n|    total_timesteps | 421420    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.00883   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 421312    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2992      |\n|    fps             | 59        |\n|    time_elapsed    | 7078      |\n|    total_timesteps | 421677    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.046     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 421568    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 2996      |\n|    fps             | 59        |\n|    time_elapsed    | 7082      |\n|    total_timesteps | 421933    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.137     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 421824    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3000      |\n|    fps             | 59        |\n|    time_elapsed    | 7087      |\n|    total_timesteps | 422189    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.188     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 422080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3004      |\n|    fps             | 59        |\n|    time_elapsed    | 7091      |\n|    total_timesteps | 422445    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.0102    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 422336    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 3008     |\n|    fps             | 59       |\n|    time_elapsed    | 7096     |\n|    total_timesteps | 422701   |\n| train/             |          |\n|    actor_loss      | 1.1e+03  |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 422592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 3012     |\n|    fps             | 59       |\n|    time_elapsed    | 7100     |\n|    total_timesteps | 422957   |\n| train/             |          |\n|    actor_loss      | 1.09e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0883   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 422848   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 3016     |\n|    fps             | 59       |\n|    time_elapsed    | 7105     |\n|    total_timesteps | 423213   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 423104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 3020     |\n|    fps             | 59       |\n|    time_elapsed    | 7109     |\n|    total_timesteps | 423469   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 423360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3024     |\n|    fps             | 59       |\n|    time_elapsed    | 7113     |\n|    total_timesteps | 423725   |\n| train/             |          |\n|    actor_loss      | 1.09e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.044    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 423616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3028     |\n|    fps             | 59       |\n|    time_elapsed    | 7118     |\n|    total_timesteps | 423981   |\n| train/             |          |\n|    actor_loss      | 1.09e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0629  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 423872   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3032     |\n|    fps             | 59       |\n|    time_elapsed    | 7122     |\n|    total_timesteps | 424237   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 424128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3036      |\n|    fps             | 59        |\n|    time_elapsed    | 7126      |\n|    total_timesteps | 424493    |\n| train/             |           |\n|    actor_loss      | 1.1e+03   |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0824   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 424384    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3040     |\n|    fps             | 59       |\n|    time_elapsed    | 7131     |\n|    total_timesteps | 424749   |\n| train/             |          |\n|    actor_loss      | 1.09e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0493  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 424640   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3044     |\n|    fps             | 59       |\n|    time_elapsed    | 7135     |\n|    total_timesteps | 425005   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 424896   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 3048     |\n|    fps             | 59       |\n|    time_elapsed    | 7140     |\n|    total_timesteps | 425264   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0463  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 425152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3052     |\n|    fps             | 59       |\n|    time_elapsed    | 7144     |\n|    total_timesteps | 425520   |\n| train/             |          |\n|    actor_loss      | 1.09e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0921   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 425408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3056     |\n|    fps             | 59       |\n|    time_elapsed    | 7149     |\n|    total_timesteps | 425778   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.158    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 425664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3060      |\n|    fps             | 59        |\n|    time_elapsed    | 7153      |\n|    total_timesteps | 426034    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.153    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 425920    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3064      |\n|    fps             | 59        |\n|    time_elapsed    | 7157      |\n|    total_timesteps | 426291    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0437   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 426176    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3068      |\n|    fps             | 59        |\n|    time_elapsed    | 7162      |\n|    total_timesteps | 426547    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0806   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 426432    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3072      |\n|    fps             | 59        |\n|    time_elapsed    | 7166      |\n|    total_timesteps | 426803    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0103   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 426688    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3076      |\n|    fps             | 59        |\n|    time_elapsed    | 7171      |\n|    total_timesteps | 427059    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.131    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 426944    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3080      |\n|    fps             | 59        |\n|    time_elapsed    | 7175      |\n|    total_timesteps | 427315    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.039    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 427200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3084      |\n|    fps             | 59        |\n|    time_elapsed    | 7179      |\n|    total_timesteps | 427571    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0781   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 427456    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 3088      |\n|    fps             | 59        |\n|    time_elapsed    | 7184      |\n|    total_timesteps | 427828    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0519   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 427712    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3092      |\n|    fps             | 59        |\n|    time_elapsed    | 7188      |\n|    total_timesteps | 428085    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.128    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 427968    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3096      |\n|    fps             | 59        |\n|    time_elapsed    | 7193      |\n|    total_timesteps | 428341    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.0182   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 428224    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3100      |\n|    fps             | 59        |\n|    time_elapsed    | 7197      |\n|    total_timesteps | 428597    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.0493    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 428480    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3104      |\n|    fps             | 59        |\n|    time_elapsed    | 7202      |\n|    total_timesteps | 428854    |\n| train/             |           |\n|    actor_loss      | 1.06e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.118    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 428736    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3108      |\n|    fps             | 59        |\n|    time_elapsed    | 7206      |\n|    total_timesteps | 429110    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.0911    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 428992    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 3112      |\n|    fps             | 59        |\n|    time_elapsed    | 7210      |\n|    total_timesteps | 429367    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.00287   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 429248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 3116      |\n|    fps             | 59        |\n|    time_elapsed    | 7215      |\n|    total_timesteps | 429624    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.131    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 429504    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 3120      |\n|    fps             | 59        |\n|    time_elapsed    | 7219      |\n|    total_timesteps | 429880    |\n| train/             |           |\n|    actor_loss      | 1.09e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.2       |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 429760    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3124      |\n|    fps             | 59        |\n|    time_elapsed    | 7223      |\n|    total_timesteps | 430136    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.173     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 430016    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3128      |\n|    fps             | 59        |\n|    time_elapsed    | 7228      |\n|    total_timesteps | 430392    |\n| train/             |           |\n|    actor_loss      | 1.06e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.124     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 430272    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 3132      |\n|    fps             | 59        |\n|    time_elapsed    | 7232      |\n|    total_timesteps | 430650    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.141     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 430528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3136      |\n|    fps             | 59        |\n|    time_elapsed    | 7237      |\n|    total_timesteps | 430906    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.133    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 430784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3140      |\n|    fps             | 59        |\n|    time_elapsed    | 7241      |\n|    total_timesteps | 431163    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.13     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 431040    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3144      |\n|    fps             | 59        |\n|    time_elapsed    | 7245      |\n|    total_timesteps | 431420    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0883    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 431296    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3148      |\n|    fps             | 59        |\n|    time_elapsed    | 7250      |\n|    total_timesteps | 431677    |\n| train/             |           |\n|    actor_loss      | 1.08e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.00854   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 431552    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3152      |\n|    fps             | 59        |\n|    time_elapsed    | 7254      |\n|    total_timesteps | 431933    |\n| train/             |           |\n|    actor_loss      | 1.07e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.131     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 431808    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 3156     |\n|    fps             | 59       |\n|    time_elapsed    | 7259     |\n|    total_timesteps | 432189   |\n| train/             |          |\n|    actor_loss      | 1.08e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0146  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 432064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 3160     |\n|    fps             | 59       |\n|    time_elapsed    | 7263     |\n|    total_timesteps | 432444   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0422   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 432320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 3164     |\n|    fps             | 59       |\n|    time_elapsed    | 7268     |\n|    total_timesteps | 432700   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0416   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 432576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3168     |\n|    fps             | 59       |\n|    time_elapsed    | 7272     |\n|    total_timesteps | 432956   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0565   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 432832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3172     |\n|    fps             | 59       |\n|    time_elapsed    | 7276     |\n|    total_timesteps | 433214   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 433088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 3176     |\n|    fps             | 59       |\n|    time_elapsed    | 7281     |\n|    total_timesteps | 433470   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 433344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 3180     |\n|    fps             | 59       |\n|    time_elapsed    | 7285     |\n|    total_timesteps | 433727   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0263  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 433600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 3184     |\n|    fps             | 59       |\n|    time_elapsed    | 7290     |\n|    total_timesteps | 433986   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.00409  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 433920   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 3188     |\n|    fps             | 59       |\n|    time_elapsed    | 7295     |\n|    total_timesteps | 434244   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0582  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 434176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 3192     |\n|    fps             | 59       |\n|    time_elapsed    | 7299     |\n|    total_timesteps | 434502   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.106   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 434432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -970     |\n| time/              |          |\n|    episodes        | 3196     |\n|    fps             | 59       |\n|    time_elapsed    | 7303     |\n|    total_timesteps | 434759   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0249   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 434688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -976     |\n| time/              |          |\n|    episodes        | 3200     |\n|    fps             | 59       |\n|    time_elapsed    | 7308     |\n|    total_timesteps | 435019   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0959   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 434944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3204     |\n|    fps             | 59       |\n|    time_elapsed    | 7312     |\n|    total_timesteps | 435277   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.289   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 435200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 3208     |\n|    fps             | 59       |\n|    time_elapsed    | 7317     |\n|    total_timesteps | 435535   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0563  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 435456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 3212     |\n|    fps             | 59       |\n|    time_elapsed    | 7321     |\n|    total_timesteps | 435791   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0245  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 435712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 3216     |\n|    fps             | 59       |\n|    time_elapsed    | 7326     |\n|    total_timesteps | 436048   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0889  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 435968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 3220     |\n|    fps             | 59       |\n|    time_elapsed    | 7330     |\n|    total_timesteps | 436305   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0318  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 436224   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 3224     |\n|    fps             | 59       |\n|    time_elapsed    | 7334     |\n|    total_timesteps | 436564   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0839  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 436480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 3228     |\n|    fps             | 59       |\n|    time_elapsed    | 7339     |\n|    total_timesteps | 436822   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0723   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 436736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 3232     |\n|    fps             | 59       |\n|    time_elapsed    | 7343     |\n|    total_timesteps | 437079   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0762   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 436992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 3236     |\n|    fps             | 59       |\n|    time_elapsed    | 7348     |\n|    total_timesteps | 437337   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0343  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 437248   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 3240     |\n|    fps             | 59       |\n|    time_elapsed    | 7352     |\n|    total_timesteps | 437595   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00747  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 437504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 3244     |\n|    fps             | 59       |\n|    time_elapsed    | 7356     |\n|    total_timesteps | 437853   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0608  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 437760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -970     |\n| time/              |          |\n|    episodes        | 3248     |\n|    fps             | 59       |\n|    time_elapsed    | 7361     |\n|    total_timesteps | 438111   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 438016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 3252     |\n|    fps             | 59       |\n|    time_elapsed    | 7365     |\n|    total_timesteps | 438371   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0567   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 438272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 3256     |\n|    fps             | 59       |\n|    time_elapsed    | 7369     |\n|    total_timesteps | 438629   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.149   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 438528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -976     |\n| time/              |          |\n|    episodes        | 3260     |\n|    fps             | 59       |\n|    time_elapsed    | 7374     |\n|    total_timesteps | 438891   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0289   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 438784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -976     |\n| time/              |          |\n|    episodes        | 3264     |\n|    fps             | 59       |\n|    time_elapsed    | 7378     |\n|    total_timesteps | 439151   |\n| train/             |          |\n|    actor_loss      | 1.07e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.14     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 439040   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3268     |\n|    fps             | 59       |\n|    time_elapsed    | 7383     |\n|    total_timesteps | 439411   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0347   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 439296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 3272     |\n|    fps             | 59       |\n|    time_elapsed    | 7387     |\n|    total_timesteps | 439673   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.117   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 439552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 3276     |\n|    fps             | 59       |\n|    time_elapsed    | 7392     |\n|    total_timesteps | 439935   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0333  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 439808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 3280     |\n|    fps             | 59       |\n|    time_elapsed    | 7397     |\n|    total_timesteps | 440196   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0774   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 440128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 3284     |\n|    fps             | 59       |\n|    time_elapsed    | 7401     |\n|    total_timesteps | 440456   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.161    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 440384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 3288     |\n|    fps             | 59       |\n|    time_elapsed    | 7406     |\n|    total_timesteps | 440722   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0131   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 440640   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 3292     |\n|    fps             | 59       |\n|    time_elapsed    | 7410     |\n|    total_timesteps | 440981   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00183  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 440896   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3296     |\n|    fps             | 59       |\n|    time_elapsed    | 7415     |\n|    total_timesteps | 441242   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0429   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 441152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 3300     |\n|    fps             | 59       |\n|    time_elapsed    | 7419     |\n|    total_timesteps | 441502   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0361   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 441408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 3304     |\n|    fps             | 59       |\n|    time_elapsed    | 7424     |\n|    total_timesteps | 441763   |\n| train/             |          |\n|    actor_loss      | 1.06e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.000375 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 441664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -976     |\n| time/              |          |\n|    episodes        | 3308     |\n|    fps             | 59       |\n|    time_elapsed    | 7428     |\n|    total_timesteps | 442029   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0276  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 441920   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 3312     |\n|    fps             | 59       |\n|    time_elapsed    | 7432     |\n|    total_timesteps | 442292   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.143    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 442176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 3316     |\n|    fps             | 59       |\n|    time_elapsed    | 7438     |\n|    total_timesteps | 442561   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.11     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 442496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 3320     |\n|    fps             | 59       |\n|    time_elapsed    | 7442     |\n|    total_timesteps | 442824   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0897  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 442752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 3324     |\n|    fps             | 59       |\n|    time_elapsed    | 7446     |\n|    total_timesteps | 443095   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.158   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 443008   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 3328     |\n|    fps             | 59       |\n|    time_elapsed    | 7451     |\n|    total_timesteps | 443363   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0298   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 443264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 3332     |\n|    fps             | 59       |\n|    time_elapsed    | 7456     |\n|    total_timesteps | 443654   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 443584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 3336     |\n|    fps             | 59       |\n|    time_elapsed    | 7460     |\n|    total_timesteps | 443940   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.149   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 443840   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.5     |\n|    ep_rew_mean     | -991     |\n| time/              |          |\n|    episodes        | 3340     |\n|    fps             | 59       |\n|    time_elapsed    | 7466     |\n|    total_timesteps | 444242   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0165  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 444160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3344     |\n|    fps             | 59       |\n|    time_elapsed    | 7471     |\n|    total_timesteps | 444564   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | -0.00164 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 444480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.6      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3348      |\n|    fps             | 59        |\n|    time_elapsed    | 7476      |\n|    total_timesteps | 444874    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | -0.0216   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 444800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3352      |\n|    fps             | 59        |\n|    time_elapsed    | 7482      |\n|    total_timesteps | 445189    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.00649   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 445120    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3356      |\n|    fps             | 59        |\n|    time_elapsed    | 7487      |\n|    total_timesteps | 445537    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | -0.196    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 445440    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.9      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 3360      |\n|    fps             | 59        |\n|    time_elapsed    | 7493      |\n|    total_timesteps | 445881    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.222     |\n|    ent_coef_loss   | 0.0144    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 445760    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 71        |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 3364      |\n|    fps             | 59        |\n|    time_elapsed    | 7499      |\n|    total_timesteps | 446253    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0758   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 446144    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 72.6      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 3368      |\n|    fps             | 59        |\n|    time_elapsed    | 7506      |\n|    total_timesteps | 446668    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | -0.0301   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 446592    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 74        |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 3372      |\n|    fps             | 59        |\n|    time_elapsed    | 7513      |\n|    total_timesteps | 447072    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.0725   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 446976    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 75.3      |\n|    ep_rew_mean     | -1.16e+03 |\n| time/              |           |\n|    episodes        | 3376      |\n|    fps             | 59        |\n|    time_elapsed    | 7519      |\n|    total_timesteps | 447466    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | -0.0399   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 447360    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 76.7      |\n|    ep_rew_mean     | -1.17e+03 |\n| time/              |           |\n|    episodes        | 3380      |\n|    fps             | 59        |\n|    time_elapsed    | 7526      |\n|    total_timesteps | 447869    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.161     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 447744    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 78.2      |\n|    ep_rew_mean     | -1.21e+03 |\n| time/              |           |\n|    episodes        | 3384      |\n|    fps             | 59        |\n|    time_elapsed    | 7533      |\n|    total_timesteps | 448279    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.229     |\n|    ent_coef_loss   | 0.0832    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 448192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 80        |\n|    ep_rew_mean     | -1.25e+03 |\n| time/              |           |\n|    episodes        | 3388      |\n|    fps             | 59        |\n|    time_elapsed    | 7540      |\n|    total_timesteps | 448723    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | -0.0158   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 448640    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 82        |\n|    ep_rew_mean     | -1.29e+03 |\n| time/              |           |\n|    episodes        | 3392      |\n|    fps             | 59        |\n|    time_elapsed    | 7548      |\n|    total_timesteps | 449184    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.227     |\n|    ent_coef_loss   | 0.158     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 449088    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 84.2      |\n|    ep_rew_mean     | -1.34e+03 |\n| time/              |           |\n|    episodes        | 3396      |\n|    fps             | 59        |\n|    time_elapsed    | 7557      |\n|    total_timesteps | 449667    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.226     |\n|    ent_coef_loss   | 0.0234    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 449600    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 87        |\n|    ep_rew_mean     | -1.41e+03 |\n| time/              |           |\n|    episodes        | 3400      |\n|    fps             | 59        |\n|    time_elapsed    | 7566      |\n|    total_timesteps | 450200    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.223     |\n|    ent_coef_loss   | 0.0211    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 450112    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 89.6      |\n|    ep_rew_mean     | -1.45e+03 |\n| time/              |           |\n|    episodes        | 3404      |\n|    fps             | 59        |\n|    time_elapsed    | 7574      |\n|    total_timesteps | 450725    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0362    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 450624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 92.6      |\n|    ep_rew_mean     | -1.53e+03 |\n| time/              |           |\n|    episodes        | 3408      |\n|    fps             | 59        |\n|    time_elapsed    | 7584      |\n|    total_timesteps | 451286    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 14.4      |\n|    ent_coef        | 0.225     |\n|    ent_coef_loss   | 0.0414    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 451200    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 95.9     |\n|    ep_rew_mean     | -1.6e+03 |\n| time/              |          |\n|    episodes        | 3412     |\n|    fps             | 59       |\n|    time_elapsed    | 7594     |\n|    total_timesteps | 451884   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 14       |\n|    ent_coef        | 0.224    |\n|    ent_coef_loss   | 0.0784   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 451776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 99.1      |\n|    ep_rew_mean     | -1.68e+03 |\n| time/              |           |\n|    episodes        | 3416      |\n|    fps             | 59        |\n|    time_elapsed    | 7603      |\n|    total_timesteps | 452470    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.222     |\n|    ent_coef_loss   | 0.138     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 452352    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 102       |\n|    ep_rew_mean     | -1.74e+03 |\n| time/              |           |\n|    episodes        | 3420      |\n|    fps             | 59        |\n|    time_elapsed    | 7612      |\n|    total_timesteps | 452983    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.224     |\n|    ent_coef_loss   | 0.0168    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 452864    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 104      |\n|    ep_rew_mean     | -1.8e+03 |\n| time/              |          |\n|    episodes        | 3424     |\n|    fps             | 59       |\n|    time_elapsed    | 7621     |\n|    total_timesteps | 453534   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.116   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 453440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 107       |\n|    ep_rew_mean     | -1.86e+03 |\n| time/              |           |\n|    episodes        | 3428      |\n|    fps             | 59        |\n|    time_elapsed    | 7631      |\n|    total_timesteps | 454107    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.22      |\n|    ent_coef_loss   | -0.0603   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 454016    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 110       |\n|    ep_rew_mean     | -1.93e+03 |\n| time/              |           |\n|    episodes        | 3432      |\n|    fps             | 59        |\n|    time_elapsed    | 7639      |\n|    total_timesteps | 454605    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.221     |\n|    ent_coef_loss   | 0.0739    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 454528    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 112       |\n|    ep_rew_mean     | -1.98e+03 |\n| time/              |           |\n|    episodes        | 3436      |\n|    fps             | 59        |\n|    time_elapsed    | 7647      |\n|    total_timesteps | 455104    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 14.2      |\n|    ent_coef        | 0.219     |\n|    ent_coef_loss   | -0.0304   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 454976    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 114       |\n|    ep_rew_mean     | -2.05e+03 |\n| time/              |           |\n|    episodes        | 3440      |\n|    fps             | 59        |\n|    time_elapsed    | 7657      |\n|    total_timesteps | 455643    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0643   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 455552    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 118       |\n|    ep_rew_mean     | -2.12e+03 |\n| time/              |           |\n|    episodes        | 3444      |\n|    fps             | 59        |\n|    time_elapsed    | 7668      |\n|    total_timesteps | 456321    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0413    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 456256    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 120       |\n|    ep_rew_mean     | -2.17e+03 |\n| time/              |           |\n|    episodes        | 3448      |\n|    fps             | 59        |\n|    time_elapsed    | 7678      |\n|    total_timesteps | 456911    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.125     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 456832    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 123       |\n|    ep_rew_mean     | -2.21e+03 |\n| time/              |           |\n|    episodes        | 3452      |\n|    fps             | 59        |\n|    time_elapsed    | 7688      |\n|    total_timesteps | 457476    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.105     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 457408    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 124       |\n|    ep_rew_mean     | -2.23e+03 |\n| time/              |           |\n|    episodes        | 3456      |\n|    fps             | 59        |\n|    time_elapsed    | 7695      |\n|    total_timesteps | 457962    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.105    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 457856    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 126       |\n|    ep_rew_mean     | -2.25e+03 |\n| time/              |           |\n|    episodes        | 3460      |\n|    fps             | 59        |\n|    time_elapsed    | 7704      |\n|    total_timesteps | 458470    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.155     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 458368    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 126       |\n|    ep_rew_mean     | -2.22e+03 |\n| time/              |           |\n|    episodes        | 3464      |\n|    fps             | 59        |\n|    time_elapsed    | 7711      |\n|    total_timesteps | 458821    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0187   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 458752    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 125      |\n|    ep_rew_mean     | -2.2e+03 |\n| time/              |          |\n|    episodes        | 3468     |\n|    fps             | 59       |\n|    time_elapsed    | 7715     |\n|    total_timesteps | 459129   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0872  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 459008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 123       |\n|    ep_rew_mean     | -2.19e+03 |\n| time/              |           |\n|    episodes        | 3472      |\n|    fps             | 59        |\n|    time_elapsed    | 7720      |\n|    total_timesteps | 459384    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0771    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 459264    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 122       |\n|    ep_rew_mean     | -2.16e+03 |\n| time/              |           |\n|    episodes        | 3476      |\n|    fps             | 59        |\n|    time_elapsed    | 7725      |\n|    total_timesteps | 459668    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.133    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 459584    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 121       |\n|    ep_rew_mean     | -2.13e+03 |\n| time/              |           |\n|    episodes        | 3480      |\n|    fps             | 59        |\n|    time_elapsed    | 7730      |\n|    total_timesteps | 459923    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.135     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 459840    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 119      |\n|    ep_rew_mean     | -2.1e+03 |\n| time/              |          |\n|    episodes        | 3484     |\n|    fps             | 59       |\n|    time_elapsed    | 7734     |\n|    total_timesteps | 460180   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0343  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 460096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 117       |\n|    ep_rew_mean     | -2.07e+03 |\n| time/              |           |\n|    episodes        | 3488      |\n|    fps             | 59        |\n|    time_elapsed    | 7739      |\n|    total_timesteps | 460452    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 14.1      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0527    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 460352    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 115       |\n|    ep_rew_mean     | -2.03e+03 |\n| time/              |           |\n|    episodes        | 3492      |\n|    fps             | 59        |\n|    time_elapsed    | 7743      |\n|    total_timesteps | 460709    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.238    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 460608    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 113       |\n|    ep_rew_mean     | -1.98e+03 |\n| time/              |           |\n|    episodes        | 3496      |\n|    fps             | 59        |\n|    time_elapsed    | 7748      |\n|    total_timesteps | 460965    |\n| train/             |           |\n|    actor_loss      | 1.06e+03  |\n|    critic_loss     | 13.9      |\n|    ent_coef        | 0.22      |\n|    ent_coef_loss   | 0.201     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 460864    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 110       |\n|    ep_rew_mean     | -1.91e+03 |\n| time/              |           |\n|    episodes        | 3500      |\n|    fps             | 59        |\n|    time_elapsed    | 7752      |\n|    total_timesteps | 461222    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.22      |\n|    ent_coef_loss   | -0.144    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 461120    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 108       |\n|    ep_rew_mean     | -1.87e+03 |\n| time/              |           |\n|    episodes        | 3504      |\n|    fps             | 59        |\n|    time_elapsed    | 7757      |\n|    total_timesteps | 461478    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | -0.0665   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 461376    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 104       |\n|    ep_rew_mean     | -1.78e+03 |\n| time/              |           |\n|    episodes        | 3508      |\n|    fps             | 59        |\n|    time_elapsed    | 7761      |\n|    total_timesteps | 461734    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0698    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 461632    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 101      |\n|    ep_rew_mean     | -1.7e+03 |\n| time/              |          |\n|    episodes        | 3512     |\n|    fps             | 59       |\n|    time_elapsed    | 7766     |\n|    total_timesteps | 461990   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.164   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 461888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 97.8      |\n|    ep_rew_mean     | -1.62e+03 |\n| time/              |           |\n|    episodes        | 3516      |\n|    fps             | 59        |\n|    time_elapsed    | 7770      |\n|    total_timesteps | 462246    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0253    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 462144    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 95.2      |\n|    ep_rew_mean     | -1.56e+03 |\n| time/              |           |\n|    episodes        | 3520      |\n|    fps             | 59        |\n|    time_elapsed    | 7774      |\n|    total_timesteps | 462502    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.219     |\n|    ent_coef_loss   | -0.0356   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 462400    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 92.2      |\n|    ep_rew_mean     | -1.49e+03 |\n| time/              |           |\n|    episodes        | 3524      |\n|    fps             | 59        |\n|    time_elapsed    | 7779      |\n|    total_timesteps | 462757    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.214     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 462656    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 89        |\n|    ep_rew_mean     | -1.43e+03 |\n| time/              |           |\n|    episodes        | 3528      |\n|    fps             | 59        |\n|    time_elapsed    | 7783      |\n|    total_timesteps | 463011    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 14        |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.016     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 462912    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 86.6      |\n|    ep_rew_mean     | -1.37e+03 |\n| time/              |           |\n|    episodes        | 3532      |\n|    fps             | 59        |\n|    time_elapsed    | 7788      |\n|    total_timesteps | 463268    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | 0.0351    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 463168    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 84.2     |\n|    ep_rew_mean     | -1.3e+03 |\n| time/              |          |\n|    episodes        | 3536     |\n|    fps             | 59       |\n|    time_elapsed    | 7792     |\n|    total_timesteps | 463523   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00612 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 463424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 81.3      |\n|    ep_rew_mean     | -1.23e+03 |\n| time/              |           |\n|    episodes        | 3540      |\n|    fps             | 59        |\n|    time_elapsed    | 7797      |\n|    total_timesteps | 463778    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.0406    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 463680    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 77.1      |\n|    ep_rew_mean     | -1.15e+03 |\n| time/              |           |\n|    episodes        | 3544      |\n|    fps             | 59        |\n|    time_elapsed    | 7801      |\n|    total_timesteps | 464034    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.22      |\n|    ent_coef_loss   | -0.104    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 463936    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 73.8      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 3548      |\n|    fps             | 59        |\n|    time_elapsed    | 7805      |\n|    total_timesteps | 464289    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.221     |\n|    ent_coef_loss   | 0.144     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 464192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 70.7      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 3552      |\n|    fps             | 59        |\n|    time_elapsed    | 7810      |\n|    total_timesteps | 464545    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.7      |\n|    ent_coef        | 0.221     |\n|    ent_coef_loss   | -0.038    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 464448    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3556      |\n|    fps             | 59        |\n|    time_elapsed    | 7814      |\n|    total_timesteps | 464801    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.222     |\n|    ent_coef_loss   | 0.0397    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 464704    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -976     |\n| time/              |          |\n|    episodes        | 3560     |\n|    fps             | 59       |\n|    time_elapsed    | 7818     |\n|    total_timesteps | 465057   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | -0.14    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 464960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3564     |\n|    fps             | 59       |\n|    time_elapsed    | 7823     |\n|    total_timesteps | 465313   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.9     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 465216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 3568     |\n|    fps             | 59       |\n|    time_elapsed    | 7827     |\n|    total_timesteps | 465569   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0227   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 465472   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 3572     |\n|    fps             | 59       |\n|    time_elapsed    | 7832     |\n|    total_timesteps | 465825   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.089   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 465728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 3576     |\n|    fps             | 59       |\n|    time_elapsed    | 7836     |\n|    total_timesteps | 466081   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0428  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 465984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 3580     |\n|    fps             | 59       |\n|    time_elapsed    | 7841     |\n|    total_timesteps | 466337   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0291   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 466240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -958     |\n| time/              |          |\n|    episodes        | 3584     |\n|    fps             | 59       |\n|    time_elapsed    | 7845     |\n|    total_timesteps | 466594   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0647   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 466496   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 3588     |\n|    fps             | 59       |\n|    time_elapsed    | 7849     |\n|    total_timesteps | 466847   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0681   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 466752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 3592     |\n|    fps             | 59       |\n|    time_elapsed    | 7854     |\n|    total_timesteps | 467103   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.6     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0214   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 467008   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 3596     |\n|    fps             | 59       |\n|    time_elapsed    | 7858     |\n|    total_timesteps | 467359   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.111   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 467264   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 3600     |\n|    fps             | 59       |\n|    time_elapsed    | 7863     |\n|    total_timesteps | 467615   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.14    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 467520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 3604     |\n|    fps             | 59       |\n|    time_elapsed    | 7867     |\n|    total_timesteps | 467872   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0293   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 467776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 3608     |\n|    fps             | 59       |\n|    time_elapsed    | 7871     |\n|    total_timesteps | 468129   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0774  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 468032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -981     |\n| time/              |          |\n|    episodes        | 3612     |\n|    fps             | 59       |\n|    time_elapsed    | 7876     |\n|    total_timesteps | 468385   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0303  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 468288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 3616     |\n|    fps             | 59       |\n|    time_elapsed    | 7880     |\n|    total_timesteps | 468642   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0526  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 468544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 3620     |\n|    fps             | 59       |\n|    time_elapsed    | 7885     |\n|    total_timesteps | 468898   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0862   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 468800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3624     |\n|    fps             | 59       |\n|    time_elapsed    | 7889     |\n|    total_timesteps | 469154   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.166   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 469056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -996     |\n| time/              |          |\n|    episodes        | 3628     |\n|    fps             | 59       |\n|    time_elapsed    | 7894     |\n|    total_timesteps | 469410   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 469312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3632     |\n|    fps             | 59       |\n|    time_elapsed    | 7898     |\n|    total_timesteps | 469665   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0418   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 469568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3636     |\n|    fps             | 59       |\n|    time_elapsed    | 7902     |\n|    total_timesteps | 469921   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0283   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 469824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3640      |\n|    fps             | 59        |\n|    time_elapsed    | 7907      |\n|    total_timesteps | 470177    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | -0.154    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 470080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3644      |\n|    fps             | 59        |\n|    time_elapsed    | 7911      |\n|    total_timesteps | 470433    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0354    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 470336    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3648      |\n|    fps             | 59        |\n|    time_elapsed    | 7915      |\n|    total_timesteps | 470689    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.0762    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 470592    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3652      |\n|    fps             | 59        |\n|    time_elapsed    | 7920      |\n|    total_timesteps | 470946    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.029     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 470848    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3656      |\n|    fps             | 59        |\n|    time_elapsed    | 7924      |\n|    total_timesteps | 471203    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0331   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 471104    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3660      |\n|    fps             | 59        |\n|    time_elapsed    | 7929      |\n|    total_timesteps | 471459    |\n| train/             |           |\n|    actor_loss      | 1.01e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.0723    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 471360    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3664      |\n|    fps             | 59        |\n|    time_elapsed    | 7933      |\n|    total_timesteps | 471715    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.103    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 471616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3668      |\n|    fps             | 59        |\n|    time_elapsed    | 7937      |\n|    total_timesteps | 471971    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.0467    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 471872    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3672      |\n|    fps             | 59        |\n|    time_elapsed    | 7942      |\n|    total_timesteps | 472227    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.172     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 472128    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3676     |\n|    fps             | 59       |\n|    time_elapsed    | 7946     |\n|    total_timesteps | 472483   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00538  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 472384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3680      |\n|    fps             | 59        |\n|    time_elapsed    | 7951      |\n|    total_timesteps | 472739    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.00308  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 472640    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3684      |\n|    fps             | 59        |\n|    time_elapsed    | 7955      |\n|    total_timesteps | 472996    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0036   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 472896    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3688      |\n|    fps             | 59        |\n|    time_elapsed    | 7960      |\n|    total_timesteps | 473253    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.0624    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 473152    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3692      |\n|    fps             | 59        |\n|    time_elapsed    | 7964      |\n|    total_timesteps | 473509    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.8      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.112     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 473408    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3696      |\n|    fps             | 59        |\n|    time_elapsed    | 7968      |\n|    total_timesteps | 473764    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0965    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 473664    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3700      |\n|    fps             | 59        |\n|    time_elapsed    | 7973      |\n|    total_timesteps | 474022    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.179     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 473920    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3704      |\n|    fps             | 59        |\n|    time_elapsed    | 7977      |\n|    total_timesteps | 474279    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.00709   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 474176    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3708      |\n|    fps             | 59        |\n|    time_elapsed    | 7982      |\n|    total_timesteps | 474535    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0264   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 474432    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3712      |\n|    fps             | 59        |\n|    time_elapsed    | 7986      |\n|    total_timesteps | 474792    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.4      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0389   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 474688    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3716      |\n|    fps             | 59        |\n|    time_elapsed    | 7991      |\n|    total_timesteps | 475048    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.6      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.028    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 474944    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3720      |\n|    fps             | 59        |\n|    time_elapsed    | 7995      |\n|    total_timesteps | 475304    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0547    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 475200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3724      |\n|    fps             | 59        |\n|    time_elapsed    | 8000      |\n|    total_timesteps | 475562    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.2      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.0874   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 475456    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3728      |\n|    fps             | 59        |\n|    time_elapsed    | 8004      |\n|    total_timesteps | 475818    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13.5      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.11     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 475712    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3732      |\n|    fps             | 59        |\n|    time_elapsed    | 8009      |\n|    total_timesteps | 476074    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.091    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 475968    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3736      |\n|    fps             | 59        |\n|    time_elapsed    | 8013      |\n|    total_timesteps | 476330    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.24     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 476224    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3740      |\n|    fps             | 59        |\n|    time_elapsed    | 8018      |\n|    total_timesteps | 476587    |\n| train/             |           |\n|    actor_loss      | 1.01e+03  |\n|    critic_loss     | 12.7      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.0119   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 476480    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3744      |\n|    fps             | 59        |\n|    time_elapsed    | 8022      |\n|    total_timesteps | 476844    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 12.9      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.0584   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 476736    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3748      |\n|    fps             | 59        |\n|    time_elapsed    | 8027      |\n|    total_timesteps | 477100    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 12.9      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.145    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 476992    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3752      |\n|    fps             | 59        |\n|    time_elapsed    | 8031      |\n|    total_timesteps | 477360    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.0343    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 477248    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3756      |\n|    fps             | 59        |\n|    time_elapsed    | 8035      |\n|    total_timesteps | 477616    |\n| train/             |           |\n|    actor_loss      | 1.03e+03  |\n|    critic_loss     | 13.3      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.0956   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 477504    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3760     |\n|    fps             | 59       |\n|    time_elapsed    | 8039     |\n|    total_timesteps | 477873   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.145   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 477760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3764     |\n|    fps             | 59       |\n|    time_elapsed    | 8044     |\n|    total_timesteps | 478130   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0322  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 478016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3768     |\n|    fps             | 59       |\n|    time_elapsed    | 8048     |\n|    total_timesteps | 478387   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.073    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 478272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3772      |\n|    fps             | 59        |\n|    time_elapsed    | 8053      |\n|    total_timesteps | 478643    |\n| train/             |           |\n|    actor_loss      | 1.05e+03  |\n|    critic_loss     | 12.8      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.122    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 478528    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3776      |\n|    fps             | 59        |\n|    time_elapsed    | 8057      |\n|    total_timesteps | 478900    |\n| train/             |           |\n|    actor_loss      | 1.01e+03  |\n|    critic_loss     | 12.8      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.153    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 478784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3780      |\n|    fps             | 59        |\n|    time_elapsed    | 8061      |\n|    total_timesteps | 479157    |\n| train/             |           |\n|    actor_loss      | 1.02e+03  |\n|    critic_loss     | 13        |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.011     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 479040    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 3784      |\n|    fps             | 59        |\n|    time_elapsed    | 8066      |\n|    total_timesteps | 479413    |\n| train/             |           |\n|    actor_loss      | 1.04e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.00291   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 479296    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 3788      |\n|    fps             | 59        |\n|    time_elapsed    | 8070      |\n|    total_timesteps | 479669    |\n| train/             |           |\n|    actor_loss      | 1.01e+03  |\n|    critic_loss     | 13.1      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.171    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 479552    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3792     |\n|    fps             | 59       |\n|    time_elapsed    | 8074     |\n|    total_timesteps | 479925   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0451  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 479808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3796     |\n|    fps             | 59       |\n|    time_elapsed    | 8079     |\n|    total_timesteps | 480181   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0542  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 480064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 3800     |\n|    fps             | 59       |\n|    time_elapsed    | 8083     |\n|    total_timesteps | 480437   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.165    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 480320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 3804     |\n|    fps             | 59       |\n|    time_elapsed    | 8087     |\n|    total_timesteps | 480694   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00874 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 480576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 3808     |\n|    fps             | 59       |\n|    time_elapsed    | 8092     |\n|    total_timesteps | 480951   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.256   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 480832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -992     |\n| time/              |          |\n|    episodes        | 3812     |\n|    fps             | 59       |\n|    time_elapsed    | 8096     |\n|    total_timesteps | 481208   |\n| train/             |          |\n|    actor_loss      | 1.04e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.216    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 481088   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 3816     |\n|    fps             | 59       |\n|    time_elapsed    | 8100     |\n|    total_timesteps | 481464   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 481344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 3820     |\n|    fps             | 59       |\n|    time_elapsed    | 8105     |\n|    total_timesteps | 481721   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0983   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 481600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 3824     |\n|    fps             | 59       |\n|    time_elapsed    | 8109     |\n|    total_timesteps | 481977   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0766   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 481856   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 3828     |\n|    fps             | 59       |\n|    time_elapsed    | 8113     |\n|    total_timesteps | 482233   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0521   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 482112   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -978     |\n| time/              |          |\n|    episodes        | 3832     |\n|    fps             | 59       |\n|    time_elapsed    | 8117     |\n|    total_timesteps | 482489   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0853   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 482368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -977     |\n| time/              |          |\n|    episodes        | 3836     |\n|    fps             | 59       |\n|    time_elapsed    | 8122     |\n|    total_timesteps | 482745   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0131  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 482624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 3840     |\n|    fps             | 59       |\n|    time_elapsed    | 8126     |\n|    total_timesteps | 483001   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0774   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 482880   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 3844     |\n|    fps             | 59       |\n|    time_elapsed    | 8130     |\n|    total_timesteps | 483258   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0947   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 483136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 3848     |\n|    fps             | 59       |\n|    time_elapsed    | 8135     |\n|    total_timesteps | 483514   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.00637  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 483392   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 3852     |\n|    fps             | 59       |\n|    time_elapsed    | 8139     |\n|    total_timesteps | 483770   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.239   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 483648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 3856     |\n|    fps             | 59       |\n|    time_elapsed    | 8144     |\n|    total_timesteps | 484027   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.086    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 483904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3860     |\n|    fps             | 59       |\n|    time_elapsed    | 8148     |\n|    total_timesteps | 484283   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0274   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 484160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 3864     |\n|    fps             | 59       |\n|    time_elapsed    | 8152     |\n|    total_timesteps | 484539   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.000705 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 484416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -975     |\n| time/              |          |\n|    episodes        | 3868     |\n|    fps             | 59       |\n|    time_elapsed    | 8157     |\n|    total_timesteps | 484796   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 484672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 3872     |\n|    fps             | 59       |\n|    time_elapsed    | 8161     |\n|    total_timesteps | 485052   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0864   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 484928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 3876     |\n|    fps             | 59       |\n|    time_elapsed    | 8166     |\n|    total_timesteps | 485309   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.131   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 485184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 3880     |\n|    fps             | 59       |\n|    time_elapsed    | 8170     |\n|    total_timesteps | 485565   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0248  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 485440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -958     |\n| time/              |          |\n|    episodes        | 3884     |\n|    fps             | 59       |\n|    time_elapsed    | 8174     |\n|    total_timesteps | 485820   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.166   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 485696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 3888     |\n|    fps             | 59       |\n|    time_elapsed    | 8179     |\n|    total_timesteps | 486076   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0321   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 485952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 3892     |\n|    fps             | 59       |\n|    time_elapsed    | 8183     |\n|    total_timesteps | 486333   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0499   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 486208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 3896     |\n|    fps             | 59       |\n|    time_elapsed    | 8187     |\n|    total_timesteps | 486589   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00454 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 486464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 3900     |\n|    fps             | 59       |\n|    time_elapsed    | 8192     |\n|    total_timesteps | 486845   |\n| train/             |          |\n|    actor_loss      | 1.05e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.034    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 486720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 3904     |\n|    fps             | 59       |\n|    time_elapsed    | 8196     |\n|    total_timesteps | 487101   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.00543  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 486976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 3908     |\n|    fps             | 59       |\n|    time_elapsed    | 8200     |\n|    total_timesteps | 487356   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00285 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 487232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 3912     |\n|    fps             | 59       |\n|    time_elapsed    | 8205     |\n|    total_timesteps | 487613   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0188   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 487488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 3916     |\n|    fps             | 59       |\n|    time_elapsed    | 8209     |\n|    total_timesteps | 487868   |\n| train/             |          |\n|    actor_loss      | 1.03e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0736  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 487744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -940     |\n| time/              |          |\n|    episodes        | 3920     |\n|    fps             | 59       |\n|    time_elapsed    | 8213     |\n|    total_timesteps | 488124   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0368  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 488000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 3924     |\n|    fps             | 59       |\n|    time_elapsed    | 8218     |\n|    total_timesteps | 488380   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0544  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 488256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -948     |\n| time/              |          |\n|    episodes        | 3928     |\n|    fps             | 59       |\n|    time_elapsed    | 8222     |\n|    total_timesteps | 488636   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.113    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 488512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 3932     |\n|    fps             | 59       |\n|    time_elapsed    | 8226     |\n|    total_timesteps | 488892   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.17     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 488768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 3936     |\n|    fps             | 59       |\n|    time_elapsed    | 8231     |\n|    total_timesteps | 489149   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0268   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 489024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -955     |\n| time/              |          |\n|    episodes        | 3940     |\n|    fps             | 59       |\n|    time_elapsed    | 8235     |\n|    total_timesteps | 489407   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 489280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 3944     |\n|    fps             | 59       |\n|    time_elapsed    | 8239     |\n|    total_timesteps | 489663   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0488   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 489536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 3948     |\n|    fps             | 59       |\n|    time_elapsed    | 8244     |\n|    total_timesteps | 489919   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0865   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 489792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 3952     |\n|    fps             | 59       |\n|    time_elapsed    | 8248     |\n|    total_timesteps | 490175   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.115    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 490048   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 3956     |\n|    fps             | 59       |\n|    time_elapsed    | 8252     |\n|    total_timesteps | 490432   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0419  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 490304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 3960     |\n|    fps             | 59       |\n|    time_elapsed    | 8257     |\n|    total_timesteps | 490688   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0864   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 490560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 3964     |\n|    fps             | 59       |\n|    time_elapsed    | 8261     |\n|    total_timesteps | 490944   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 490816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -924     |\n| time/              |          |\n|    episodes        | 3968     |\n|    fps             | 59       |\n|    time_elapsed    | 8266     |\n|    total_timesteps | 491200   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0699   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 491072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -926     |\n| time/              |          |\n|    episodes        | 3972     |\n|    fps             | 59       |\n|    time_elapsed    | 8270     |\n|    total_timesteps | 491456   |\n| train/             |          |\n|    actor_loss      | 1.02e+03 |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0305   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 491328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -924     |\n| time/              |          |\n|    episodes        | 3976     |\n|    fps             | 59       |\n|    time_elapsed    | 8275     |\n|    total_timesteps | 491712   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0121  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 491584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -918     |\n| time/              |          |\n|    episodes        | 3980     |\n|    fps             | 59       |\n|    time_elapsed    | 8279     |\n|    total_timesteps | 491968   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0295  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 491840   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -925     |\n| time/              |          |\n|    episodes        | 3984     |\n|    fps             | 59       |\n|    time_elapsed    | 8283     |\n|    total_timesteps | 492224   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0958   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 492096   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 3988     |\n|    fps             | 59       |\n|    time_elapsed    | 8288     |\n|    total_timesteps | 492480   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0824  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 492352   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 3992     |\n|    fps             | 59       |\n|    time_elapsed    | 8292     |\n|    total_timesteps | 492736   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.065   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 492608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -922     |\n| time/              |          |\n|    episodes        | 3996     |\n|    fps             | 59       |\n|    time_elapsed    | 8297     |\n|    total_timesteps | 492993   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 492928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -922     |\n| time/              |          |\n|    episodes        | 4000     |\n|    fps             | 59       |\n|    time_elapsed    | 8302     |\n|    total_timesteps | 493250   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00518  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 493184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 4004     |\n|    fps             | 59       |\n|    time_elapsed    | 8306     |\n|    total_timesteps | 493506   |\n| train/             |          |\n|    actor_loss      | 999      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 493440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 4008     |\n|    fps             | 59       |\n|    time_elapsed    | 8310     |\n|    total_timesteps | 493764   |\n| train/             |          |\n|    actor_loss      | 993      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0705  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 493696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4012     |\n|    fps             | 59       |\n|    time_elapsed    | 8315     |\n|    total_timesteps | 494021   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0365  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 493952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 4016     |\n|    fps             | 59       |\n|    time_elapsed    | 8319     |\n|    total_timesteps | 494277   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.167    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 494208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 4020     |\n|    fps             | 59       |\n|    time_elapsed    | 8324     |\n|    total_timesteps | 494533   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00973 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 494464   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 4024     |\n|    fps             | 59       |\n|    time_elapsed    | 8328     |\n|    total_timesteps | 494791   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0552   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 494720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4028     |\n|    fps             | 59       |\n|    time_elapsed    | 8333     |\n|    total_timesteps | 495048   |\n| train/             |          |\n|    actor_loss      | 999      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0728  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 494976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4032     |\n|    fps             | 59       |\n|    time_elapsed    | 8337     |\n|    total_timesteps | 495304   |\n| train/             |          |\n|    actor_loss      | 998      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0976   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 495232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4036     |\n|    fps             | 59       |\n|    time_elapsed    | 8341     |\n|    total_timesteps | 495561   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 495488   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 4040     |\n|    fps             | 59       |\n|    time_elapsed    | 8346     |\n|    total_timesteps | 495817   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 495744   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 4044     |\n|    fps             | 59       |\n|    time_elapsed    | 8350     |\n|    total_timesteps | 496075   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0965  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 496000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 4048     |\n|    fps             | 59       |\n|    time_elapsed    | 8355     |\n|    total_timesteps | 496332   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0483   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 496256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 4052     |\n|    fps             | 59       |\n|    time_elapsed    | 8359     |\n|    total_timesteps | 496590   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0448   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 496512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4056     |\n|    fps             | 59       |\n|    time_elapsed    | 8363     |\n|    total_timesteps | 496847   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 496768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 4060     |\n|    fps             | 59       |\n|    time_elapsed    | 8368     |\n|    total_timesteps | 497103   |\n| train/             |          |\n|    actor_loss      | 1.01e+03 |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0402   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 497024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 4064     |\n|    fps             | 59       |\n|    time_elapsed    | 8372     |\n|    total_timesteps | 497361   |\n| train/             |          |\n|    actor_loss      | 996      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 497280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4068     |\n|    fps             | 59       |\n|    time_elapsed    | 8377     |\n|    total_timesteps | 497619   |\n| train/             |          |\n|    actor_loss      | 987      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0176   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 497536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4072     |\n|    fps             | 59       |\n|    time_elapsed    | 8381     |\n|    total_timesteps | 497876   |\n| train/             |          |\n|    actor_loss      | 998      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.111   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 497792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 4076     |\n|    fps             | 59       |\n|    time_elapsed    | 8386     |\n|    total_timesteps | 498134   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0505   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 498048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -958     |\n| time/              |          |\n|    episodes        | 4080     |\n|    fps             | 59       |\n|    time_elapsed    | 8390     |\n|    total_timesteps | 498391   |\n| train/             |          |\n|    actor_loss      | 988      |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0203  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 498304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4084     |\n|    fps             | 59       |\n|    time_elapsed    | 8394     |\n|    total_timesteps | 498648   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0597   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 498560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 4088     |\n|    fps             | 59       |\n|    time_elapsed    | 8399     |\n|    total_timesteps | 498904   |\n| train/             |          |\n|    actor_loss      | 991      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0763  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 498816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4092     |\n|    fps             | 59       |\n|    time_elapsed    | 8403     |\n|    total_timesteps | 499161   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0315   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 499072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 4096     |\n|    fps             | 59       |\n|    time_elapsed    | 8408     |\n|    total_timesteps | 499417   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0344   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 499328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -952     |\n| time/              |          |\n|    episodes        | 4100     |\n|    fps             | 59       |\n|    time_elapsed    | 8412     |\n|    total_timesteps | 499674   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.138    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 499584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -944     |\n| time/              |          |\n|    episodes        | 4104     |\n|    fps             | 59       |\n|    time_elapsed    | 8416     |\n|    total_timesteps | 499930   |\n| train/             |          |\n|    actor_loss      | 992      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0103  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 499840   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 4108     |\n|    fps             | 59       |\n|    time_elapsed    | 8421     |\n|    total_timesteps | 500187   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0324   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 500096   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -948     |\n| time/              |          |\n|    episodes        | 4112     |\n|    fps             | 59       |\n|    time_elapsed    | 8425     |\n|    total_timesteps | 500443   |\n| train/             |          |\n|    actor_loss      | 998      |\n|    critic_loss     | 13.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.185   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 500352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -952     |\n| time/              |          |\n|    episodes        | 4116     |\n|    fps             | 59       |\n|    time_elapsed    | 8430     |\n|    total_timesteps | 500702   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 500608   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4120     |\n|    fps             | 59       |\n|    time_elapsed    | 8434     |\n|    total_timesteps | 500960   |\n| train/             |          |\n|    actor_loss      | 988      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0353   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 500864   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4124     |\n|    fps             | 59       |\n|    time_elapsed    | 8438     |\n|    total_timesteps | 501220   |\n| train/             |          |\n|    actor_loss      | 988      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 501120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4128     |\n|    fps             | 59       |\n|    time_elapsed    | 8443     |\n|    total_timesteps | 501478   |\n| train/             |          |\n|    actor_loss      | 999      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0542  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 501376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4132     |\n|    fps             | 59       |\n|    time_elapsed    | 8447     |\n|    total_timesteps | 501735   |\n| train/             |          |\n|    actor_loss      | 987      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0465   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 501632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -952     |\n| time/              |          |\n|    episodes        | 4136     |\n|    fps             | 59       |\n|    time_elapsed    | 8452     |\n|    total_timesteps | 501992   |\n| train/             |          |\n|    actor_loss      | 983      |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0375   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 501888   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -954     |\n| time/              |          |\n|    episodes        | 4140     |\n|    fps             | 59       |\n|    time_elapsed    | 8456     |\n|    total_timesteps | 502250   |\n| train/             |          |\n|    actor_loss      | 982      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0503  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 502144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4144     |\n|    fps             | 59       |\n|    time_elapsed    | 8461     |\n|    total_timesteps | 502508   |\n| train/             |          |\n|    actor_loss      | 997      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.00396 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 502400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -961     |\n| time/              |          |\n|    episodes        | 4148     |\n|    fps             | 59       |\n|    time_elapsed    | 8465     |\n|    total_timesteps | 502767   |\n| train/             |          |\n|    actor_loss      | 994      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.11    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 502656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 4152     |\n|    fps             | 59       |\n|    time_elapsed    | 8470     |\n|    total_timesteps | 503024   |\n| train/             |          |\n|    actor_loss      | 987      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0225  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 502912   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4156     |\n|    fps             | 59       |\n|    time_elapsed    | 8474     |\n|    total_timesteps | 503281   |\n| train/             |          |\n|    actor_loss      | 983      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0328   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 503168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4160     |\n|    fps             | 59       |\n|    time_elapsed    | 8478     |\n|    total_timesteps | 503537   |\n| train/             |          |\n|    actor_loss      | 991      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0261   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 503424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -955     |\n| time/              |          |\n|    episodes        | 4164     |\n|    fps             | 59       |\n|    time_elapsed    | 8483     |\n|    total_timesteps | 503793   |\n| train/             |          |\n|    actor_loss      | 984      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.02    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 503680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 4168     |\n|    fps             | 59       |\n|    time_elapsed    | 8487     |\n|    total_timesteps | 504049   |\n| train/             |          |\n|    actor_loss      | 992      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0237   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 503936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 4172     |\n|    fps             | 59       |\n|    time_elapsed    | 8492     |\n|    total_timesteps | 504306   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0153   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 504192   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 4176     |\n|    fps             | 59       |\n|    time_elapsed    | 8496     |\n|    total_timesteps | 504564   |\n| train/             |          |\n|    actor_loss      | 987      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0013  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 504448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 4180     |\n|    fps             | 59       |\n|    time_elapsed    | 8500     |\n|    total_timesteps | 504820   |\n| train/             |          |\n|    actor_loss      | 992      |\n|    critic_loss     | 13.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0275  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 504704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -948     |\n| time/              |          |\n|    episodes        | 4184     |\n|    fps             | 59       |\n|    time_elapsed    | 8505     |\n|    total_timesteps | 505076   |\n| train/             |          |\n|    actor_loss      | 994      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0835  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 504960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -944     |\n| time/              |          |\n|    episodes        | 4188     |\n|    fps             | 59       |\n|    time_elapsed    | 8509     |\n|    total_timesteps | 505334   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0718   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 505216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4192     |\n|    fps             | 59       |\n|    time_elapsed    | 8514     |\n|    total_timesteps | 505591   |\n| train/             |          |\n|    actor_loss      | 985      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.131    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 505472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -951     |\n| time/              |          |\n|    episodes        | 4196     |\n|    fps             | 59       |\n|    time_elapsed    | 8518     |\n|    total_timesteps | 505848   |\n| train/             |          |\n|    actor_loss      | 976      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.161   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 505728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2297.934397350957\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 4200     |\n|    fps             | 59       |\n|    time_elapsed    | 8523     |\n|    total_timesteps | 506104   |\n| train/             |          |\n|    actor_loss      | 988      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0729   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 505984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4204     |\n|    fps             | 59       |\n|    time_elapsed    | 8527     |\n|    total_timesteps | 506360   |\n| train/             |          |\n|    actor_loss      | 986      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.18    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 506240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4208     |\n|    fps             | 59       |\n|    time_elapsed    | 8531     |\n|    total_timesteps | 506617   |\n| train/             |          |\n|    actor_loss      | 986      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.13    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 506496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 4212     |\n|    fps             | 59       |\n|    time_elapsed    | 8536     |\n|    total_timesteps | 506874   |\n| train/             |          |\n|    actor_loss      | 1e+03    |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.012    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 506752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2292.951381659318\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4216     |\n|    fps             | 59       |\n|    time_elapsed    | 8540     |\n|    total_timesteps | 507132   |\n| train/             |          |\n|    actor_loss      | 979      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 507008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4220     |\n|    fps             | 59       |\n|    time_elapsed    | 8545     |\n|    total_timesteps | 507391   |\n| train/             |          |\n|    actor_loss      | 991      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0246   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 507264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4224     |\n|    fps             | 59       |\n|    time_elapsed    | 8550     |\n|    total_timesteps | 507649   |\n| train/             |          |\n|    actor_loss      | 974      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0496  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 507584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 4228     |\n|    fps             | 59       |\n|    time_elapsed    | 8554     |\n|    total_timesteps | 507908   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0295  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 507840   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2288.522772044686\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 4232     |\n|    fps             | 59       |\n|    time_elapsed    | 8559     |\n|    total_timesteps | 508165   |\n| train/             |          |\n|    actor_loss      | 983      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0724  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 508096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 4236     |\n|    fps             | 59       |\n|    time_elapsed    | 8563     |\n|    total_timesteps | 508421   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0474   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 508352   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 4240     |\n|    fps             | 59       |\n|    time_elapsed    | 8568     |\n|    total_timesteps | 508680   |\n| train/             |          |\n|    actor_loss      | 978      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.238   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 508608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 4244     |\n|    fps             | 59       |\n|    time_elapsed    | 8572     |\n|    total_timesteps | 508938   |\n| train/             |          |\n|    actor_loss      | 987      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0222  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 508864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2283.7672414953668\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 4248     |\n|    fps             | 59       |\n|    time_elapsed    | 8577     |\n|    total_timesteps | 509196   |\n| train/             |          |\n|    actor_loss      | 986      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0338   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 509120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 4252     |\n|    fps             | 59       |\n|    time_elapsed    | 8581     |\n|    total_timesteps | 509452   |\n| train/             |          |\n|    actor_loss      | 981      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0796   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 509376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4256     |\n|    fps             | 59       |\n|    time_elapsed    | 8586     |\n|    total_timesteps | 509711   |\n| train/             |          |\n|    actor_loss      | 972      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0784  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 509632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4260     |\n|    fps             | 59       |\n|    time_elapsed    | 8590     |\n|    total_timesteps | 509967   |\n| train/             |          |\n|    actor_loss      | 988      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 509888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2278.589362808579\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4264     |\n|    fps             | 59       |\n|    time_elapsed    | 8594     |\n|    total_timesteps | 510225   |\n| train/             |          |\n|    actor_loss      | 977      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0323  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 510144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 4268     |\n|    fps             | 59       |\n|    time_elapsed    | 8599     |\n|    total_timesteps | 510483   |\n| train/             |          |\n|    actor_loss      | 986      |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0422  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 510400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 4272     |\n|    fps             | 59       |\n|    time_elapsed    | 8603     |\n|    total_timesteps | 510740   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.172   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 510656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 4276     |\n|    fps             | 59       |\n|    time_elapsed    | 8608     |\n|    total_timesteps | 510997   |\n| train/             |          |\n|    actor_loss      | 985      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 510912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2273.761650264573\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 4280     |\n|    fps             | 59       |\n|    time_elapsed    | 8612     |\n|    total_timesteps | 511256   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 511168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 4284     |\n|    fps             | 59       |\n|    time_elapsed    | 8616     |\n|    total_timesteps | 511513   |\n| train/             |          |\n|    actor_loss      | 998      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0719   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 511424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 4288     |\n|    fps             | 59       |\n|    time_elapsed    | 8621     |\n|    total_timesteps | 511771   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0512  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 511680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2268.8934408146088\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4292     |\n|    fps             | 59       |\n|    time_elapsed    | 8625     |\n|    total_timesteps | 512027   |\n| train/             |          |\n|    actor_loss      | 998      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.065   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 511936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 4296     |\n|    fps             | 59       |\n|    time_elapsed    | 8629     |\n|    total_timesteps | 512284   |\n| train/             |          |\n|    actor_loss      | 979      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 512192   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 4300     |\n|    fps             | 59       |\n|    time_elapsed    | 8634     |\n|    total_timesteps | 512543   |\n| train/             |          |\n|    actor_loss      | 981      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.101   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 512448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -967     |\n| time/              |          |\n|    episodes        | 4304     |\n|    fps             | 59       |\n|    time_elapsed    | 8638     |\n|    total_timesteps | 512802   |\n| train/             |          |\n|    actor_loss      | 970      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 512704   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2264.194209364743\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 4308     |\n|    fps             | 59       |\n|    time_elapsed    | 8643     |\n|    total_timesteps | 513060   |\n| train/             |          |\n|    actor_loss      | 989      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | 0.103    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 512960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -972     |\n| time/              |          |\n|    episodes        | 4312     |\n|    fps             | 59       |\n|    time_elapsed    | 8647     |\n|    total_timesteps | 513317   |\n| train/             |          |\n|    actor_loss      | 992      |\n|    critic_loss     | 13.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.249    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 513216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4316     |\n|    fps             | 59       |\n|    time_elapsed    | 8651     |\n|    total_timesteps | 513574   |\n| train/             |          |\n|    actor_loss      | 977      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0757  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 513472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 4320     |\n|    fps             | 59       |\n|    time_elapsed    | 8656     |\n|    total_timesteps | 513830   |\n| train/             |          |\n|    actor_loss      | 976      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0208   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 513728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2259.390206642941\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -948     |\n| time/              |          |\n|    episodes        | 4324     |\n|    fps             | 59       |\n|    time_elapsed    | 8660     |\n|    total_timesteps | 514088   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0169  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 513984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 4328     |\n|    fps             | 59       |\n|    time_elapsed    | 8664     |\n|    total_timesteps | 514346   |\n| train/             |          |\n|    actor_loss      | 978      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.045   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 514240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 4332     |\n|    fps             | 59       |\n|    time_elapsed    | 8669     |\n|    total_timesteps | 514603   |\n| train/             |          |\n|    actor_loss      | 970      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.185   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 514496   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4336     |\n|    fps             | 59       |\n|    time_elapsed    | 8673     |\n|    total_timesteps | 514859   |\n| train/             |          |\n|    actor_loss      | 985      |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 514752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2254.123782633093\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 4340     |\n|    fps             | 59       |\n|    time_elapsed    | 8678     |\n|    total_timesteps | 515117   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0327   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 515008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 4344     |\n|    fps             | 59       |\n|    time_elapsed    | 8682     |\n|    total_timesteps | 515373   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.00927 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 515264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -913     |\n| time/              |          |\n|    episodes        | 4348     |\n|    fps             | 59       |\n|    time_elapsed    | 8686     |\n|    total_timesteps | 515630   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0552  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 515520   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -921     |\n| time/              |          |\n|    episodes        | 4352     |\n|    fps             | 59       |\n|    time_elapsed    | 8691     |\n|    total_timesteps | 515887   |\n| train/             |          |\n|    actor_loss      | 984      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 515776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2249.5915371924366\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4356     |\n|    fps             | 59       |\n|    time_elapsed    | 8695     |\n|    total_timesteps | 516144   |\n| train/             |          |\n|    actor_loss      | 989      |\n|    critic_loss     | 12.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 516032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4360     |\n|    fps             | 59       |\n|    time_elapsed    | 8700     |\n|    total_timesteps | 516400   |\n| train/             |          |\n|    actor_loss      | 966      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.224    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 516288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -921     |\n| time/              |          |\n|    episodes        | 4364     |\n|    fps             | 59       |\n|    time_elapsed    | 8704     |\n|    total_timesteps | 516658   |\n| train/             |          |\n|    actor_loss      | 973      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.00906 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 516544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 4368     |\n|    fps             | 59       |\n|    time_elapsed    | 8708     |\n|    total_timesteps | 516914   |\n| train/             |          |\n|    actor_loss      | 983      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0192  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 516800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2244.8971496753306\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -918     |\n| time/              |          |\n|    episodes        | 4372     |\n|    fps             | 59       |\n|    time_elapsed    | 8713     |\n|    total_timesteps | 517170   |\n| train/             |          |\n|    actor_loss      | 983      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0405  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 517056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -915     |\n| time/              |          |\n|    episodes        | 4376     |\n|    fps             | 59       |\n|    time_elapsed    | 8717     |\n|    total_timesteps | 517428   |\n| train/             |          |\n|    actor_loss      | 977      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 517312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -915     |\n| time/              |          |\n|    episodes        | 4380     |\n|    fps             | 59       |\n|    time_elapsed    | 8721     |\n|    total_timesteps | 517685   |\n| train/             |          |\n|    actor_loss      | 984      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.16     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 517568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -920     |\n| time/              |          |\n|    episodes        | 4384     |\n|    fps             | 59       |\n|    time_elapsed    | 8726     |\n|    total_timesteps | 517945   |\n| train/             |          |\n|    actor_loss      | 995      |\n|    critic_loss     | 12.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 517824   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2240.4808531324156\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -920     |\n| time/              |          |\n|    episodes        | 4388     |\n|    fps             | 59       |\n|    time_elapsed    | 8731     |\n|    total_timesteps | 518203   |\n| train/             |          |\n|    actor_loss      | 977      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 518080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 4392     |\n|    fps             | 59       |\n|    time_elapsed    | 8735     |\n|    total_timesteps | 518463   |\n| train/             |          |\n|    actor_loss      | 975      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.00561  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 518336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -939     |\n| time/              |          |\n|    episodes        | 4396     |\n|    fps             | 59       |\n|    time_elapsed    | 8739     |\n|    total_timesteps | 518720   |\n| train/             |          |\n|    actor_loss      | 990      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00781 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 518592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -940     |\n| time/              |          |\n|    episodes        | 4400     |\n|    fps             | 59       |\n|    time_elapsed    | 8745     |\n|    total_timesteps | 518978   |\n| train/             |          |\n|    actor_loss      | 979      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.14     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 518912   |\n---------------------------------\nNew best mean reward across all envs: -2236.056136212468\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 4404     |\n|    fps             | 59       |\n|    time_elapsed    | 8749     |\n|    total_timesteps | 519235   |\n| train/             |          |\n|    actor_loss      | 985      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.107    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 519168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 4408     |\n|    fps             | 59       |\n|    time_elapsed    | 8753     |\n|    total_timesteps | 519493   |\n| train/             |          |\n|    actor_loss      | 982      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 519424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4412     |\n|    fps             | 59       |\n|    time_elapsed    | 8758     |\n|    total_timesteps | 519752   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.101   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 519680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2232.2031578865654\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 4416     |\n|    fps             | 59       |\n|    time_elapsed    | 8762     |\n|    total_timesteps | 520009   |\n| train/             |          |\n|    actor_loss      | 984      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.184    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 519936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 4420     |\n|    fps             | 59       |\n|    time_elapsed    | 8767     |\n|    total_timesteps | 520266   |\n| train/             |          |\n|    actor_loss      | 963      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 520192   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -978     |\n| time/              |          |\n|    episodes        | 4424     |\n|    fps             | 59       |\n|    time_elapsed    | 8771     |\n|    total_timesteps | 520524   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0496  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 520448   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -978     |\n| time/              |          |\n|    episodes        | 4428     |\n|    fps             | 59       |\n|    time_elapsed    | 8775     |\n|    total_timesteps | 520781   |\n| train/             |          |\n|    actor_loss      | 964      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0975  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 520704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2227.6206559737884\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -979     |\n| time/              |          |\n|    episodes        | 4432     |\n|    fps             | 59       |\n|    time_elapsed    | 8780     |\n|    total_timesteps | 521039   |\n| train/             |          |\n|    actor_loss      | 980      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0979   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 520960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 4436     |\n|    fps             | 59       |\n|    time_elapsed    | 8784     |\n|    total_timesteps | 521295   |\n| train/             |          |\n|    actor_loss      | 974      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.029   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 521216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 4440     |\n|    fps             | 59       |\n|    time_elapsed    | 8788     |\n|    total_timesteps | 521552   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0283   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 521472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -998     |\n| time/              |          |\n|    episodes        | 4444     |\n|    fps             | 59       |\n|    time_elapsed    | 8793     |\n|    total_timesteps | 521810   |\n| train/             |          |\n|    actor_loss      | 978      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0592   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 521728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2223.5589569991835\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 4448     |\n|    fps             | 59       |\n|    time_elapsed    | 8797     |\n|    total_timesteps | 522067   |\n| train/             |          |\n|    actor_loss      | 976      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0572  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 521984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 4452     |\n|    fps             | 59       |\n|    time_elapsed    | 8802     |\n|    total_timesteps | 522323   |\n| train/             |          |\n|    actor_loss      | 974      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.00481  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 522240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -986     |\n| time/              |          |\n|    episodes        | 4456     |\n|    fps             | 59       |\n|    time_elapsed    | 8806     |\n|    total_timesteps | 522579   |\n| train/             |          |\n|    actor_loss      | 986      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.00678 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 522496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 4460     |\n|    fps             | 59       |\n|    time_elapsed    | 8810     |\n|    total_timesteps | 522836   |\n| train/             |          |\n|    actor_loss      | 977      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0531  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 522752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2219.067271619493\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 4464      |\n|    fps             | 59        |\n|    time_elapsed    | 8815      |\n|    total_timesteps | 523093    |\n| train/             |           |\n|    actor_loss      | 962       |\n|    critic_loss     | 12        |\n|    ent_coef        | 0.205     |\n|    ent_coef_loss   | -0.0517   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 523008    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 4468      |\n|    fps             | 59        |\n|    time_elapsed    | 8819      |\n|    total_timesteps | 523350    |\n| train/             |           |\n|    actor_loss      | 968       |\n|    critic_loss     | 12.7      |\n|    ent_coef        | 0.205     |\n|    ent_coef_loss   | 0.0734    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 523264    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 4472      |\n|    fps             | 59        |\n|    time_elapsed    | 8824      |\n|    total_timesteps | 523606    |\n| train/             |           |\n|    actor_loss      | 968       |\n|    critic_loss     | 12.5      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.0609    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 523520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 4476      |\n|    fps             | 59        |\n|    time_elapsed    | 8828      |\n|    total_timesteps | 523864    |\n| train/             |           |\n|    actor_loss      | 975       |\n|    critic_loss     | 12.3      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.0233    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 523776    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2214.714914270664\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.4      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 4480      |\n|    fps             | 59        |\n|    time_elapsed    | 8832      |\n|    total_timesteps | 524122    |\n| train/             |           |\n|    actor_loss      | 973       |\n|    critic_loss     | 12.3      |\n|    ent_coef        | 0.206     |\n|    ent_coef_loss   | 0.094     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 524032    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 4484      |\n|    fps             | 59        |\n|    time_elapsed    | 8837      |\n|    total_timesteps | 524380    |\n| train/             |           |\n|    actor_loss      | 970       |\n|    critic_loss     | 12.7      |\n|    ent_coef        | 0.206     |\n|    ent_coef_loss   | 0.127     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 524288    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 4488      |\n|    fps             | 59        |\n|    time_elapsed    | 8841      |\n|    total_timesteps | 524636    |\n| train/             |           |\n|    actor_loss      | 960       |\n|    critic_loss     | 12.5      |\n|    ent_coef        | 0.206     |\n|    ent_coef_loss   | -0.0491   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 524544    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.3      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 4492      |\n|    fps             | 59        |\n|    time_elapsed    | 8846      |\n|    total_timesteps | 524894    |\n| train/             |           |\n|    actor_loss      | 972       |\n|    critic_loss     | 12.2      |\n|    ent_coef        | 0.202     |\n|    ent_coef_loss   | -0.118    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 524800    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2210.526410209097\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 4496     |\n|    fps             | 59       |\n|    time_elapsed    | 8850     |\n|    total_timesteps | 525150   |\n| train/             |          |\n|    actor_loss      | 978      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.145   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 525056   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -989     |\n| time/              |          |\n|    episodes        | 4500     |\n|    fps             | 59       |\n|    time_elapsed    | 8855     |\n|    total_timesteps | 525407   |\n| train/             |          |\n|    actor_loss      | 979      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0901   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 525312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -981     |\n| time/              |          |\n|    episodes        | 4504     |\n|    fps             | 59       |\n|    time_elapsed    | 8859     |\n|    total_timesteps | 525664   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.144   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 525568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -981     |\n| time/              |          |\n|    episodes        | 4508     |\n|    fps             | 59       |\n|    time_elapsed    | 8863     |\n|    total_timesteps | 525921   |\n| train/             |          |\n|    actor_loss      | 965      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | -0.0948  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 525824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2205.944335795235\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -983     |\n| time/              |          |\n|    episodes        | 4512     |\n|    fps             | 59       |\n|    time_elapsed    | 8868     |\n|    total_timesteps | 526179   |\n| train/             |          |\n|    actor_loss      | 961      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0673   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 526080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 4516     |\n|    fps             | 59       |\n|    time_elapsed    | 8872     |\n|    total_timesteps | 526436   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0529   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 526336   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 4520     |\n|    fps             | 59       |\n|    time_elapsed    | 8877     |\n|    total_timesteps | 526694   |\n| train/             |          |\n|    actor_loss      | 968      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0182  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 526592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 4524     |\n|    fps             | 59       |\n|    time_elapsed    | 8881     |\n|    total_timesteps | 526954   |\n| train/             |          |\n|    actor_loss      | 961      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0297   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 526848   |\n---------------------------------\nNew best mean reward across all envs: -2202.211930639029\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 4528     |\n|    fps             | 59       |\n|    time_elapsed    | 8886     |\n|    total_timesteps | 527211   |\n| train/             |          |\n|    actor_loss      | 976      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.119    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 527104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 4532     |\n|    fps             | 59       |\n|    time_elapsed    | 8890     |\n|    total_timesteps | 527467   |\n| train/             |          |\n|    actor_loss      | 965      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.273   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 527360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 4536     |\n|    fps             | 59       |\n|    time_elapsed    | 8895     |\n|    total_timesteps | 527726   |\n| train/             |          |\n|    actor_loss      | 965      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0148   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 527616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 4540     |\n|    fps             | 59       |\n|    time_elapsed    | 8899     |\n|    total_timesteps | 527984   |\n| train/             |          |\n|    actor_loss      | 973      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0575   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 527872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2198.04842630791\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 4544     |\n|    fps             | 59       |\n|    time_elapsed    | 8904     |\n|    total_timesteps | 528240   |\n| train/             |          |\n|    actor_loss      | 971      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0781   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 528128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -994     |\n| time/              |          |\n|    episodes        | 4548     |\n|    fps             | 59       |\n|    time_elapsed    | 8908     |\n|    total_timesteps | 528499   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0513   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 528384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -997     |\n| time/              |          |\n|    episodes        | 4552     |\n|    fps             | 59       |\n|    time_elapsed    | 8913     |\n|    total_timesteps | 528758   |\n| train/             |          |\n|    actor_loss      | 959      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.286   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 528640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2194.0345081309356\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 4556     |\n|    fps             | 59       |\n|    time_elapsed    | 8917     |\n|    total_timesteps | 529015   |\n| train/             |          |\n|    actor_loss      | 970      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.109   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 528896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -993     |\n| time/              |          |\n|    episodes        | 4560     |\n|    fps             | 59       |\n|    time_elapsed    | 8921     |\n|    total_timesteps | 529273   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0437   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 529152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 4564     |\n|    fps             | 59       |\n|    time_elapsed    | 8926     |\n|    total_timesteps | 529529   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 529408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 4568     |\n|    fps             | 59       |\n|    time_elapsed    | 8930     |\n|    total_timesteps | 529785   |\n| train/             |          |\n|    actor_loss      | 960      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0208   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 529664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2189.625457917009\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 4572     |\n|    fps             | 59       |\n|    time_elapsed    | 8935     |\n|    total_timesteps | 530041   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0286  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 529920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 4576     |\n|    fps             | 59       |\n|    time_elapsed    | 8939     |\n|    total_timesteps | 530298   |\n| train/             |          |\n|    actor_loss      | 961      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | -0.0861  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 530176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -978     |\n| time/              |          |\n|    episodes        | 4580     |\n|    fps             | 59       |\n|    time_elapsed    | 8944     |\n|    total_timesteps | 530554   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0302  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 530432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 4584     |\n|    fps             | 59       |\n|    time_elapsed    | 8948     |\n|    total_timesteps | 530811   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.109   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 530688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2185.5319439130776\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 4588     |\n|    fps             | 59       |\n|    time_elapsed    | 8953     |\n|    total_timesteps | 531067   |\n| train/             |          |\n|    actor_loss      | 966      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0103  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 530944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 4592     |\n|    fps             | 59       |\n|    time_elapsed    | 8957     |\n|    total_timesteps | 531323   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.27    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 531200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 4596     |\n|    fps             | 59       |\n|    time_elapsed    | 8962     |\n|    total_timesteps | 531579   |\n| train/             |          |\n|    actor_loss      | 959      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.0422  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 531456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -971     |\n| time/              |          |\n|    episodes        | 4600     |\n|    fps             | 59       |\n|    time_elapsed    | 8966     |\n|    total_timesteps | 531835   |\n| train/             |          |\n|    actor_loss      | 964      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.033    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 531712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2180.8925972566867\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 4604     |\n|    fps             | 59       |\n|    time_elapsed    | 8970     |\n|    total_timesteps | 532091   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 13       |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0931   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 531968   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 4608     |\n|    fps             | 59       |\n|    time_elapsed    | 8975     |\n|    total_timesteps | 532347   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0388   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 532224   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 4612     |\n|    fps             | 59       |\n|    time_elapsed    | 8979     |\n|    total_timesteps | 532604   |\n| train/             |          |\n|    actor_loss      | 956      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0267   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 532480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 4616     |\n|    fps             | 59       |\n|    time_elapsed    | 8984     |\n|    total_timesteps | 532860   |\n| train/             |          |\n|    actor_loss      | 959      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 532736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2176.455526174437\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 4620     |\n|    fps             | 59       |\n|    time_elapsed    | 8988     |\n|    total_timesteps | 533118   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | 0.0327   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 532992   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 4624     |\n|    fps             | 59       |\n|    time_elapsed    | 8992     |\n|    total_timesteps | 533376   |\n| train/             |          |\n|    actor_loss      | 963      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 533248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -924     |\n| time/              |          |\n|    episodes        | 4628     |\n|    fps             | 59       |\n|    time_elapsed    | 8997     |\n|    total_timesteps | 533632   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.6     |\n|    ent_coef        | 0.198    |\n|    ent_coef_loss   | -0.042   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 533504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 4632     |\n|    fps             | 59       |\n|    time_elapsed    | 9002     |\n|    total_timesteps | 533889   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.198    |\n|    ent_coef_loss   | 0.0809   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 533824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2172.395319078458\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -914     |\n| time/              |          |\n|    episodes        | 4636     |\n|    fps             | 59       |\n|    time_elapsed    | 9006     |\n|    total_timesteps | 534148   |\n| train/             |          |\n|    actor_loss      | 970      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | 0.109    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 534080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -913     |\n| time/              |          |\n|    episodes        | 4640     |\n|    fps             | 59       |\n|    time_elapsed    | 9011     |\n|    total_timesteps | 534406   |\n| train/             |          |\n|    actor_loss      | 957      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | -0.177   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 534336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -912     |\n| time/              |          |\n|    episodes        | 4644     |\n|    fps             | 59       |\n|    time_elapsed    | 9015     |\n|    total_timesteps | 534667   |\n| train/             |          |\n|    actor_loss      | 966      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.0411  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 534592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 4648     |\n|    fps             | 59       |\n|    time_elapsed    | 9020     |\n|    total_timesteps | 534923   |\n| train/             |          |\n|    actor_loss      | 960      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.2      |\n|    ent_coef_loss   | -0.0345  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 534848   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2168.2120759964932\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 4652     |\n|    fps             | 59       |\n|    time_elapsed    | 9024     |\n|    total_timesteps | 535181   |\n| train/             |          |\n|    actor_loss      | 971      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.00116  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 535104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -925     |\n| time/              |          |\n|    episodes        | 4656     |\n|    fps             | 59       |\n|    time_elapsed    | 9028     |\n|    total_timesteps | 535439   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0774   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 535360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 4660     |\n|    fps             | 59       |\n|    time_elapsed    | 9033     |\n|    total_timesteps | 535695   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.16    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 535616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 4664     |\n|    fps             | 59       |\n|    time_elapsed    | 9037     |\n|    total_timesteps | 535951   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | -0.0259  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 535872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2164.549907233931\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 4668     |\n|    fps             | 59       |\n|    time_elapsed    | 9042     |\n|    total_timesteps | 536209   |\n| train/             |          |\n|    actor_loss      | 949      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.154   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 536128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 4672     |\n|    fps             | 59       |\n|    time_elapsed    | 9046     |\n|    total_timesteps | 536465   |\n| train/             |          |\n|    actor_loss      | 960      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0872   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 536384   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 4676     |\n|    fps             | 59       |\n|    time_elapsed    | 9051     |\n|    total_timesteps | 536722   |\n| train/             |          |\n|    actor_loss      | 955      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0981  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 536640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -940     |\n| time/              |          |\n|    episodes        | 4680     |\n|    fps             | 59       |\n|    time_elapsed    | 9055     |\n|    total_timesteps | 536979   |\n| train/             |          |\n|    actor_loss      | 961      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | -0.193   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 536896   |\n---------------------------------\nNew best mean reward across all envs: -2160.5449042449004\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 4684     |\n|    fps             | 59       |\n|    time_elapsed    | 9060     |\n|    total_timesteps | 537235   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 537152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -938     |\n| time/              |          |\n|    episodes        | 4688     |\n|    fps             | 59       |\n|    time_elapsed    | 9064     |\n|    total_timesteps | 537491   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.018    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 537408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 4692     |\n|    fps             | 59       |\n|    time_elapsed    | 9069     |\n|    total_timesteps | 537748   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0628  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 537664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2156.5817149292648\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4696     |\n|    fps             | 59       |\n|    time_elapsed    | 9073     |\n|    total_timesteps | 538005   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0987   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 537920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -955     |\n| time/              |          |\n|    episodes        | 4700     |\n|    fps             | 59       |\n|    time_elapsed    | 9078     |\n|    total_timesteps | 538262   |\n| train/             |          |\n|    actor_loss      | 967      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.136    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 538176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 4704     |\n|    fps             | 59       |\n|    time_elapsed    | 9082     |\n|    total_timesteps | 538519   |\n| train/             |          |\n|    actor_loss      | 965      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0208  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 538432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4708     |\n|    fps             | 59       |\n|    time_elapsed    | 9087     |\n|    total_timesteps | 538776   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.00895 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 538688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2152.3822475469647\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 4712     |\n|    fps             | 59       |\n|    time_elapsed    | 9091     |\n|    total_timesteps | 539032   |\n| train/             |          |\n|    actor_loss      | 968      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0147  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 538944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 4716     |\n|    fps             | 59       |\n|    time_elapsed    | 9096     |\n|    total_timesteps | 539288   |\n| train/             |          |\n|    actor_loss      | 946      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.000327 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 539200   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -961     |\n| time/              |          |\n|    episodes        | 4720     |\n|    fps             | 59       |\n|    time_elapsed    | 9101     |\n|    total_timesteps | 539544   |\n| train/             |          |\n|    actor_loss      | 957      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0417   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 539456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4724     |\n|    fps             | 59       |\n|    time_elapsed    | 9105     |\n|    total_timesteps | 539799   |\n| train/             |          |\n|    actor_loss      | 958      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0183   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 539712   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2148.1611629873764\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -958     |\n| time/              |          |\n|    episodes        | 4728     |\n|    fps             | 59       |\n|    time_elapsed    | 9110     |\n|    total_timesteps | 540056   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.058   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 539968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 4732     |\n|    fps             | 59       |\n|    time_elapsed    | 9114     |\n|    total_timesteps | 540313   |\n| train/             |          |\n|    actor_loss      | 946      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0861   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 540224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -960     |\n| time/              |          |\n|    episodes        | 4736     |\n|    fps             | 59       |\n|    time_elapsed    | 9118     |\n|    total_timesteps | 540571   |\n| train/             |          |\n|    actor_loss      | 951      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0403   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 540480   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -966     |\n| time/              |          |\n|    episodes        | 4740     |\n|    fps             | 59       |\n|    time_elapsed    | 9123     |\n|    total_timesteps | 540827   |\n| train/             |          |\n|    actor_loss      | 951      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0732  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 540736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2144.416745429961\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 4744     |\n|    fps             | 59       |\n|    time_elapsed    | 9127     |\n|    total_timesteps | 541084   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0469  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 540992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 4748     |\n|    fps             | 59       |\n|    time_elapsed    | 9132     |\n|    total_timesteps | 541342   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 541248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -951     |\n| time/              |          |\n|    episodes        | 4752     |\n|    fps             | 59       |\n|    time_elapsed    | 9136     |\n|    total_timesteps | 541600   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.168   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 541504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 4756     |\n|    fps             | 59       |\n|    time_elapsed    | 9141     |\n|    total_timesteps | 541858   |\n| train/             |          |\n|    actor_loss      | 957      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.02     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 541760   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2140.3301156039374\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 4760     |\n|    fps             | 59       |\n|    time_elapsed    | 9145     |\n|    total_timesteps | 542115   |\n| train/             |          |\n|    actor_loss      | 957      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0302   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 542016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 4764     |\n|    fps             | 59       |\n|    time_elapsed    | 9150     |\n|    total_timesteps | 542372   |\n| train/             |          |\n|    actor_loss      | 936      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0768   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 542272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 4768     |\n|    fps             | 59       |\n|    time_elapsed    | 9154     |\n|    total_timesteps | 542629   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 542528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 4772     |\n|    fps             | 59       |\n|    time_elapsed    | 9158     |\n|    total_timesteps | 542886   |\n| train/             |          |\n|    actor_loss      | 952      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0365  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 542784   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2136.697146000316\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -939     |\n| time/              |          |\n|    episodes        | 4776     |\n|    fps             | 59       |\n|    time_elapsed    | 9163     |\n|    total_timesteps | 543143   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0541  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 543040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4780     |\n|    fps             | 59       |\n|    time_elapsed    | 9167     |\n|    total_timesteps | 543399   |\n| train/             |          |\n|    actor_loss      | 952      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0764  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 543296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 4784     |\n|    fps             | 59       |\n|    time_elapsed    | 9172     |\n|    total_timesteps | 543655   |\n| train/             |          |\n|    actor_loss      | 959      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0233   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 543552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 4788     |\n|    fps             | 59       |\n|    time_elapsed    | 9176     |\n|    total_timesteps | 543911   |\n| train/             |          |\n|    actor_loss      | 943      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 543808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2132.4323583366136\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -938     |\n| time/              |          |\n|    episodes        | 4792     |\n|    fps             | 59       |\n|    time_elapsed    | 9181     |\n|    total_timesteps | 544168   |\n| train/             |          |\n|    actor_loss      | 946      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0785   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 544064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -920     |\n| time/              |          |\n|    episodes        | 4796     |\n|    fps             | 59       |\n|    time_elapsed    | 9185     |\n|    total_timesteps | 544425   |\n| train/             |          |\n|    actor_loss      | 945      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.00162 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 544320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -926     |\n| time/              |          |\n|    episodes        | 4800     |\n|    fps             | 59       |\n|    time_elapsed    | 9189     |\n|    total_timesteps | 544681   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0815  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 544576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -928     |\n| time/              |          |\n|    episodes        | 4804     |\n|    fps             | 59       |\n|    time_elapsed    | 9194     |\n|    total_timesteps | 544941   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0652  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 544832   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2128.825571800284\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4808     |\n|    fps             | 59       |\n|    time_elapsed    | 9198     |\n|    total_timesteps | 545197   |\n| train/             |          |\n|    actor_loss      | 951      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0572  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 545088   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 4812     |\n|    fps             | 59       |\n|    time_elapsed    | 9203     |\n|    total_timesteps | 545453   |\n| train/             |          |\n|    actor_loss      | 951      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0674  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 545344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 4816     |\n|    fps             | 59       |\n|    time_elapsed    | 9207     |\n|    total_timesteps | 545710   |\n| train/             |          |\n|    actor_loss      | 942      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0508  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 545600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 4820     |\n|    fps             | 59       |\n|    time_elapsed    | 9211     |\n|    total_timesteps | 545966   |\n| train/             |          |\n|    actor_loss      | 935      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0143  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 545856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2125.106577839937\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -956     |\n| time/              |          |\n|    episodes        | 4824     |\n|    fps             | 59       |\n|    time_elapsed    | 9215     |\n|    total_timesteps | 546223   |\n| train/             |          |\n|    actor_loss      | 947      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0071   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 546112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 4828     |\n|    fps             | 59       |\n|    time_elapsed    | 9220     |\n|    total_timesteps | 546480   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.00107 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 546368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -952     |\n| time/              |          |\n|    episodes        | 4832     |\n|    fps             | 59       |\n|    time_elapsed    | 9224     |\n|    total_timesteps | 546738   |\n| train/             |          |\n|    actor_loss      | 955      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.183    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 546624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 4836     |\n|    fps             | 59       |\n|    time_elapsed    | 9229     |\n|    total_timesteps | 546995   |\n| train/             |          |\n|    actor_loss      | 937      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0713  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 546880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2121.1257572252716\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -938     |\n| time/              |          |\n|    episodes        | 4840     |\n|    fps             | 59       |\n|    time_elapsed    | 9233     |\n|    total_timesteps | 547251   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0468  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 547136   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 4844     |\n|    fps             | 59       |\n|    time_elapsed    | 9237     |\n|    total_timesteps | 547508   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.114    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 547392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -926     |\n| time/              |          |\n|    episodes        | 4848     |\n|    fps             | 59       |\n|    time_elapsed    | 9241     |\n|    total_timesteps | 547763   |\n| train/             |          |\n|    actor_loss      | 962      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0142  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 547648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2117.2373216731035\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -924     |\n| time/              |          |\n|    episodes        | 4852     |\n|    fps             | 59       |\n|    time_elapsed    | 9246     |\n|    total_timesteps | 548018   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.178    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 547904   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4856     |\n|    fps             | 59       |\n|    time_elapsed    | 9250     |\n|    total_timesteps | 548275   |\n| train/             |          |\n|    actor_loss      | 945      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0831  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 548160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 4860     |\n|    fps             | 59       |\n|    time_elapsed    | 9254     |\n|    total_timesteps | 548534   |\n| train/             |          |\n|    actor_loss      | 953      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.05     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 548416   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -940     |\n| time/              |          |\n|    episodes        | 4864     |\n|    fps             | 59       |\n|    time_elapsed    | 9259     |\n|    total_timesteps | 548790   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.064   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 548672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2113.4924182115924\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 4868     |\n|    fps             | 59       |\n|    time_elapsed    | 9263     |\n|    total_timesteps | 549046   |\n| train/             |          |\n|    actor_loss      | 943      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0483   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 548928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -938     |\n| time/              |          |\n|    episodes        | 4872     |\n|    fps             | 59       |\n|    time_elapsed    | 9267     |\n|    total_timesteps | 549303   |\n| train/             |          |\n|    actor_loss      | 944      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.196    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 549184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 4876     |\n|    fps             | 59       |\n|    time_elapsed    | 9272     |\n|    total_timesteps | 549559   |\n| train/             |          |\n|    actor_loss      | 952      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0497  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 549440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 4880     |\n|    fps             | 59       |\n|    time_elapsed    | 9276     |\n|    total_timesteps | 549814   |\n| train/             |          |\n|    actor_loss      | 942      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0189  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 549696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2109.803586438208\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -945     |\n| time/              |          |\n|    episodes        | 4884     |\n|    fps             | 59       |\n|    time_elapsed    | 9280     |\n|    total_timesteps | 550070   |\n| train/             |          |\n|    actor_loss      | 950      |\n|    critic_loss     | 12.7     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 549952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -944     |\n| time/              |          |\n|    episodes        | 4888     |\n|    fps             | 59       |\n|    time_elapsed    | 9284     |\n|    total_timesteps | 550327   |\n| train/             |          |\n|    actor_loss      | 948      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.186    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 550208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 4892     |\n|    fps             | 59       |\n|    time_elapsed    | 9289     |\n|    total_timesteps | 550584   |\n| train/             |          |\n|    actor_loss      | 944      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0134  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 550464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 4896     |\n|    fps             | 59       |\n|    time_elapsed    | 9293     |\n|    total_timesteps | 550841   |\n| train/             |          |\n|    actor_loss      | 949      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 550720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2105.913990390449\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 4900     |\n|    fps             | 59       |\n|    time_elapsed    | 9297     |\n|    total_timesteps | 551097   |\n| train/             |          |\n|    actor_loss      | 946      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.137   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 550976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 4904     |\n|    fps             | 59       |\n|    time_elapsed    | 9302     |\n|    total_timesteps | 551355   |\n| train/             |          |\n|    actor_loss      | 947      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0334   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 551232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 4908     |\n|    fps             | 59       |\n|    time_elapsed    | 9306     |\n|    total_timesteps | 551611   |\n| train/             |          |\n|    actor_loss      | 957      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.107    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 551488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -923     |\n| time/              |          |\n|    episodes        | 4912     |\n|    fps             | 59       |\n|    time_elapsed    | 9310     |\n|    total_timesteps | 551868   |\n| train/             |          |\n|    actor_loss      | 933      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0415  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 551744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2102.0050279652405\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -915     |\n| time/              |          |\n|    episodes        | 4916     |\n|    fps             | 59       |\n|    time_elapsed    | 9315     |\n|    total_timesteps | 552125   |\n| train/             |          |\n|    actor_loss      | 944      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0463   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 552000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 4920     |\n|    fps             | 59       |\n|    time_elapsed    | 9319     |\n|    total_timesteps | 552384   |\n| train/             |          |\n|    actor_loss      | 938      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 552256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 4924     |\n|    fps             | 59       |\n|    time_elapsed    | 9324     |\n|    total_timesteps | 552641   |\n| train/             |          |\n|    actor_loss      | 936      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0229  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 552576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -911     |\n| time/              |          |\n|    episodes        | 4928     |\n|    fps             | 59       |\n|    time_elapsed    | 9328     |\n|    total_timesteps | 552899   |\n| train/             |          |\n|    actor_loss      | 940      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.158    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 552832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2098.4764700270825\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -911     |\n| time/              |          |\n|    episodes        | 4932     |\n|    fps             | 59       |\n|    time_elapsed    | 9333     |\n|    total_timesteps | 553156   |\n| train/             |          |\n|    actor_loss      | 939      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0317   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 553088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -908     |\n| time/              |          |\n|    episodes        | 4936     |\n|    fps             | 59       |\n|    time_elapsed    | 9337     |\n|    total_timesteps | 553414   |\n| train/             |          |\n|    actor_loss      | 943      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.0182   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 553344   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -914     |\n| time/              |          |\n|    episodes        | 4940     |\n|    fps             | 59       |\n|    time_elapsed    | 9341     |\n|    total_timesteps | 553672   |\n| train/             |          |\n|    actor_loss      | 937      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0404  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 553600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 4944     |\n|    fps             | 59       |\n|    time_elapsed    | 9346     |\n|    total_timesteps | 553928   |\n| train/             |          |\n|    actor_loss      | 939      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 553856   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2094.3580832461917\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 4948     |\n|    fps             | 59       |\n|    time_elapsed    | 9350     |\n|    total_timesteps | 554188   |\n| train/             |          |\n|    actor_loss      | 937      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.107    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 554112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -921     |\n| time/              |          |\n|    episodes        | 4952     |\n|    fps             | 59       |\n|    time_elapsed    | 9354     |\n|    total_timesteps | 554446   |\n| train/             |          |\n|    actor_loss      | 955      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 554368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 4956     |\n|    fps             | 59       |\n|    time_elapsed    | 9359     |\n|    total_timesteps | 554702   |\n| train/             |          |\n|    actor_loss      | 932      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0299   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 554624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 4960     |\n|    fps             | 59       |\n|    time_elapsed    | 9363     |\n|    total_timesteps | 554960   |\n| train/             |          |\n|    actor_loss      | 934      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.201    |\n|    ent_coef_loss   | 0.0737   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 554880   |\n---------------------------------\nNew best mean reward across all envs: -2090.7648916427465\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 4964     |\n|    fps             | 59       |\n|    time_elapsed    | 9367     |\n|    total_timesteps | 555216   |\n| train/             |          |\n|    actor_loss      | 933      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0483   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 555136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -901     |\n| time/              |          |\n|    episodes        | 4968     |\n|    fps             | 59       |\n|    time_elapsed    | 9372     |\n|    total_timesteps | 555475   |\n| train/             |          |\n|    actor_loss      | 956      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0714   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 555392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 4972     |\n|    fps             | 59       |\n|    time_elapsed    | 9376     |\n|    total_timesteps | 555733   |\n| train/             |          |\n|    actor_loss      | 951      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0373   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 555648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 4976     |\n|    fps             | 59       |\n|    time_elapsed    | 9380     |\n|    total_timesteps | 555991   |\n| train/             |          |\n|    actor_loss      | 952      |\n|    critic_loss     | 12.5     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0748  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 555904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2086.6299551141565\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 4980     |\n|    fps             | 59       |\n|    time_elapsed    | 9385     |\n|    total_timesteps | 556251   |\n| train/             |          |\n|    actor_loss      | 954      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0824   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 556160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 4984     |\n|    fps             | 59       |\n|    time_elapsed    | 9389     |\n|    total_timesteps | 556507   |\n| train/             |          |\n|    actor_loss      | 933      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0197  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 556416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 4988     |\n|    fps             | 59       |\n|    time_elapsed    | 9393     |\n|    total_timesteps | 556763   |\n| train/             |          |\n|    actor_loss      | 938      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.03     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 556672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2082.933352474938\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 4992     |\n|    fps             | 59       |\n|    time_elapsed    | 9398     |\n|    total_timesteps | 557022   |\n| train/             |          |\n|    actor_loss      | 944      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0419   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 556928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 4996     |\n|    fps             | 59       |\n|    time_elapsed    | 9402     |\n|    total_timesteps | 557282   |\n| train/             |          |\n|    actor_loss      | 932      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0636  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 557184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 5000     |\n|    fps             | 59       |\n|    time_elapsed    | 9407     |\n|    total_timesteps | 557540   |\n| train/             |          |\n|    actor_loss      | 928      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.251   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 557440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 5004     |\n|    fps             | 59       |\n|    time_elapsed    | 9411     |\n|    total_timesteps | 557798   |\n| train/             |          |\n|    actor_loss      | 947      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.0808   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 557696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2079.363752032828\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5008     |\n|    fps             | 59       |\n|    time_elapsed    | 9416     |\n|    total_timesteps | 558054   |\n| train/             |          |\n|    actor_loss      | 924      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0279  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 557952   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 5012     |\n|    fps             | 59       |\n|    time_elapsed    | 9420     |\n|    total_timesteps | 558310   |\n| train/             |          |\n|    actor_loss      | 936      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.172    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 558208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 5016     |\n|    fps             | 59       |\n|    time_elapsed    | 9424     |\n|    total_timesteps | 558567   |\n| train/             |          |\n|    actor_loss      | 939      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0561  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 558464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 5020     |\n|    fps             | 59       |\n|    time_elapsed    | 9429     |\n|    total_timesteps | 558826   |\n| train/             |          |\n|    actor_loss      | 943      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0614  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 558720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2075.9911018330267\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5024     |\n|    fps             | 59       |\n|    time_elapsed    | 9433     |\n|    total_timesteps | 559084   |\n| train/             |          |\n|    actor_loss      | 932      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | -0.159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 558976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 5028     |\n|    fps             | 59       |\n|    time_elapsed    | 9437     |\n|    total_timesteps | 559343   |\n| train/             |          |\n|    actor_loss      | 949      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | 0.163    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 559232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5032     |\n|    fps             | 59       |\n|    time_elapsed    | 9442     |\n|    total_timesteps | 559600   |\n| train/             |          |\n|    actor_loss      | 932      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.139    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 559488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 5036     |\n|    fps             | 59       |\n|    time_elapsed    | 9446     |\n|    total_timesteps | 559862   |\n| train/             |          |\n|    actor_loss      | 932      |\n|    critic_loss     | 12.4     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0532   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 559744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2072.1096997563427\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5040     |\n|    fps             | 59       |\n|    time_elapsed    | 9451     |\n|    total_timesteps | 560119   |\n| train/             |          |\n|    actor_loss      | 931      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.202    |\n|    ent_coef_loss   | 0.226    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 560000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 5044     |\n|    fps             | 59       |\n|    time_elapsed    | 9455     |\n|    total_timesteps | 560378   |\n| train/             |          |\n|    actor_loss      | 930      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0573  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 560256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 5048     |\n|    fps             | 59       |\n|    time_elapsed    | 9459     |\n|    total_timesteps | 560636   |\n| train/             |          |\n|    actor_loss      | 929      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.271   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 560512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5052     |\n|    fps             | 59       |\n|    time_elapsed    | 9464     |\n|    total_timesteps | 560894   |\n| train/             |          |\n|    actor_loss      | 929      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0505  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 560768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2068.307801899325\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 5056     |\n|    fps             | 59       |\n|    time_elapsed    | 9468     |\n|    total_timesteps | 561151   |\n| train/             |          |\n|    actor_loss      | 930      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0542   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 561024   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5060     |\n|    fps             | 59       |\n|    time_elapsed    | 9474     |\n|    total_timesteps | 561409   |\n| train/             |          |\n|    actor_loss      | 934      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.237   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 561344   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 5064     |\n|    fps             | 59       |\n|    time_elapsed    | 9478     |\n|    total_timesteps | 561668   |\n| train/             |          |\n|    actor_loss      | 923      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 561600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 5068     |\n|    fps             | 59       |\n|    time_elapsed    | 9482     |\n|    total_timesteps | 561927   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 561856   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2064.555777255168\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 5072     |\n|    fps             | 59       |\n|    time_elapsed    | 9487     |\n|    total_timesteps | 562185   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.069    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 562112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 5076     |\n|    fps             | 59       |\n|    time_elapsed    | 9491     |\n|    total_timesteps | 562443   |\n| train/             |          |\n|    actor_loss      | 928      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0879  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 562368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 5080     |\n|    fps             | 59       |\n|    time_elapsed    | 9496     |\n|    total_timesteps | 562701   |\n| train/             |          |\n|    actor_loss      | 929      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.00219 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 562624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 5084     |\n|    fps             | 59       |\n|    time_elapsed    | 9500     |\n|    total_timesteps | 562958   |\n| train/             |          |\n|    actor_loss      | 926      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.189    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 562880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2060.799190441466\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 5088     |\n|    fps             | 59       |\n|    time_elapsed    | 9504     |\n|    total_timesteps | 563214   |\n| train/             |          |\n|    actor_loss      | 929      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.289   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 563136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 5092     |\n|    fps             | 59       |\n|    time_elapsed    | 9509     |\n|    total_timesteps | 563472   |\n| train/             |          |\n|    actor_loss      | 938      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0772   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 563392   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 5096     |\n|    fps             | 59       |\n|    time_elapsed    | 9513     |\n|    total_timesteps | 563728   |\n| train/             |          |\n|    actor_loss      | 927      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0215   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 563648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 5100     |\n|    fps             | 59       |\n|    time_elapsed    | 9517     |\n|    total_timesteps | 563986   |\n| train/             |          |\n|    actor_loss      | 929      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.252    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 563904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2056.9453150949585\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5104     |\n|    fps             | 59       |\n|    time_elapsed    | 9522     |\n|    total_timesteps | 564242   |\n| train/             |          |\n|    actor_loss      | 920      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.00138 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 564160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 5108     |\n|    fps             | 59       |\n|    time_elapsed    | 9526     |\n|    total_timesteps | 564498   |\n| train/             |          |\n|    actor_loss      | 922      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0041   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 564416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 5112     |\n|    fps             | 59       |\n|    time_elapsed    | 9531     |\n|    total_timesteps | 564755   |\n| train/             |          |\n|    actor_loss      | 938      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.184    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 564672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2053.3901794912745\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 5116     |\n|    fps             | 59       |\n|    time_elapsed    | 9535     |\n|    total_timesteps | 565012   |\n| train/             |          |\n|    actor_loss      | 927      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.199   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 564928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 5120     |\n|    fps             | 59       |\n|    time_elapsed    | 9540     |\n|    total_timesteps | 565268   |\n| train/             |          |\n|    actor_loss      | 936      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 565184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 5124     |\n|    fps             | 59       |\n|    time_elapsed    | 9544     |\n|    total_timesteps | 565524   |\n| train/             |          |\n|    actor_loss      | 931      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0145  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 565440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 5128     |\n|    fps             | 59       |\n|    time_elapsed    | 9548     |\n|    total_timesteps | 565783   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0175   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 565696   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2049.8984252226933\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 5132     |\n|    fps             | 59       |\n|    time_elapsed    | 9553     |\n|    total_timesteps | 566040   |\n| train/             |          |\n|    actor_loss      | 930      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0669   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 565952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 5136     |\n|    fps             | 59       |\n|    time_elapsed    | 9557     |\n|    total_timesteps | 566303   |\n| train/             |          |\n|    actor_loss      | 931      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0266   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 566208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5140     |\n|    fps             | 59       |\n|    time_elapsed    | 9562     |\n|    total_timesteps | 566564   |\n| train/             |          |\n|    actor_loss      | 916      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0904  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 566464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 5144     |\n|    fps             | 59       |\n|    time_elapsed    | 9566     |\n|    total_timesteps | 566820   |\n| train/             |          |\n|    actor_loss      | 923      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0498  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 566720   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2046.7258125534568\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 5148     |\n|    fps             | 59       |\n|    time_elapsed    | 9570     |\n|    total_timesteps | 567076   |\n| train/             |          |\n|    actor_loss      | 928      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 566976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 5152     |\n|    fps             | 59       |\n|    time_elapsed    | 9575     |\n|    total_timesteps | 567334   |\n| train/             |          |\n|    actor_loss      | 922      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.114   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 567232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 5156     |\n|    fps             | 59       |\n|    time_elapsed    | 9579     |\n|    total_timesteps | 567590   |\n| train/             |          |\n|    actor_loss      | 927      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.124    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 567488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 5160     |\n|    fps             | 59       |\n|    time_elapsed    | 9584     |\n|    total_timesteps | 567846   |\n| train/             |          |\n|    actor_loss      | 921      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0886   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 567744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2042.908587168009\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 5164     |\n|    fps             | 59       |\n|    time_elapsed    | 9588     |\n|    total_timesteps | 568103   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.056   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 568000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 5168     |\n|    fps             | 59       |\n|    time_elapsed    | 9593     |\n|    total_timesteps | 568360   |\n| train/             |          |\n|    actor_loss      | 924      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 568256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 5172     |\n|    fps             | 59       |\n|    time_elapsed    | 9597     |\n|    total_timesteps | 568618   |\n| train/             |          |\n|    actor_loss      | 930      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 568512   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 5176     |\n|    fps             | 59       |\n|    time_elapsed    | 9602     |\n|    total_timesteps | 568874   |\n| train/             |          |\n|    actor_loss      | 922      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.121    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 568768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2039.3039554373115\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5180     |\n|    fps             | 59       |\n|    time_elapsed    | 9606     |\n|    total_timesteps | 569131   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0966  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 569024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 5184     |\n|    fps             | 59       |\n|    time_elapsed    | 9611     |\n|    total_timesteps | 569390   |\n| train/             |          |\n|    actor_loss      | 921      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0408  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 569280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5188     |\n|    fps             | 59       |\n|    time_elapsed    | 9615     |\n|    total_timesteps | 569647   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0896   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 569536   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 5192     |\n|    fps             | 59       |\n|    time_elapsed    | 9619     |\n|    total_timesteps | 569904   |\n| train/             |          |\n|    actor_loss      | 917      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0895   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 569792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2035.6106207494058\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5196     |\n|    fps             | 59       |\n|    time_elapsed    | 9624     |\n|    total_timesteps | 570164   |\n| train/             |          |\n|    actor_loss      | 928      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.076    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 570048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 5200     |\n|    fps             | 59       |\n|    time_elapsed    | 9628     |\n|    total_timesteps | 570424   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0244  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 570304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5204     |\n|    fps             | 59       |\n|    time_elapsed    | 9633     |\n|    total_timesteps | 570681   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0562   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 570560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 5208     |\n|    fps             | 59       |\n|    time_elapsed    | 9637     |\n|    total_timesteps | 570936   |\n| train/             |          |\n|    actor_loss      | 920      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.257   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 570816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2031.918051509482\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 5212     |\n|    fps             | 59       |\n|    time_elapsed    | 9641     |\n|    total_timesteps | 571192   |\n| train/             |          |\n|    actor_loss      | 907      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0501   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 571072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 5216     |\n|    fps             | 59       |\n|    time_elapsed    | 9646     |\n|    total_timesteps | 571448   |\n| train/             |          |\n|    actor_loss      | 906      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0696   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 571328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 5220     |\n|    fps             | 59       |\n|    time_elapsed    | 9650     |\n|    total_timesteps | 571705   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0608  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 571584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 5224     |\n|    fps             | 59       |\n|    time_elapsed    | 9655     |\n|    total_timesteps | 571961   |\n| train/             |          |\n|    actor_loss      | 925      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.101   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 571840   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2028.5006848231303\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5228     |\n|    fps             | 59       |\n|    time_elapsed    | 9659     |\n|    total_timesteps | 572218   |\n| train/             |          |\n|    actor_loss      | 916      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 572096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 5232     |\n|    fps             | 59       |\n|    time_elapsed    | 9663     |\n|    total_timesteps | 572475   |\n| train/             |          |\n|    actor_loss      | 917      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.051   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 572352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 5236     |\n|    fps             | 59       |\n|    time_elapsed    | 9668     |\n|    total_timesteps | 572732   |\n| train/             |          |\n|    actor_loss      | 920      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.226    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 572608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 5240     |\n|    fps             | 59       |\n|    time_elapsed    | 9672     |\n|    total_timesteps | 572990   |\n| train/             |          |\n|    actor_loss      | 921      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.219   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 572864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -2024.6348624340123\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 5244     |\n|    fps             | 59       |\n|    time_elapsed    | 9677     |\n|    total_timesteps | 573246   |\n| train/             |          |\n|    actor_loss      | 928      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0939  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 573120   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 5248     |\n|    fps             | 59       |\n|    time_elapsed    | 9682     |\n|    total_timesteps | 573508   |\n| train/             |          |\n|    actor_loss      | 920      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.192   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 573440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 5252     |\n|    fps             | 59       |\n|    time_elapsed    | 9686     |\n|    total_timesteps | 573764   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0903   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 573696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2021.4894416356758\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 5256     |\n|    fps             | 59       |\n|    time_elapsed    | 9691     |\n|    total_timesteps | 574020   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0732   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 573952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 5260     |\n|    fps             | 59       |\n|    time_elapsed    | 9695     |\n|    total_timesteps | 574277   |\n| train/             |          |\n|    actor_loss      | 916      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 574208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 5264     |\n|    fps             | 59       |\n|    time_elapsed    | 9700     |\n|    total_timesteps | 574533   |\n| train/             |          |\n|    actor_loss      | 907      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 574464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 5268     |\n|    fps             | 59       |\n|    time_elapsed    | 9704     |\n|    total_timesteps | 574789   |\n| train/             |          |\n|    actor_loss      | 923      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0369  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 574720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2017.6949561003676\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 5272     |\n|    fps             | 59       |\n|    time_elapsed    | 9708     |\n|    total_timesteps | 575046   |\n| train/             |          |\n|    actor_loss      | 924      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.167    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 574976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 5276     |\n|    fps             | 59       |\n|    time_elapsed    | 9713     |\n|    total_timesteps | 575302   |\n| train/             |          |\n|    actor_loss      | 921      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0114  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 575232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 5280     |\n|    fps             | 59       |\n|    time_elapsed    | 9717     |\n|    total_timesteps | 575562   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.116   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 575488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 5284     |\n|    fps             | 59       |\n|    time_elapsed    | 9722     |\n|    total_timesteps | 575819   |\n| train/             |          |\n|    actor_loss      | 906      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.00892 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 575744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2014.3029494691064\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 5288     |\n|    fps             | 59       |\n|    time_elapsed    | 9726     |\n|    total_timesteps | 576075   |\n| train/             |          |\n|    actor_loss      | 909      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.107   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 576000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 5292     |\n|    fps             | 59       |\n|    time_elapsed    | 9731     |\n|    total_timesteps | 576334   |\n| train/             |          |\n|    actor_loss      | 921      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0287  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 576256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 5296     |\n|    fps             | 59       |\n|    time_elapsed    | 9735     |\n|    total_timesteps | 576592   |\n| train/             |          |\n|    actor_loss      | 910      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0328   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 576512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 5300     |\n|    fps             | 59       |\n|    time_elapsed    | 9739     |\n|    total_timesteps | 576851   |\n| train/             |          |\n|    actor_loss      | 926      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.136    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 576768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2010.5820200411565\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 5304     |\n|    fps             | 59       |\n|    time_elapsed    | 9744     |\n|    total_timesteps | 577108   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0478  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 577024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 5308     |\n|    fps             | 59       |\n|    time_elapsed    | 9748     |\n|    total_timesteps | 577366   |\n| train/             |          |\n|    actor_loss      | 911      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0385   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 577280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 5312     |\n|    fps             | 59       |\n|    time_elapsed    | 9752     |\n|    total_timesteps | 577623   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0382  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 577536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 5316     |\n|    fps             | 59       |\n|    time_elapsed    | 9757     |\n|    total_timesteps | 577879   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0242  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 577792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2007.2663124432358\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 5320     |\n|    fps             | 59       |\n|    time_elapsed    | 9761     |\n|    total_timesteps | 578138   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.184   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 578048   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 5324     |\n|    fps             | 59       |\n|    time_elapsed    | 9766     |\n|    total_timesteps | 578393   |\n| train/             |          |\n|    actor_loss      | 918      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 578304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 5328     |\n|    fps             | 59       |\n|    time_elapsed    | 9770     |\n|    total_timesteps | 578649   |\n| train/             |          |\n|    actor_loss      | 912      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0646  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 578560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 5332     |\n|    fps             | 59       |\n|    time_elapsed    | 9775     |\n|    total_timesteps | 578905   |\n| train/             |          |\n|    actor_loss      | 915      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0886   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 578816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -2003.863192733681\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 5336     |\n|    fps             | 59       |\n|    time_elapsed    | 9779     |\n|    total_timesteps | 579165   |\n| train/             |          |\n|    actor_loss      | 912      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.102    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 579072   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5340     |\n|    fps             | 59       |\n|    time_elapsed    | 9783     |\n|    total_timesteps | 579427   |\n| train/             |          |\n|    actor_loss      | 902      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0494  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 579328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 5344     |\n|    fps             | 59       |\n|    time_elapsed    | 9788     |\n|    total_timesteps | 579687   |\n| train/             |          |\n|    actor_loss      | 903      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.242   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 579584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 5348     |\n|    fps             | 59       |\n|    time_elapsed    | 9792     |\n|    total_timesteps | 579946   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0884  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 579840   |\n---------------------------------\nNew best mean reward across all envs: -2000.8750128119218\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 5352     |\n|    fps             | 59       |\n|    time_elapsed    | 9797     |\n|    total_timesteps | 580205   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.076    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 580096   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 5356     |\n|    fps             | 59       |\n|    time_elapsed    | 9801     |\n|    total_timesteps | 580464   |\n| train/             |          |\n|    actor_loss      | 918      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0389  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 580352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 5360     |\n|    fps             | 59       |\n|    time_elapsed    | 9805     |\n|    total_timesteps | 580722   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.203    |\n|    ent_coef_loss   | -0.0256  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 580608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 5364     |\n|    fps             | 59       |\n|    time_elapsed    | 9809     |\n|    total_timesteps | 580982   |\n| train/             |          |\n|    actor_loss      | 902      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0288   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 580864   |\n---------------------------------\nNew best mean reward across all envs: -1997.5219617703588\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 5368     |\n|    fps             | 59       |\n|    time_elapsed    | 9814     |\n|    total_timesteps | 581240   |\n| train/             |          |\n|    actor_loss      | 897      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0725   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 581120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 5372     |\n|    fps             | 59       |\n|    time_elapsed    | 9818     |\n|    total_timesteps | 581497   |\n| train/             |          |\n|    actor_loss      | 915      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0258   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 581376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 5376     |\n|    fps             | 59       |\n|    time_elapsed    | 9822     |\n|    total_timesteps | 581755   |\n| train/             |          |\n|    actor_loss      | 915      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.00219  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 581632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1994.4247412569105\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 5380     |\n|    fps             | 59       |\n|    time_elapsed    | 9827     |\n|    total_timesteps | 582015   |\n| train/             |          |\n|    actor_loss      | 903      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 581888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5384     |\n|    fps             | 59       |\n|    time_elapsed    | 9832     |\n|    total_timesteps | 582274   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.201   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 582208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 5388     |\n|    fps             | 59       |\n|    time_elapsed    | 9836     |\n|    total_timesteps | 582531   |\n| train/             |          |\n|    actor_loss      | 909      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 582464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 5392     |\n|    fps             | 59       |\n|    time_elapsed    | 9841     |\n|    total_timesteps | 582789   |\n| train/             |          |\n|    actor_loss      | 911      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0985   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 582720   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1991.2498707636582\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 5396     |\n|    fps             | 59       |\n|    time_elapsed    | 9845     |\n|    total_timesteps | 583048   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0775  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 582976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5400     |\n|    fps             | 59       |\n|    time_elapsed    | 9849     |\n|    total_timesteps | 583307   |\n| train/             |          |\n|    actor_loss      | 902      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0734   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 583232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5404     |\n|    fps             | 59       |\n|    time_elapsed    | 9854     |\n|    total_timesteps | 583565   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.137    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 583488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -898     |\n| time/              |          |\n|    episodes        | 5408     |\n|    fps             | 59       |\n|    time_elapsed    | 9858     |\n|    total_timesteps | 583826   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.205    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 583744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1988.2551823811464\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -908     |\n| time/              |          |\n|    episodes        | 5412     |\n|    fps             | 59       |\n|    time_elapsed    | 9862     |\n|    total_timesteps | 584086   |\n| train/             |          |\n|    actor_loss      | 909      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 584000   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -907     |\n| time/              |          |\n|    episodes        | 5416     |\n|    fps             | 59       |\n|    time_elapsed    | 9867     |\n|    total_timesteps | 584344   |\n| train/             |          |\n|    actor_loss      | 906      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0986   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 584256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -901     |\n| time/              |          |\n|    episodes        | 5420     |\n|    fps             | 59       |\n|    time_elapsed    | 9871     |\n|    total_timesteps | 584602   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 584512   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5424     |\n|    fps             | 59       |\n|    time_elapsed    | 9875     |\n|    total_timesteps | 584860   |\n| train/             |          |\n|    actor_loss      | 895      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0131  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 584768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1984.8610780494753\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 5428     |\n|    fps             | 59       |\n|    time_elapsed    | 9880     |\n|    total_timesteps | 585118   |\n| train/             |          |\n|    actor_loss      | 900      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0951   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 585024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 5432     |\n|    fps             | 59       |\n|    time_elapsed    | 9884     |\n|    total_timesteps | 585376   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.029   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 585280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 5436     |\n|    fps             | 59       |\n|    time_elapsed    | 9889     |\n|    total_timesteps | 585634   |\n| train/             |          |\n|    actor_loss      | 916      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0954   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 585536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5440     |\n|    fps             | 59       |\n|    time_elapsed    | 9893     |\n|    total_timesteps | 585893   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 585792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1981.918493465078\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 5444     |\n|    fps             | 59       |\n|    time_elapsed    | 9897     |\n|    total_timesteps | 586153   |\n| train/             |          |\n|    actor_loss      | 906      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0798   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 586048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 5448     |\n|    fps             | 59       |\n|    time_elapsed    | 9902     |\n|    total_timesteps | 586412   |\n| train/             |          |\n|    actor_loss      | 897      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.111   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 586304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5452     |\n|    fps             | 59       |\n|    time_elapsed    | 9906     |\n|    total_timesteps | 586671   |\n| train/             |          |\n|    actor_loss      | 900      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0816  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 586560   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5456     |\n|    fps             | 59       |\n|    time_elapsed    | 9910     |\n|    total_timesteps | 586930   |\n| train/             |          |\n|    actor_loss      | 909      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0796   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 586816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1978.8272152378427\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -896     |\n| time/              |          |\n|    episodes        | 5460     |\n|    fps             | 59       |\n|    time_elapsed    | 9915     |\n|    total_timesteps | 587189   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0174   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 587072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -899     |\n| time/              |          |\n|    episodes        | 5464     |\n|    fps             | 59       |\n|    time_elapsed    | 9919     |\n|    total_timesteps | 587446   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0252   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 587328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 5468     |\n|    fps             | 59       |\n|    time_elapsed    | 9924     |\n|    total_timesteps | 587707   |\n| train/             |          |\n|    actor_loss      | 920      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.034   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 587584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 5472     |\n|    fps             | 59       |\n|    time_elapsed    | 9929     |\n|    total_timesteps | 587969   |\n| train/             |          |\n|    actor_loss      | 901      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0308  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 587904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1976.327726325542\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 5476     |\n|    fps             | 59       |\n|    time_elapsed    | 9933     |\n|    total_timesteps | 588229   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0607  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 588160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -922     |\n| time/              |          |\n|    episodes        | 5480     |\n|    fps             | 59       |\n|    time_elapsed    | 9937     |\n|    total_timesteps | 588488   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0844  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 588416   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 5484     |\n|    fps             | 59       |\n|    time_elapsed    | 9942     |\n|    total_timesteps | 588748   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0616   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 588672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1973.31780469492\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -913     |\n| time/              |          |\n|    episodes        | 5488     |\n|    fps             | 59       |\n|    time_elapsed    | 9947     |\n|    total_timesteps | 589010   |\n| train/             |          |\n|    actor_loss      | 914      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.29     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 588928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 5492     |\n|    fps             | 59       |\n|    time_elapsed    | 9951     |\n|    total_timesteps | 589268   |\n| train/             |          |\n|    actor_loss      | 899      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.0364  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 589184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -923     |\n| time/              |          |\n|    episodes        | 5496     |\n|    fps             | 59       |\n|    time_elapsed    | 9955     |\n|    total_timesteps | 589526   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0218  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 589440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 5500     |\n|    fps             | 59       |\n|    time_elapsed    | 9960     |\n|    total_timesteps | 589785   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.111   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 589696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1970.3923674264029\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 5504     |\n|    fps             | 59       |\n|    time_elapsed    | 9964     |\n|    total_timesteps | 590045   |\n| train/             |          |\n|    actor_loss      | 902      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | 0.0323   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 589952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 5508     |\n|    fps             | 59       |\n|    time_elapsed    | 9968     |\n|    total_timesteps | 590308   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.156    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 590208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 5512     |\n|    fps             | 59       |\n|    time_elapsed    | 9973     |\n|    total_timesteps | 590564   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 590464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -923     |\n| time/              |          |\n|    episodes        | 5516     |\n|    fps             | 59       |\n|    time_elapsed    | 9977     |\n|    total_timesteps | 590822   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0859   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 590720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1967.3531007133222\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -920     |\n| time/              |          |\n|    episodes        | 5520     |\n|    fps             | 59       |\n|    time_elapsed    | 9981     |\n|    total_timesteps | 591081   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0777  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 590976   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -925     |\n| time/              |          |\n|    episodes        | 5524     |\n|    fps             | 59       |\n|    time_elapsed    | 9986     |\n|    total_timesteps | 591341   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0744   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 591232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 5528     |\n|    fps             | 59       |\n|    time_elapsed    | 9990     |\n|    total_timesteps | 591601   |\n| train/             |          |\n|    actor_loss      | 888      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.00514  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 591488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 5532     |\n|    fps             | 59       |\n|    time_elapsed    | 9994     |\n|    total_timesteps | 591862   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.00451 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 591744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1964.329918986356\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 5536     |\n|    fps             | 59       |\n|    time_elapsed    | 9999     |\n|    total_timesteps | 592122   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0727  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 592000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 5540     |\n|    fps             | 59       |\n|    time_elapsed    | 10003    |\n|    total_timesteps | 592383   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0168  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 592256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 5544     |\n|    fps             | 59       |\n|    time_elapsed    | 10008    |\n|    total_timesteps | 592643   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0452  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 592576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -940     |\n| time/              |          |\n|    episodes        | 5548     |\n|    fps             | 59       |\n|    time_elapsed    | 10013    |\n|    total_timesteps | 592901   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.223    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 592832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1961.6876801765181\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 5552     |\n|    fps             | 59       |\n|    time_elapsed    | 10017    |\n|    total_timesteps | 593165   |\n| train/             |          |\n|    actor_loss      | 901      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.00176 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 593088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 5556     |\n|    fps             | 59       |\n|    time_elapsed    | 10021    |\n|    total_timesteps | 593428   |\n| train/             |          |\n|    actor_loss      | 907      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0819  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 593344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -955     |\n| time/              |          |\n|    episodes        | 5560     |\n|    fps             | 59       |\n|    time_elapsed    | 10026    |\n|    total_timesteps | 593688   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0205  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 593600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 5564     |\n|    fps             | 59       |\n|    time_elapsed    | 10030    |\n|    total_timesteps | 593949   |\n| train/             |          |\n|    actor_loss      | 913      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0203  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 593856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1959.0758625599385\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 5568     |\n|    fps             | 59       |\n|    time_elapsed    | 10035    |\n|    total_timesteps | 594208   |\n| train/             |          |\n|    actor_loss      | 912      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.0381   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 594112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -936     |\n| time/              |          |\n|    episodes        | 5572     |\n|    fps             | 59       |\n|    time_elapsed    | 10039    |\n|    total_timesteps | 594470   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.203    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 594368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -928     |\n| time/              |          |\n|    episodes        | 5576     |\n|    fps             | 59       |\n|    time_elapsed    | 10044    |\n|    total_timesteps | 594731   |\n| train/             |          |\n|    actor_loss      | 901      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.148    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 594624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 5580     |\n|    fps             | 59       |\n|    time_elapsed    | 10048    |\n|    total_timesteps | 594990   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.00822  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 594880   |\n---------------------------------\nNew best mean reward across all envs: -1956.017927051124\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 5584     |\n|    fps             | 59       |\n|    time_elapsed    | 10052    |\n|    total_timesteps | 595249   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.119    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 595136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 5588     |\n|    fps             | 59       |\n|    time_elapsed    | 10057    |\n|    total_timesteps | 595508   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0642   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 595392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -923     |\n| time/              |          |\n|    episodes        | 5592     |\n|    fps             | 59       |\n|    time_elapsed    | 10061    |\n|    total_timesteps | 595768   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.051    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 595648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1952.9570675453995\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 5596     |\n|    fps             | 59       |\n|    time_elapsed    | 10066    |\n|    total_timesteps | 596024   |\n| train/             |          |\n|    actor_loss      | 904      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0657   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 595904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -912     |\n| time/              |          |\n|    episodes        | 5600     |\n|    fps             | 59       |\n|    time_elapsed    | 10070    |\n|    total_timesteps | 596281   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.201    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 596160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -905     |\n| time/              |          |\n|    episodes        | 5604     |\n|    fps             | 59       |\n|    time_elapsed    | 10074    |\n|    total_timesteps | 596537   |\n| train/             |          |\n|    actor_loss      | 890      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 596416   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 5608     |\n|    fps             | 59       |\n|    time_elapsed    | 10079    |\n|    total_timesteps | 596796   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0344  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 596672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1949.633305974099\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 5612     |\n|    fps             | 59       |\n|    time_elapsed    | 10083    |\n|    total_timesteps | 597054   |\n| train/             |          |\n|    actor_loss      | 895      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.0914  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 596928   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -885     |\n| time/              |          |\n|    episodes        | 5616     |\n|    fps             | 59       |\n|    time_elapsed    | 10088    |\n|    total_timesteps | 597312   |\n| train/             |          |\n|    actor_loss      | 905      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0413   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 597184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 5620     |\n|    fps             | 59       |\n|    time_elapsed    | 10093    |\n|    total_timesteps | 597572   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0386  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 597504   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 5624     |\n|    fps             | 59       |\n|    time_elapsed    | 10097    |\n|    total_timesteps | 597830   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.095   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 597760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1946.6357397489746\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 5628     |\n|    fps             | 59       |\n|    time_elapsed    | 10102    |\n|    total_timesteps | 598088   |\n| train/             |          |\n|    actor_loss      | 888      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 598016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -885     |\n| time/              |          |\n|    episodes        | 5632     |\n|    fps             | 59       |\n|    time_elapsed    | 10106    |\n|    total_timesteps | 598348   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0519  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 598272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 5636     |\n|    fps             | 59       |\n|    time_elapsed    | 10111    |\n|    total_timesteps | 598608   |\n| train/             |          |\n|    actor_loss      | 900      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 598528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 5640     |\n|    fps             | 59       |\n|    time_elapsed    | 10115    |\n|    total_timesteps | 598864   |\n| train/             |          |\n|    actor_loss      | 901      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0588   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 598784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1943.6689434279647\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 5644     |\n|    fps             | 59       |\n|    time_elapsed    | 10119    |\n|    total_timesteps | 599120   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.229    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 599040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 5648     |\n|    fps             | 59       |\n|    time_elapsed    | 10124    |\n|    total_timesteps | 599376   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 599296   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5652     |\n|    fps             | 59       |\n|    time_elapsed    | 10128    |\n|    total_timesteps | 599633   |\n| train/             |          |\n|    actor_loss      | 885      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0463  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 599552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 5656     |\n|    fps             | 59       |\n|    time_elapsed    | 10133    |\n|    total_timesteps | 599891   |\n| train/             |          |\n|    actor_loss      | 907      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 599808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1940.8392937277033\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 5660     |\n|    fps             | 59       |\n|    time_elapsed    | 10137    |\n|    total_timesteps | 600148   |\n| train/             |          |\n|    actor_loss      | 883      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0149  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 600064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5664     |\n|    fps             | 59       |\n|    time_elapsed    | 10142    |\n|    total_timesteps | 600404   |\n| train/             |          |\n|    actor_loss      | 907      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0542   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 600320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 5668     |\n|    fps             | 59       |\n|    time_elapsed    | 10146    |\n|    total_timesteps | 600662   |\n| train/             |          |\n|    actor_loss      | 891      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 600576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 5672     |\n|    fps             | 59       |\n|    time_elapsed    | 10150    |\n|    total_timesteps | 600918   |\n| train/             |          |\n|    actor_loss      | 900      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.00298  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 600832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1938.132790995527\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 5676     |\n|    fps             | 59       |\n|    time_elapsed    | 10155    |\n|    total_timesteps | 601176   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 12.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0644   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 601088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -876     |\n| time/              |          |\n|    episodes        | 5680     |\n|    fps             | 59       |\n|    time_elapsed    | 10159    |\n|    total_timesteps | 601433   |\n| train/             |          |\n|    actor_loss      | 901      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 601344   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 5684     |\n|    fps             | 59       |\n|    time_elapsed    | 10163    |\n|    total_timesteps | 601689   |\n| train/             |          |\n|    actor_loss      | 887      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0994   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 601600   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 5688     |\n|    fps             | 59       |\n|    time_elapsed    | 10168    |\n|    total_timesteps | 601945   |\n| train/             |          |\n|    actor_loss      | 899      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0671  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 601856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1935.5166047091252\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 5692     |\n|    fps             | 59       |\n|    time_elapsed    | 10172    |\n|    total_timesteps | 602202   |\n| train/             |          |\n|    actor_loss      | 895      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.204    |\n|    ent_coef_loss   | -0.0701  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 602112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 5696     |\n|    fps             | 59       |\n|    time_elapsed    | 10177    |\n|    total_timesteps | 602462   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.205    |\n|    ent_coef_loss   | 0.193    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 602368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 5700     |\n|    fps             | 59       |\n|    time_elapsed    | 10181    |\n|    total_timesteps | 602718   |\n| train/             |          |\n|    actor_loss      | 891      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0375   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 602624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 5704     |\n|    fps             | 59       |\n|    time_elapsed    | 10186    |\n|    total_timesteps | 602975   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.157    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 602880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1932.3211421299136\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 5708     |\n|    fps             | 59       |\n|    time_elapsed    | 10190    |\n|    total_timesteps | 603231   |\n| train/             |          |\n|    actor_loss      | 887      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.114   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 603136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 5712     |\n|    fps             | 59       |\n|    time_elapsed    | 10194    |\n|    total_timesteps | 603489   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.239    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 603392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -904     |\n| time/              |          |\n|    episodes        | 5716     |\n|    fps             | 59       |\n|    time_elapsed    | 10199    |\n|    total_timesteps | 603747   |\n| train/             |          |\n|    actor_loss      | 885      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0396  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 603648   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1929.7123446121611\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 5720     |\n|    fps             | 59       |\n|    time_elapsed    | 10203    |\n|    total_timesteps | 604005   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 603904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 5724     |\n|    fps             | 59       |\n|    time_elapsed    | 10208    |\n|    total_timesteps | 604263   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | -0.00674 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 604160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 5728     |\n|    fps             | 59       |\n|    time_elapsed    | 10212    |\n|    total_timesteps | 604519   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.00937 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 604416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 5732     |\n|    fps             | 59       |\n|    time_elapsed    | 10216    |\n|    total_timesteps | 604776   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.192    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 604672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1926.734532472778\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -899     |\n| time/              |          |\n|    episodes        | 5736     |\n|    fps             | 59       |\n|    time_elapsed    | 10221    |\n|    total_timesteps | 605032   |\n| train/             |          |\n|    actor_loss      | 886      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0375  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 604928   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -907     |\n| time/              |          |\n|    episodes        | 5740     |\n|    fps             | 59       |\n|    time_elapsed    | 10225    |\n|    total_timesteps | 605290   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00594  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 605184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 5744     |\n|    fps             | 59       |\n|    time_elapsed    | 10230    |\n|    total_timesteps | 605546   |\n| train/             |          |\n|    actor_loss      | 891      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.098   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 605440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 5748     |\n|    fps             | 59       |\n|    time_elapsed    | 10234    |\n|    total_timesteps | 605805   |\n| train/             |          |\n|    actor_loss      | 908      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.269    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 605696   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1923.7844340964743\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 5752     |\n|    fps             | 59       |\n|    time_elapsed    | 10238    |\n|    total_timesteps | 606064   |\n| train/             |          |\n|    actor_loss      | 898      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0251  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 605952   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 5756     |\n|    fps             | 59       |\n|    time_elapsed    | 10243    |\n|    total_timesteps | 606321   |\n| train/             |          |\n|    actor_loss      | 888      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 606208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 5760     |\n|    fps             | 59       |\n|    time_elapsed    | 10247    |\n|    total_timesteps | 606578   |\n| train/             |          |\n|    actor_loss      | 884      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.18    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 606464   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 5764     |\n|    fps             | 59       |\n|    time_elapsed    | 10252    |\n|    total_timesteps | 606834   |\n| train/             |          |\n|    actor_loss      | 884      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0482   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 606720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1921.2747772626572\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 5768     |\n|    fps             | 59       |\n|    time_elapsed    | 10256    |\n|    total_timesteps | 607090   |\n| train/             |          |\n|    actor_loss      | 891      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.183    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 606976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5772     |\n|    fps             | 59       |\n|    time_elapsed    | 10260    |\n|    total_timesteps | 607348   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.166    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 607232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -901     |\n| time/              |          |\n|    episodes        | 5776     |\n|    fps             | 59       |\n|    time_elapsed    | 10265    |\n|    total_timesteps | 607607   |\n| train/             |          |\n|    actor_loss      | 879      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0741   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 607488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 5780     |\n|    fps             | 59       |\n|    time_elapsed    | 10269    |\n|    total_timesteps | 607863   |\n| train/             |          |\n|    actor_loss      | 892      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0984  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 607744   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1918.5467060938856\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 5784     |\n|    fps             | 59       |\n|    time_elapsed    | 10274    |\n|    total_timesteps | 608122   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.16    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 608000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 5788     |\n|    fps             | 59       |\n|    time_elapsed    | 10278    |\n|    total_timesteps | 608379   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.077   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 608256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 5792     |\n|    fps             | 59       |\n|    time_elapsed    | 10282    |\n|    total_timesteps | 608636   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.128    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 608512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -896     |\n| time/              |          |\n|    episodes        | 5796     |\n|    fps             | 59       |\n|    time_elapsed    | 10287    |\n|    total_timesteps | 608892   |\n| train/             |          |\n|    actor_loss      | 885      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0439   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 608768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1915.9457880304378\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -899     |\n| time/              |          |\n|    episodes        | 5800     |\n|    fps             | 59       |\n|    time_elapsed    | 10291    |\n|    total_timesteps | 609149   |\n| train/             |          |\n|    actor_loss      | 895      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0421   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 609024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 5804     |\n|    fps             | 59       |\n|    time_elapsed    | 10295    |\n|    total_timesteps | 609406   |\n| train/             |          |\n|    actor_loss      | 883      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0329   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 609280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 5808     |\n|    fps             | 59       |\n|    time_elapsed    | 10300    |\n|    total_timesteps | 609663   |\n| train/             |          |\n|    actor_loss      | 892      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.09    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 609536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 5812     |\n|    fps             | 59       |\n|    time_elapsed    | 10305    |\n|    total_timesteps | 609923   |\n| train/             |          |\n|    actor_loss      | 894      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0179  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 609856   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1912.7745699674717\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 5816     |\n|    fps             | 59       |\n|    time_elapsed    | 10309    |\n|    total_timesteps | 610180   |\n| train/             |          |\n|    actor_loss      | 896      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0812  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 610112   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 5820     |\n|    fps             | 59       |\n|    time_elapsed    | 10314    |\n|    total_timesteps | 610438   |\n| train/             |          |\n|    actor_loss      | 890      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 610368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 5824     |\n|    fps             | 59       |\n|    time_elapsed    | 10318    |\n|    total_timesteps | 610697   |\n| train/             |          |\n|    actor_loss      | 883      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0674   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 610624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 5828     |\n|    fps             | 59       |\n|    time_elapsed    | 10323    |\n|    total_timesteps | 610958   |\n| train/             |          |\n|    actor_loss      | 878      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.214   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 610880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1910.0396198607154\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 5832     |\n|    fps             | 59       |\n|    time_elapsed    | 10327    |\n|    total_timesteps | 611216   |\n| train/             |          |\n|    actor_loss      | 888      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.174    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 611136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 5836     |\n|    fps             | 59       |\n|    time_elapsed    | 10332    |\n|    total_timesteps | 611473   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0869  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 611392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 5840     |\n|    fps             | 59       |\n|    time_elapsed    | 10336    |\n|    total_timesteps | 611731   |\n| train/             |          |\n|    actor_loss      | 889      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0869   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 611648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 5844     |\n|    fps             | 59       |\n|    time_elapsed    | 10340    |\n|    total_timesteps | 611988   |\n| train/             |          |\n|    actor_loss      | 883      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 611904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1906.9009284669198\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 5848     |\n|    fps             | 59       |\n|    time_elapsed    | 10345    |\n|    total_timesteps | 612244   |\n| train/             |          |\n|    actor_loss      | 884      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0841  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 612160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 5852     |\n|    fps             | 59       |\n|    time_elapsed    | 10349    |\n|    total_timesteps | 612503   |\n| train/             |          |\n|    actor_loss      | 886      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0257  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 612416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 5856     |\n|    fps             | 59       |\n|    time_elapsed    | 10354    |\n|    total_timesteps | 612759   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0444   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 612672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1904.291273473779\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 5860     |\n|    fps             | 59       |\n|    time_elapsed    | 10358    |\n|    total_timesteps | 613016   |\n| train/             |          |\n|    actor_loss      | 887      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0271  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 612928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 5864     |\n|    fps             | 59       |\n|    time_elapsed    | 10363    |\n|    total_timesteps | 613274   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 613184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 5868     |\n|    fps             | 59       |\n|    time_elapsed    | 10367    |\n|    total_timesteps | 613536   |\n| train/             |          |\n|    actor_loss      | 891      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0446   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 613440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 5872     |\n|    fps             | 59       |\n|    time_elapsed    | 10372    |\n|    total_timesteps | 613792   |\n| train/             |          |\n|    actor_loss      | 889      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.163   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 613696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1901.672558844515\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 5876     |\n|    fps             | 59       |\n|    time_elapsed    | 10376    |\n|    total_timesteps | 614051   |\n| train/             |          |\n|    actor_loss      | 886      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.00368  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 613952   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5880     |\n|    fps             | 59       |\n|    time_elapsed    | 10380    |\n|    total_timesteps | 614309   |\n| train/             |          |\n|    actor_loss      | 881      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.145    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 614208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 5884     |\n|    fps             | 59       |\n|    time_elapsed    | 10385    |\n|    total_timesteps | 614570   |\n| train/             |          |\n|    actor_loss      | 892      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0321   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 614464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 5888     |\n|    fps             | 59       |\n|    time_elapsed    | 10389    |\n|    total_timesteps | 614827   |\n| train/             |          |\n|    actor_loss      | 871      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 614720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1898.9609851506832\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 5892     |\n|    fps             | 59       |\n|    time_elapsed    | 10393    |\n|    total_timesteps | 615084   |\n| train/             |          |\n|    actor_loss      | 889      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.066   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 614976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5896     |\n|    fps             | 59       |\n|    time_elapsed    | 10398    |\n|    total_timesteps | 615343   |\n| train/             |          |\n|    actor_loss      | 876      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.031   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 615232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 5900     |\n|    fps             | 59       |\n|    time_elapsed    | 10402    |\n|    total_timesteps | 615603   |\n| train/             |          |\n|    actor_loss      | 885      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.00718  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 615488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 5904     |\n|    fps             | 59       |\n|    time_elapsed    | 10406    |\n|    total_timesteps | 615858   |\n| train/             |          |\n|    actor_loss      | 895      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0716  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 615744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1896.155325862059\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 5908     |\n|    fps             | 59       |\n|    time_elapsed    | 10411    |\n|    total_timesteps | 616116   |\n| train/             |          |\n|    actor_loss      | 873      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.0872   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 616000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 5912     |\n|    fps             | 59       |\n|    time_elapsed    | 10415    |\n|    total_timesteps | 616380   |\n| train/             |          |\n|    actor_loss      | 893      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.206    |\n|    ent_coef_loss   | 0.085    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 616256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 5916     |\n|    fps             | 59       |\n|    time_elapsed    | 10419    |\n|    total_timesteps | 616640   |\n| train/             |          |\n|    actor_loss      | 877      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.127    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 616512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 5920     |\n|    fps             | 59       |\n|    time_elapsed    | 10424    |\n|    total_timesteps | 616899   |\n| train/             |          |\n|    actor_loss      | 881      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 616832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1893.4247945373427\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 5924     |\n|    fps             | 59       |\n|    time_elapsed    | 10429    |\n|    total_timesteps | 617158   |\n| train/             |          |\n|    actor_loss      | 879      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 617088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 5928     |\n|    fps             | 59       |\n|    time_elapsed    | 10433    |\n|    total_timesteps | 617418   |\n| train/             |          |\n|    actor_loss      | 872      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0822  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 617344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 5932     |\n|    fps             | 59       |\n|    time_elapsed    | 10437    |\n|    total_timesteps | 617674   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0406  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 617600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 5936     |\n|    fps             | 59       |\n|    time_elapsed    | 10442    |\n|    total_timesteps | 617936   |\n| train/             |          |\n|    actor_loss      | 871      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.183   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 617856   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1890.8435811942052\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 5940     |\n|    fps             | 59       |\n|    time_elapsed    | 10446    |\n|    total_timesteps | 618199   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.038    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 618112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 5944     |\n|    fps             | 59       |\n|    time_elapsed    | 10451    |\n|    total_timesteps | 618457   |\n| train/             |          |\n|    actor_loss      | 881      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0421   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 618368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 5948     |\n|    fps             | 59       |\n|    time_elapsed    | 10455    |\n|    total_timesteps | 618716   |\n| train/             |          |\n|    actor_loss      | 878      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0283   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 618624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 5952     |\n|    fps             | 59       |\n|    time_elapsed    | 10459    |\n|    total_timesteps | 618978   |\n| train/             |          |\n|    actor_loss      | 873      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.082   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 618880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1888.177323920437\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 5956     |\n|    fps             | 59       |\n|    time_elapsed    | 10463    |\n|    total_timesteps | 619240   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.13     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 619136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 5960     |\n|    fps             | 59       |\n|    time_elapsed    | 10468    |\n|    total_timesteps | 619501   |\n| train/             |          |\n|    actor_loss      | 875      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.18    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 619392   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 5964     |\n|    fps             | 59       |\n|    time_elapsed    | 10472    |\n|    total_timesteps | 619760   |\n| train/             |          |\n|    actor_loss      | 877      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0158  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 619648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1885.4124808516756\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 5968     |\n|    fps             | 59       |\n|    time_elapsed    | 10476    |\n|    total_timesteps | 620017   |\n| train/             |          |\n|    actor_loss      | 885      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.05     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 619904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 5972     |\n|    fps             | 59       |\n|    time_elapsed    | 10481    |\n|    total_timesteps | 620280   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0255  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 620160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 5976     |\n|    fps             | 59       |\n|    time_elapsed    | 10485    |\n|    total_timesteps | 620538   |\n| train/             |          |\n|    actor_loss      | 873      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0854  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 620416   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 5980     |\n|    fps             | 59       |\n|    time_elapsed    | 10489    |\n|    total_timesteps | 620795   |\n| train/             |          |\n|    actor_loss      | 868      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0513  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 620672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1882.4541602024842\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 5984     |\n|    fps             | 59       |\n|    time_elapsed    | 10494    |\n|    total_timesteps | 621054   |\n| train/             |          |\n|    actor_loss      | 877      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.147    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 620928   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 5988     |\n|    fps             | 59       |\n|    time_elapsed    | 10498    |\n|    total_timesteps | 621311   |\n| train/             |          |\n|    actor_loss      | 872      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 621184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 5992     |\n|    fps             | 59       |\n|    time_elapsed    | 10503    |\n|    total_timesteps | 621570   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 621504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 5996     |\n|    fps             | 59       |\n|    time_elapsed    | 10508    |\n|    total_timesteps | 621829   |\n| train/             |          |\n|    actor_loss      | 881      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0293   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 621760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1879.4665481380503\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -801     |\n| time/              |          |\n|    episodes        | 6000     |\n|    fps             | 59       |\n|    time_elapsed    | 10512    |\n|    total_timesteps | 622089   |\n| train/             |          |\n|    actor_loss      | 871      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 622016   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 6004     |\n|    fps             | 59       |\n|    time_elapsed    | 10517    |\n|    total_timesteps | 622348   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0241  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 622272   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 6008     |\n|    fps             | 59       |\n|    time_elapsed    | 10521    |\n|    total_timesteps | 622610   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0758   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 622528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 6012     |\n|    fps             | 59       |\n|    time_elapsed    | 10525    |\n|    total_timesteps | 622871   |\n| train/             |          |\n|    actor_loss      | 868      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0799  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 622784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1876.6880846048566\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 6016     |\n|    fps             | 59       |\n|    time_elapsed    | 10530    |\n|    total_timesteps | 623130   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.19    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 623040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 6020     |\n|    fps             | 59       |\n|    time_elapsed    | 10534    |\n|    total_timesteps | 623391   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00126 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 623296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 6024     |\n|    fps             | 59       |\n|    time_elapsed    | 10539    |\n|    total_timesteps | 623659   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0667  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 623552   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 6028     |\n|    fps             | 59       |\n|    time_elapsed    | 10543    |\n|    total_timesteps | 623919   |\n| train/             |          |\n|    actor_loss      | 886      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0834   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 623808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1874.3145702099719\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 6032     |\n|    fps             | 59       |\n|    time_elapsed    | 10548    |\n|    total_timesteps | 624183   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0539  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 624064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 6036     |\n|    fps             | 59       |\n|    time_elapsed    | 10552    |\n|    total_timesteps | 624443   |\n| train/             |          |\n|    actor_loss      | 878      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0525   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 624320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 6040     |\n|    fps             | 59       |\n|    time_elapsed    | 10557    |\n|    total_timesteps | 624708   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0922   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 624640   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 6044     |\n|    fps             | 59       |\n|    time_elapsed    | 10561    |\n|    total_timesteps | 624968   |\n| train/             |          |\n|    actor_loss      | 872      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.254   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 624896   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1872.1155079659234\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 6048     |\n|    fps             | 59       |\n|    time_elapsed    | 10566    |\n|    total_timesteps | 625237   |\n| train/             |          |\n|    actor_loss      | 863      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.112   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 625152   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 6052     |\n|    fps             | 59       |\n|    time_elapsed    | 10570    |\n|    total_timesteps | 625503   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0597   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 625408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 6056     |\n|    fps             | 59       |\n|    time_elapsed    | 10575    |\n|    total_timesteps | 625766   |\n| train/             |          |\n|    actor_loss      | 868      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.132   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 625664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1869.9863105612615\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 6060     |\n|    fps             | 59       |\n|    time_elapsed    | 10579    |\n|    total_timesteps | 626029   |\n| train/             |          |\n|    actor_loss      | 870      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0508   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 625920   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 6064     |\n|    fps             | 59       |\n|    time_elapsed    | 10583    |\n|    total_timesteps | 626293   |\n| train/             |          |\n|    actor_loss      | 878      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00932 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 626176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 6068     |\n|    fps             | 59       |\n|    time_elapsed    | 10588    |\n|    total_timesteps | 626558   |\n| train/             |          |\n|    actor_loss      | 872      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0807   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 626432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 6072     |\n|    fps             | 59       |\n|    time_elapsed    | 10592    |\n|    total_timesteps | 626816   |\n| train/             |          |\n|    actor_loss      | 869      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.16     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 626688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1867.4954576305665\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 6076     |\n|    fps             | 59       |\n|    time_elapsed    | 10597    |\n|    total_timesteps | 627075   |\n| train/             |          |\n|    actor_loss      | 865      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0168   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 627008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 6080     |\n|    fps             | 59       |\n|    time_elapsed    | 10602    |\n|    total_timesteps | 627334   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.242   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 627264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 65.4      |\n|    ep_rew_mean     | -893      |\n| time/              |           |\n|    episodes        | 6084      |\n|    fps             | 59        |\n|    time_elapsed    | 10606     |\n|    total_timesteps | 627591    |\n| train/             |           |\n|    actor_loss      | 865       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | -0.000877 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 627520    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 6088     |\n|    fps             | 59       |\n|    time_elapsed    | 10610    |\n|    total_timesteps | 627853   |\n| train/             |          |\n|    actor_loss      | 871      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0461   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 627776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1864.937744923122\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -905     |\n| time/              |          |\n|    episodes        | 6092     |\n|    fps             | 59       |\n|    time_elapsed    | 10615    |\n|    total_timesteps | 628115   |\n| train/             |          |\n|    actor_loss      | 870      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0562  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 628032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 6096     |\n|    fps             | 59       |\n|    time_elapsed    | 10619    |\n|    total_timesteps | 628372   |\n| train/             |          |\n|    actor_loss      | 871      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0868   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 628288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 6100     |\n|    fps             | 59       |\n|    time_elapsed    | 10624    |\n|    total_timesteps | 628627   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0317   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 628544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 6104     |\n|    fps             | 59       |\n|    time_elapsed    | 10628    |\n|    total_timesteps | 628884   |\n| train/             |          |\n|    actor_loss      | 882      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0738  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 628800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1862.3667237875256\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -909     |\n| time/              |          |\n|    episodes        | 6108     |\n|    fps             | 59       |\n|    time_elapsed    | 10633    |\n|    total_timesteps | 629143   |\n| train/             |          |\n|    actor_loss      | 876      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.000749 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 629056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 6112     |\n|    fps             | 59       |\n|    time_elapsed    | 10637    |\n|    total_timesteps | 629406   |\n| train/             |          |\n|    actor_loss      | 880      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0843   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 629312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -907     |\n| time/              |          |\n|    episodes        | 6116     |\n|    fps             | 59       |\n|    time_elapsed    | 10642    |\n|    total_timesteps | 629663   |\n| train/             |          |\n|    actor_loss      | 859      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0808   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 629568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -901     |\n| time/              |          |\n|    episodes        | 6120     |\n|    fps             | 59       |\n|    time_elapsed    | 10646    |\n|    total_timesteps | 629919   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0667   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 629824   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1859.5664404714205\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 6124     |\n|    fps             | 59       |\n|    time_elapsed    | 10650    |\n|    total_timesteps | 630176   |\n| train/             |          |\n|    actor_loss      | 886      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0551  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 630080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 6128     |\n|    fps             | 59       |\n|    time_elapsed    | 10655    |\n|    total_timesteps | 630431   |\n| train/             |          |\n|    actor_loss      | 869      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 630336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -899     |\n| time/              |          |\n|    episodes        | 6132     |\n|    fps             | 59       |\n|    time_elapsed    | 10659    |\n|    total_timesteps | 630686   |\n| train/             |          |\n|    actor_loss      | 864      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0348   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 630592   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 6136     |\n|    fps             | 59       |\n|    time_elapsed    | 10663    |\n|    total_timesteps | 630943   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0381   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 630848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1857.1198120215513\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 6140     |\n|    fps             | 59       |\n|    time_elapsed    | 10668    |\n|    total_timesteps | 631201   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0622  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 631104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 6144     |\n|    fps             | 59       |\n|    time_elapsed    | 10672    |\n|    total_timesteps | 631459   |\n| train/             |          |\n|    actor_loss      | 863      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.00725 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 631360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 6148     |\n|    fps             | 59       |\n|    time_elapsed    | 10677    |\n|    total_timesteps | 631715   |\n| train/             |          |\n|    actor_loss      | 887      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0871   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 631616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 6152     |\n|    fps             | 59       |\n|    time_elapsed    | 10681    |\n|    total_timesteps | 631980   |\n| train/             |          |\n|    actor_loss      | 866      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.107   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 631872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1854.3906081638324\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 6156     |\n|    fps             | 59       |\n|    time_elapsed    | 10686    |\n|    total_timesteps | 632236   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0602   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 632128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 6160     |\n|    fps             | 59       |\n|    time_elapsed    | 10690    |\n|    total_timesteps | 632495   |\n| train/             |          |\n|    actor_loss      | 865      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0671   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 632384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 6164     |\n|    fps             | 59       |\n|    time_elapsed    | 10694    |\n|    total_timesteps | 632756   |\n| train/             |          |\n|    actor_loss      | 864      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0309  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 632640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1852.077651263393\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 6168     |\n|    fps             | 59       |\n|    time_elapsed    | 10699    |\n|    total_timesteps | 633013   |\n| train/             |          |\n|    actor_loss      | 861      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0633   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 632896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 6172     |\n|    fps             | 59       |\n|    time_elapsed    | 10703    |\n|    total_timesteps | 633268   |\n| train/             |          |\n|    actor_loss      | 874      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0563   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 633152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 6176     |\n|    fps             | 59       |\n|    time_elapsed    | 10707    |\n|    total_timesteps | 633524   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0157  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 633408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 6180     |\n|    fps             | 59       |\n|    time_elapsed    | 10712    |\n|    total_timesteps | 633785   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.073   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 633664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1849.606850358685\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 6184     |\n|    fps             | 59       |\n|    time_elapsed    | 10716    |\n|    total_timesteps | 634047   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0839  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 633920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6188     |\n|    fps             | 59       |\n|    time_elapsed    | 10721    |\n|    total_timesteps | 634307   |\n| train/             |          |\n|    actor_loss      | 865      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0893  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 634240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6192     |\n|    fps             | 59       |\n|    time_elapsed    | 10726    |\n|    total_timesteps | 634568   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0619   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 634496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6196     |\n|    fps             | 59       |\n|    time_elapsed    | 10730    |\n|    total_timesteps | 634827   |\n| train/             |          |\n|    actor_loss      | 854      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.01     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 634752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1847.2948762048607\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.6      |\n|    ep_rew_mean     | -855      |\n| time/              |           |\n|    episodes        | 6200      |\n|    fps             | 59        |\n|    time_elapsed    | 10734     |\n|    total_timesteps | 635085    |\n| train/             |           |\n|    actor_loss      | 857       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.218     |\n|    ent_coef_loss   | -0.000155 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 635008    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 6204     |\n|    fps             | 59       |\n|    time_elapsed    | 10739    |\n|    total_timesteps | 635345   |\n| train/             |          |\n|    actor_loss      | 854      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00604  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 635264   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 6208     |\n|    fps             | 59       |\n|    time_elapsed    | 10743    |\n|    total_timesteps | 635601   |\n| train/             |          |\n|    actor_loss      | 862      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0632   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 635520   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 6212     |\n|    fps             | 59       |\n|    time_elapsed    | 10747    |\n|    total_timesteps | 635856   |\n| train/             |          |\n|    actor_loss      | 858      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 635776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1844.9026896469645\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 6216     |\n|    fps             | 59       |\n|    time_elapsed    | 10752    |\n|    total_timesteps | 636113   |\n| train/             |          |\n|    actor_loss      | 854      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.124    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 636032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 6220     |\n|    fps             | 59       |\n|    time_elapsed    | 10756    |\n|    total_timesteps | 636369   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0533  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 636288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 6224     |\n|    fps             | 59       |\n|    time_elapsed    | 10761    |\n|    total_timesteps | 636625   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 636544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 6228     |\n|    fps             | 59       |\n|    time_elapsed    | 10765    |\n|    total_timesteps | 636888   |\n| train/             |          |\n|    actor_loss      | 861      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0948  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 636800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1842.5832142120062\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 6232     |\n|    fps             | 59       |\n|    time_elapsed    | 10769    |\n|    total_timesteps | 637144   |\n| train/             |          |\n|    actor_loss      | 864      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0212   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 637056   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -885     |\n| time/              |          |\n|    episodes        | 6236     |\n|    fps             | 59       |\n|    time_elapsed    | 10774    |\n|    total_timesteps | 637402   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 637312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 6240     |\n|    fps             | 59       |\n|    time_elapsed    | 10778    |\n|    total_timesteps | 637660   |\n| train/             |          |\n|    actor_loss      | 854      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0632  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 637568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 6244     |\n|    fps             | 59       |\n|    time_elapsed    | 10782    |\n|    total_timesteps | 637918   |\n| train/             |          |\n|    actor_loss      | 858      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0682   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 637824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1840.0720523357093\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 6248     |\n|    fps             | 59       |\n|    time_elapsed    | 10787    |\n|    total_timesteps | 638174   |\n| train/             |          |\n|    actor_loss      | 857      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0609  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 638080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 6252     |\n|    fps             | 59       |\n|    time_elapsed    | 10791    |\n|    total_timesteps | 638430   |\n| train/             |          |\n|    actor_loss      | 846      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.063   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 638336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 6256     |\n|    fps             | 59       |\n|    time_elapsed    | 10796    |\n|    total_timesteps | 638688   |\n| train/             |          |\n|    actor_loss      | 858      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0406  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 638592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 6260     |\n|    fps             | 59       |\n|    time_elapsed    | 10800    |\n|    total_timesteps | 638946   |\n| train/             |          |\n|    actor_loss      | 857      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.134   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 638848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1837.571203855711\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 6264     |\n|    fps             | 59       |\n|    time_elapsed    | 10804    |\n|    total_timesteps | 639204   |\n| train/             |          |\n|    actor_loss      | 864      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.157    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 639104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 6268     |\n|    fps             | 59       |\n|    time_elapsed    | 10809    |\n|    total_timesteps | 639460   |\n| train/             |          |\n|    actor_loss      | 876      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.169    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 639360   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 6272     |\n|    fps             | 59       |\n|    time_elapsed    | 10813    |\n|    total_timesteps | 639716   |\n| train/             |          |\n|    actor_loss      | 862      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0496  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 639616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 6276     |\n|    fps             | 59       |\n|    time_elapsed    | 10818    |\n|    total_timesteps | 639972   |\n| train/             |          |\n|    actor_loss      | 863      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0555   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 639872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1835.2494035413538\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 6280     |\n|    fps             | 59       |\n|    time_elapsed    | 10822    |\n|    total_timesteps | 640228   |\n| train/             |          |\n|    actor_loss      | 859      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0227  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 640128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 6284     |\n|    fps             | 59       |\n|    time_elapsed    | 10826    |\n|    total_timesteps | 640484   |\n| train/             |          |\n|    actor_loss      | 861      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0719  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 640384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 6288     |\n|    fps             | 59       |\n|    time_elapsed    | 10831    |\n|    total_timesteps | 640752   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.117   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 640640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1832.8662511270634\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 6292     |\n|    fps             | 59       |\n|    time_elapsed    | 10835    |\n|    total_timesteps | 641013   |\n| train/             |          |\n|    actor_loss      | 867      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.028   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 640896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 6296     |\n|    fps             | 59       |\n|    time_elapsed    | 10840    |\n|    total_timesteps | 641271   |\n| train/             |          |\n|    actor_loss      | 861      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0472   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 641152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 6300     |\n|    fps             | 59       |\n|    time_elapsed    | 10844    |\n|    total_timesteps | 641527   |\n| train/             |          |\n|    actor_loss      | 859      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0311   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 641408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 6304     |\n|    fps             | 59       |\n|    time_elapsed    | 10848    |\n|    total_timesteps | 641783   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0378  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 641664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1830.3853282077575\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 6308     |\n|    fps             | 59       |\n|    time_elapsed    | 10853    |\n|    total_timesteps | 642042   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00567  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 641920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 6312     |\n|    fps             | 59       |\n|    time_elapsed    | 10858    |\n|    total_timesteps | 642299   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0429   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 642176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 6316     |\n|    fps             | 59       |\n|    time_elapsed    | 10862    |\n|    total_timesteps | 642560   |\n| train/             |          |\n|    actor_loss      | 858      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0566  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 642432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 6320     |\n|    fps             | 59       |\n|    time_elapsed    | 10867    |\n|    total_timesteps | 642817   |\n| train/             |          |\n|    actor_loss      | 869      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 642752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1827.8710739172518\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 6324     |\n|    fps             | 59       |\n|    time_elapsed    | 10872    |\n|    total_timesteps | 643076   |\n| train/             |          |\n|    actor_loss      | 862      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0281  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 643008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 6328     |\n|    fps             | 59       |\n|    time_elapsed    | 10876    |\n|    total_timesteps | 643336   |\n| train/             |          |\n|    actor_loss      | 844      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0123  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 643264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 6332     |\n|    fps             | 59       |\n|    time_elapsed    | 10880    |\n|    total_timesteps | 643592   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0944   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 643520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 6336     |\n|    fps             | 59       |\n|    time_elapsed    | 10885    |\n|    total_timesteps | 643854   |\n| train/             |          |\n|    actor_loss      | 865      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0416  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 643776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1825.2074297749166\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 6340     |\n|    fps             | 59       |\n|    time_elapsed    | 10889    |\n|    total_timesteps | 644110   |\n| train/             |          |\n|    actor_loss      | 863      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0466  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 644032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 6344     |\n|    fps             | 59       |\n|    time_elapsed    | 10894    |\n|    total_timesteps | 644368   |\n| train/             |          |\n|    actor_loss      | 853      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.00316 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 644288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 6348     |\n|    fps             | 59       |\n|    time_elapsed    | 10898    |\n|    total_timesteps | 644625   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.00491 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 644544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 6352     |\n|    fps             | 59       |\n|    time_elapsed    | 10903    |\n|    total_timesteps | 644883   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0393   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 644800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1822.7334029070844\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 6356     |\n|    fps             | 59       |\n|    time_elapsed    | 10907    |\n|    total_timesteps | 645142   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0282  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 645056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 6360     |\n|    fps             | 59       |\n|    time_elapsed    | 10912    |\n|    total_timesteps | 645400   |\n| train/             |          |\n|    actor_loss      | 849      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0788  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 645312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 6364     |\n|    fps             | 59       |\n|    time_elapsed    | 10916    |\n|    total_timesteps | 645656   |\n| train/             |          |\n|    actor_loss      | 847      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.126    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 645568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 6368     |\n|    fps             | 59       |\n|    time_elapsed    | 10921    |\n|    total_timesteps | 645918   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.232   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 645824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1820.2517932337078\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 6372     |\n|    fps             | 59       |\n|    time_elapsed    | 10925    |\n|    total_timesteps | 646182   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0896  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 646080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 6376     |\n|    fps             | 59       |\n|    time_elapsed    | 10929    |\n|    total_timesteps | 646440   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0931  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 646336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 6380     |\n|    fps             | 59       |\n|    time_elapsed    | 10934    |\n|    total_timesteps | 646696   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.144   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 646592   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 6384     |\n|    fps             | 59       |\n|    time_elapsed    | 10938    |\n|    total_timesteps | 646955   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0499  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 646848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1817.9969108729629\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 6388     |\n|    fps             | 59       |\n|    time_elapsed    | 10943    |\n|    total_timesteps | 647211   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0318   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 647104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 6392     |\n|    fps             | 59       |\n|    time_elapsed    | 10947    |\n|    total_timesteps | 647468   |\n| train/             |          |\n|    actor_loss      | 852      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0226   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 647360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 6396     |\n|    fps             | 59       |\n|    time_elapsed    | 10952    |\n|    total_timesteps | 647733   |\n| train/             |          |\n|    actor_loss      | 853      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0355  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 647616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 6400     |\n|    fps             | 59       |\n|    time_elapsed    | 10956    |\n|    total_timesteps | 647991   |\n| train/             |          |\n|    actor_loss      | 854      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 647872   |\n---------------------------------\nNew best mean reward across all envs: -1815.6343869298103\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 6404     |\n|    fps             | 59       |\n|    time_elapsed    | 10961    |\n|    total_timesteps | 648257   |\n| train/             |          |\n|    actor_loss      | 857      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.127    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 648192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 6408     |\n|    fps             | 59       |\n|    time_elapsed    | 10966    |\n|    total_timesteps | 648516   |\n| train/             |          |\n|    actor_loss      | 851      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0953   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 648448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 6412     |\n|    fps             | 59       |\n|    time_elapsed    | 10970    |\n|    total_timesteps | 648773   |\n| train/             |          |\n|    actor_loss      | 844      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0485   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 648704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1813.2406720271622\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 6416     |\n|    fps             | 59       |\n|    time_elapsed    | 10974    |\n|    total_timesteps | 649030   |\n| train/             |          |\n|    actor_loss      | 853      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0775   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 648960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 6420     |\n|    fps             | 59       |\n|    time_elapsed    | 10979    |\n|    total_timesteps | 649287   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0146  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 649216   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 6424     |\n|    fps             | 59       |\n|    time_elapsed    | 10983    |\n|    total_timesteps | 649544   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0373   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 649472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 6428     |\n|    fps             | 59       |\n|    time_elapsed    | 10987    |\n|    total_timesteps | 649804   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.205   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 649728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1810.6832450246422\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 6432     |\n|    fps             | 59       |\n|    time_elapsed    | 10991    |\n|    total_timesteps | 650063   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 649984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 6436     |\n|    fps             | 59       |\n|    time_elapsed    | 10996    |\n|    total_timesteps | 650319   |\n| train/             |          |\n|    actor_loss      | 846      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | 0.0686   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 650240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 6440     |\n|    fps             | 59       |\n|    time_elapsed    | 11000    |\n|    total_timesteps | 650577   |\n| train/             |          |\n|    actor_loss      | 858      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 650496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 6444     |\n|    fps             | 59       |\n|    time_elapsed    | 11004    |\n|    total_timesteps | 650838   |\n| train/             |          |\n|    actor_loss      | 852      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0384  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 650752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1808.5208080359412\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 6448     |\n|    fps             | 59       |\n|    time_elapsed    | 11009    |\n|    total_timesteps | 651094   |\n| train/             |          |\n|    actor_loss      | 860      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0136   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 651008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 6452     |\n|    fps             | 59       |\n|    time_elapsed    | 11013    |\n|    total_timesteps | 651351   |\n| train/             |          |\n|    actor_loss      | 846      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0792  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 651264   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 6456     |\n|    fps             | 59       |\n|    time_elapsed    | 11017    |\n|    total_timesteps | 651612   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 651520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 6460     |\n|    fps             | 59       |\n|    time_elapsed    | 11021    |\n|    total_timesteps | 651875   |\n| train/             |          |\n|    actor_loss      | 852      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0616   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 651776   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1806.3137747242497\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6464     |\n|    fps             | 59       |\n|    time_elapsed    | 11026    |\n|    total_timesteps | 652135   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0983  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 652032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 6468     |\n|    fps             | 59       |\n|    time_elapsed    | 11030    |\n|    total_timesteps | 652398   |\n| train/             |          |\n|    actor_loss      | 856      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0158  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 652288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 6472     |\n|    fps             | 59       |\n|    time_elapsed    | 11034    |\n|    total_timesteps | 652656   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0721   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 652544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 6476     |\n|    fps             | 59       |\n|    time_elapsed    | 11039    |\n|    total_timesteps | 652911   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0349   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 652800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1803.8409447181157\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 6480     |\n|    fps             | 59       |\n|    time_elapsed    | 11043    |\n|    total_timesteps | 653168   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0945   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 653056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 6484     |\n|    fps             | 59       |\n|    time_elapsed    | 11047    |\n|    total_timesteps | 653432   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0594  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 653312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 6488     |\n|    fps             | 59       |\n|    time_elapsed    | 11052    |\n|    total_timesteps | 653691   |\n| train/             |          |\n|    actor_loss      | 846      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0197  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 653568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 6492     |\n|    fps             | 59       |\n|    time_elapsed    | 11056    |\n|    total_timesteps | 653948   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0225   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 653824   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1801.694900745535\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 6496     |\n|    fps             | 59       |\n|    time_elapsed    | 11060    |\n|    total_timesteps | 654204   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0333  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 654080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 6500     |\n|    fps             | 59       |\n|    time_elapsed    | 11065    |\n|    total_timesteps | 654461   |\n| train/             |          |\n|    actor_loss      | 844      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0443  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 654336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 6504     |\n|    fps             | 59       |\n|    time_elapsed    | 11070    |\n|    total_timesteps | 654721   |\n| train/             |          |\n|    actor_loss      | 851      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.103    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 654656   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 6508     |\n|    fps             | 59       |\n|    time_elapsed    | 11074    |\n|    total_timesteps | 654981   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0195   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 654912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1799.093887592167\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 6512     |\n|    fps             | 59       |\n|    time_elapsed    | 11078    |\n|    total_timesteps | 655240   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0246  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 655168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 6516     |\n|    fps             | 59       |\n|    time_elapsed    | 11082    |\n|    total_timesteps | 655500   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0565   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 655424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 6520     |\n|    fps             | 59       |\n|    time_elapsed    | 11087    |\n|    total_timesteps | 655760   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.00321 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 655680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1796.8337723640886\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 6524     |\n|    fps             | 59       |\n|    time_elapsed    | 11091    |\n|    total_timesteps | 656020   |\n| train/             |          |\n|    actor_loss      | 851      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0223   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 655936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 6528     |\n|    fps             | 59       |\n|    time_elapsed    | 11095    |\n|    total_timesteps | 656283   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.02     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 656192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 6532     |\n|    fps             | 59       |\n|    time_elapsed    | 11100    |\n|    total_timesteps | 656547   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.059    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 656448   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 6536     |\n|    fps             | 59       |\n|    time_elapsed    | 11104    |\n|    total_timesteps | 656808   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.116   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 656704   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1795.1367881924493\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 6540     |\n|    fps             | 59       |\n|    time_elapsed    | 11108    |\n|    total_timesteps | 657076   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0565   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 656960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 6544     |\n|    fps             | 59       |\n|    time_elapsed    | 11113    |\n|    total_timesteps | 657339   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0797  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 657216   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 6548     |\n|    fps             | 59       |\n|    time_elapsed    | 11118    |\n|    total_timesteps | 657603   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 657536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 6552     |\n|    fps             | 59       |\n|    time_elapsed    | 11122    |\n|    total_timesteps | 657871   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0623   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 657792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1793.8392445925203\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 6556     |\n|    fps             | 59       |\n|    time_elapsed    | 11127    |\n|    total_timesteps | 658138   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.153    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 658048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 6560     |\n|    fps             | 59       |\n|    time_elapsed    | 11131    |\n|    total_timesteps | 658399   |\n| train/             |          |\n|    actor_loss      | 849      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 658304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -909     |\n| time/              |          |\n|    episodes        | 6564     |\n|    fps             | 59       |\n|    time_elapsed    | 11135    |\n|    total_timesteps | 658659   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.00295  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 658560   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -908     |\n| time/              |          |\n|    episodes        | 6568     |\n|    fps             | 59       |\n|    time_elapsed    | 11140    |\n|    total_timesteps | 658922   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 658816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1791.5831896434142\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 6572     |\n|    fps             | 59       |\n|    time_elapsed    | 11144    |\n|    total_timesteps | 659197   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 659072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 6576     |\n|    fps             | 59       |\n|    time_elapsed    | 11149    |\n|    total_timesteps | 659459   |\n| train/             |          |\n|    actor_loss      | 850      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0343  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 659392   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 6580     |\n|    fps             | 59       |\n|    time_elapsed    | 11153    |\n|    total_timesteps | 659725   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.113    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 659648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 6584     |\n|    fps             | 59       |\n|    time_elapsed    | 11158    |\n|    total_timesteps | 659992   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.115    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 659904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1790.0449816461824\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -969     |\n| time/              |          |\n|    episodes        | 6588     |\n|    fps             | 59       |\n|    time_elapsed    | 11162    |\n|    total_timesteps | 660260   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0839  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 660160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -985     |\n| time/              |          |\n|    episodes        | 6592     |\n|    fps             | 59       |\n|    time_elapsed    | 11167    |\n|    total_timesteps | 660525   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.13     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 660416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 6596     |\n|    fps             | 59       |\n|    time_elapsed    | 11171    |\n|    total_timesteps | 660785   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.119    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 660672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1788.417824849297\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 6600     |\n|    fps             | 59       |\n|    time_elapsed    | 11175    |\n|    total_timesteps | 661052   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0253  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 660928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66        |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 6604      |\n|    fps             | 59        |\n|    time_elapsed    | 11180     |\n|    total_timesteps | 661321    |\n| train/             |           |\n|    actor_loss      | 839       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0562    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 661248    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 6608      |\n|    fps             | 59        |\n|    time_elapsed    | 11185     |\n|    total_timesteps | 661588    |\n| train/             |           |\n|    actor_loss      | 833       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.00015   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 661504    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.2      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 6612      |\n|    fps             | 59        |\n|    time_elapsed    | 11189     |\n|    total_timesteps | 661857    |\n| train/             |           |\n|    actor_loss      | 848       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0898   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 661760    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1786.9238910484455\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.3      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 6616      |\n|    fps             | 59        |\n|    time_elapsed    | 11194     |\n|    total_timesteps | 662130    |\n| train/             |           |\n|    actor_loss      | 845       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0929   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 662016    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.3      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 6620      |\n|    fps             | 59        |\n|    time_elapsed    | 11198     |\n|    total_timesteps | 662394    |\n| train/             |           |\n|    actor_loss      | 831       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0769    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 662272    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.4      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 6624      |\n|    fps             | 59        |\n|    time_elapsed    | 11203     |\n|    total_timesteps | 662664    |\n| train/             |           |\n|    actor_loss      | 837       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.093    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 662592    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.6      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6628      |\n|    fps             | 59        |\n|    time_elapsed    | 11208     |\n|    total_timesteps | 662939    |\n| train/             |           |\n|    actor_loss      | 848       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.162     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 662848    |\n----------------------------------\nNew best mean reward across all envs: -1785.6659946586074\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.7      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6632      |\n|    fps             | 59        |\n|    time_elapsed    | 11212     |\n|    total_timesteps | 663212    |\n| train/             |           |\n|    actor_loss      | 838       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.101    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 663104    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.8      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6636      |\n|    fps             | 59        |\n|    time_elapsed    | 11216     |\n|    total_timesteps | 663484    |\n| train/             |           |\n|    actor_loss      | 829       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0111   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 663360    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.8      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6640      |\n|    fps             | 59        |\n|    time_elapsed    | 11222     |\n|    total_timesteps | 663758    |\n| train/             |           |\n|    actor_loss      | 841       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0897   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 663680    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1784.3680107159714\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.8      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6644      |\n|    fps             | 59        |\n|    time_elapsed    | 11226     |\n|    total_timesteps | 664020    |\n| train/             |           |\n|    actor_loss      | 846       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.129    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 663936    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.9      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6648      |\n|    fps             | 59        |\n|    time_elapsed    | 11231     |\n|    total_timesteps | 664295    |\n| train/             |           |\n|    actor_loss      | 837       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.056     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 664192    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.9      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6652      |\n|    fps             | 59        |\n|    time_elapsed    | 11235     |\n|    total_timesteps | 664562    |\n| train/             |           |\n|    actor_loss      | 835       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.0422    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 664448    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 66.9      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6656      |\n|    fps             | 59        |\n|    time_elapsed    | 11239     |\n|    total_timesteps | 664828    |\n| train/             |           |\n|    actor_loss      | 837       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0531    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 664704    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1783.133098790177\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.1      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 6660      |\n|    fps             | 59        |\n|    time_elapsed    | 11244     |\n|    total_timesteps | 665105    |\n| train/             |           |\n|    actor_loss      | 843       |\n|    critic_loss     | 11.9      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.0783    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 665024    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 6664      |\n|    fps             | 59        |\n|    time_elapsed    | 11249     |\n|    total_timesteps | 665374    |\n| train/             |           |\n|    actor_loss      | 842       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | -0.0317   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 665280    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.15e+03 |\n| time/              |           |\n|    episodes        | 6668      |\n|    fps             | 59        |\n|    time_elapsed    | 11253     |\n|    total_timesteps | 665644    |\n| train/             |           |\n|    actor_loss      | 827       |\n|    critic_loss     | 11.2      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.0178    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 665536    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.14e+03 |\n| time/              |           |\n|    episodes        | 6672      |\n|    fps             | 59        |\n|    time_elapsed    | 11258     |\n|    total_timesteps | 665916    |\n| train/             |           |\n|    actor_loss      | 833       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.137    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 665792    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1781.4840584333947\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 6676      |\n|    fps             | 59        |\n|    time_elapsed    | 11263     |\n|    total_timesteps | 666182    |\n| train/             |           |\n|    actor_loss      | 831       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | -0.0803   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 666112    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6680      |\n|    fps             | 59        |\n|    time_elapsed    | 11267     |\n|    total_timesteps | 666447    |\n| train/             |           |\n|    actor_loss      | 838       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.153     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 666368    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6684      |\n|    fps             | 59        |\n|    time_elapsed    | 11272     |\n|    total_timesteps | 666713    |\n| train/             |           |\n|    actor_loss      | 841       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | -0.105    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 666624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6688      |\n|    fps             | 59        |\n|    time_elapsed    | 11276     |\n|    total_timesteps | 666976    |\n| train/             |           |\n|    actor_loss      | 839       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | -0.0233   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 666880    |\n----------------------------------\nargv[0]=\nNew best mean reward across all envs: -1779.5753144388461\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.3      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6692      |\n|    fps             | 59        |\n|    time_elapsed    | 11280     |\n|    total_timesteps | 667251    |\n| train/             |           |\n|    actor_loss      | 841       |\n|    critic_loss     | 12.1      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.0174    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 667136    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.12e+03 |\n| time/              |           |\n|    episodes        | 6696      |\n|    fps             | 59        |\n|    time_elapsed    | 11286     |\n|    total_timesteps | 667531    |\n| train/             |           |\n|    actor_loss      | 842       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | 0.0841    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 667456    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 6700      |\n|    fps             | 59        |\n|    time_elapsed    | 11290     |\n|    total_timesteps | 667801    |\n| train/             |           |\n|    actor_loss      | 838       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | -0.0724   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 667712    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1778.2023027874975\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.14e+03 |\n| time/              |           |\n|    episodes        | 6704      |\n|    fps             | 59        |\n|    time_elapsed    | 11295     |\n|    total_timesteps | 668071    |\n| train/             |           |\n|    actor_loss      | 832       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.206     |\n|    ent_coef_loss   | 0.1       |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 667968    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.13e+03 |\n| time/              |           |\n|    episodes        | 6708      |\n|    fps             | 59        |\n|    time_elapsed    | 11299     |\n|    total_timesteps | 668339    |\n| train/             |           |\n|    actor_loss      | 843       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.209     |\n|    ent_coef_loss   | 0.179     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 668224    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.5      |\n|    ep_rew_mean     | -1.11e+03 |\n| time/              |           |\n|    episodes        | 6712      |\n|    fps             | 59        |\n|    time_elapsed    | 11303     |\n|    total_timesteps | 668607    |\n| train/             |           |\n|    actor_loss      | 839       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.114     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 668480    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.4      |\n|    ep_rew_mean     | -1.09e+03 |\n| time/              |           |\n|    episodes        | 6716      |\n|    fps             | 59        |\n|    time_elapsed    | 11308     |\n|    total_timesteps | 668867    |\n| train/             |           |\n|    actor_loss      | 850       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.21      |\n|    ent_coef_loss   | 0.1       |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 668800    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1776.2819959766823\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.3     |\n|    ep_rew_mean     | -1.1e+03 |\n| time/              |          |\n|    episodes        | 6720     |\n|    fps             | 59       |\n|    time_elapsed    | 11313    |\n|    total_timesteps | 669128   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0956   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 669056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.3      |\n|    ep_rew_mean     | -1.08e+03 |\n| time/              |           |\n|    episodes        | 6724      |\n|    fps             | 59        |\n|    time_elapsed    | 11317     |\n|    total_timesteps | 669397    |\n| train/             |           |\n|    actor_loss      | 831       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | 0.188     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 669312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.2      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 6728      |\n|    fps             | 59        |\n|    time_elapsed    | 11322     |\n|    total_timesteps | 669661    |\n| train/             |           |\n|    actor_loss      | 834       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.245    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 669568    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.1      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 6732      |\n|    fps             | 59        |\n|    time_elapsed    | 11326     |\n|    total_timesteps | 669919    |\n| train/             |           |\n|    actor_loss      | 842       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.208     |\n|    ent_coef_loss   | 0.0432    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 669824    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1774.1317445866125\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67        |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 6736      |\n|    fps             | 59        |\n|    time_elapsed    | 11330     |\n|    total_timesteps | 670183    |\n| train/             |           |\n|    actor_loss      | 836       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.207     |\n|    ent_coef_loss   | -0.0469   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 670080    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.8     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 6740     |\n|    fps             | 59       |\n|    time_elapsed    | 11335    |\n|    total_timesteps | 670442   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0648  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 670336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.8     |\n|    ep_rew_mean     | -990     |\n| time/              |          |\n|    episodes        | 6744     |\n|    fps             | 59       |\n|    time_elapsed    | 11339    |\n|    total_timesteps | 670698   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0154   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 670592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.6     |\n|    ep_rew_mean     | -982     |\n| time/              |          |\n|    episodes        | 6748     |\n|    fps             | 59       |\n|    time_elapsed    | 11343    |\n|    total_timesteps | 670958   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0506   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 670848   |\n---------------------------------\nNew best mean reward across all envs: -1772.055029176066\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.5     |\n|    ep_rew_mean     | -954     |\n| time/              |          |\n|    episodes        | 6752     |\n|    fps             | 59       |\n|    time_elapsed    | 11348    |\n|    total_timesteps | 671217   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0434   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 671104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.6     |\n|    ep_rew_mean     | -950     |\n| time/              |          |\n|    episodes        | 6756     |\n|    fps             | 59       |\n|    time_elapsed    | 11352    |\n|    total_timesteps | 671487   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0182  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 671360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.4     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 6760     |\n|    fps             | 59       |\n|    time_elapsed    | 11356    |\n|    total_timesteps | 671743   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0498   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 671616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1769.996799506311\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 6764     |\n|    fps             | 59       |\n|    time_elapsed    | 11361    |\n|    total_timesteps | 672004   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0308  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 671936   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 6768     |\n|    fps             | 59       |\n|    time_elapsed    | 11366    |\n|    total_timesteps | 672261   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0016   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 672192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -908     |\n| time/              |          |\n|    episodes        | 6772     |\n|    fps             | 59       |\n|    time_elapsed    | 11370    |\n|    total_timesteps | 672521   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 672448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -906     |\n| time/              |          |\n|    episodes        | 6776     |\n|    fps             | 59       |\n|    time_elapsed    | 11374    |\n|    total_timesteps | 672779   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0113  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 672704   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1767.767245516671\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 6780     |\n|    fps             | 59       |\n|    time_elapsed    | 11379    |\n|    total_timesteps | 673038   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0423   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 672960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 6784     |\n|    fps             | 59       |\n|    time_elapsed    | 11383    |\n|    total_timesteps | 673307   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 673216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 6788     |\n|    fps             | 59       |\n|    time_elapsed    | 11388    |\n|    total_timesteps | 673573   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.145   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 673472   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 6792     |\n|    fps             | 59       |\n|    time_elapsed    | 11392    |\n|    total_timesteps | 673828   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.128    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 673728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1765.7488570649243\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 6796     |\n|    fps             | 59       |\n|    time_elapsed    | 11396    |\n|    total_timesteps | 674084   |\n| train/             |          |\n|    actor_loss      | 852      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0819   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 673984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 6800     |\n|    fps             | 59       |\n|    time_elapsed    | 11401    |\n|    total_timesteps | 674340   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0769  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 674240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 6804     |\n|    fps             | 59       |\n|    time_elapsed    | 11405    |\n|    total_timesteps | 674597   |\n| train/             |          |\n|    actor_loss      | 848      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.251    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 674496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 6808     |\n|    fps             | 59       |\n|    time_elapsed    | 11409    |\n|    total_timesteps | 674855   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.177    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 674752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1763.7745780318596\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 6812     |\n|    fps             | 59       |\n|    time_elapsed    | 11414    |\n|    total_timesteps | 675117   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 675008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 6816     |\n|    fps             | 59       |\n|    time_elapsed    | 11418    |\n|    total_timesteps | 675380   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 675264   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 6820     |\n|    fps             | 59       |\n|    time_elapsed    | 11423    |\n|    total_timesteps | 675636   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0449   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 675520   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 6824     |\n|    fps             | 59       |\n|    time_elapsed    | 11427    |\n|    total_timesteps | 675891   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0831  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 675776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1761.9165198810838\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 6828     |\n|    fps             | 59       |\n|    time_elapsed    | 11431    |\n|    total_timesteps | 676148   |\n| train/             |          |\n|    actor_loss      | 846      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0372   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 676032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 6832     |\n|    fps             | 59       |\n|    time_elapsed    | 11436    |\n|    total_timesteps | 676414   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0282   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 676288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 6836     |\n|    fps             | 59       |\n|    time_elapsed    | 11440    |\n|    total_timesteps | 676670   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0712  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 676544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 6840     |\n|    fps             | 59       |\n|    time_elapsed    | 11444    |\n|    total_timesteps | 676928   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0937   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 676800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1759.6762510120254\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 6844     |\n|    fps             | 59       |\n|    time_elapsed    | 11449    |\n|    total_timesteps | 677184   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.113    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 677056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 6848     |\n|    fps             | 59       |\n|    time_elapsed    | 11453    |\n|    total_timesteps | 677440   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.172    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 677312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 6852     |\n|    fps             | 59       |\n|    time_elapsed    | 11457    |\n|    total_timesteps | 677696   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 677568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 6856     |\n|    fps             | 59       |\n|    time_elapsed    | 11462    |\n|    total_timesteps | 677955   |\n| train/             |          |\n|    actor_loss      | 839      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.334   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 677888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1757.5922549937736\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 6860     |\n|    fps             | 59       |\n|    time_elapsed    | 11467    |\n|    total_timesteps | 678216   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0366  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 678144   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 6864     |\n|    fps             | 59       |\n|    time_elapsed    | 11471    |\n|    total_timesteps | 678472   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.191    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 678400   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 6868     |\n|    fps             | 59       |\n|    time_elapsed    | 11475    |\n|    total_timesteps | 678731   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0528  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 678656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 6872     |\n|    fps             | 59       |\n|    time_elapsed    | 11480    |\n|    total_timesteps | 678991   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0905   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 678912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1755.3526438673107\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 6876     |\n|    fps             | 59       |\n|    time_elapsed    | 11484    |\n|    total_timesteps | 679247   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.019    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 679168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 6880     |\n|    fps             | 59       |\n|    time_elapsed    | 11488    |\n|    total_timesteps | 679503   |\n| train/             |          |\n|    actor_loss      | 842      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.019    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 679424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 6884     |\n|    fps             | 59       |\n|    time_elapsed    | 11493    |\n|    total_timesteps | 679759   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0237  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 679680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1753.4956976216727\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 6888     |\n|    fps             | 59       |\n|    time_elapsed    | 11497    |\n|    total_timesteps | 680016   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0098   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 679936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 6892     |\n|    fps             | 59       |\n|    time_elapsed    | 11502    |\n|    total_timesteps | 680272   |\n| train/             |          |\n|    actor_loss      | 816      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0556  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 680192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 6896     |\n|    fps             | 59       |\n|    time_elapsed    | 11506    |\n|    total_timesteps | 680528   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0398  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 680448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 6900     |\n|    fps             | 59       |\n|    time_elapsed    | 11510    |\n|    total_timesteps | 680783   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.133    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 680704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1751.3848882589002\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6904     |\n|    fps             | 59       |\n|    time_elapsed    | 11515    |\n|    total_timesteps | 681049   |\n| train/             |          |\n|    actor_loss      | 845      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0135   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 680960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6908     |\n|    fps             | 59       |\n|    time_elapsed    | 11519    |\n|    total_timesteps | 681305   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0257  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 681216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 6912     |\n|    fps             | 59       |\n|    time_elapsed    | 11523    |\n|    total_timesteps | 681561   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0649  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 681472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 6916     |\n|    fps             | 59       |\n|    time_elapsed    | 11528    |\n|    total_timesteps | 681817   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0268  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 681728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1749.5804863628614\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 6920     |\n|    fps             | 59       |\n|    time_elapsed    | 11532    |\n|    total_timesteps | 682078   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0119  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 681984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 6924     |\n|    fps             | 59       |\n|    time_elapsed    | 11536    |\n|    total_timesteps | 682335   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0678  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 682240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 6928     |\n|    fps             | 59       |\n|    time_elapsed    | 11541    |\n|    total_timesteps | 682590   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0506   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 682496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 6932     |\n|    fps             | 59       |\n|    time_elapsed    | 11545    |\n|    total_timesteps | 682846   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0584   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 682752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1747.5409914539412\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 6936     |\n|    fps             | 59       |\n|    time_elapsed    | 11549    |\n|    total_timesteps | 683109   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0949   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 683008   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 6940     |\n|    fps             | 59       |\n|    time_elapsed    | 11554    |\n|    total_timesteps | 683380   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0786  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 683264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 6944     |\n|    fps             | 59       |\n|    time_elapsed    | 11558    |\n|    total_timesteps | 683639   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.106   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 683520   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 6948     |\n|    fps             | 59       |\n|    time_elapsed    | 11562    |\n|    total_timesteps | 683896   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0698   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 683776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1745.8843623002062\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 6952     |\n|    fps             | 59       |\n|    time_elapsed    | 11567    |\n|    total_timesteps | 684152   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.047    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 684032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 6956     |\n|    fps             | 59       |\n|    time_elapsed    | 11571    |\n|    total_timesteps | 684409   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.114   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 684288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 6960     |\n|    fps             | 59       |\n|    time_elapsed    | 11575    |\n|    total_timesteps | 684665   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.00108  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 684544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 6964     |\n|    fps             | 59       |\n|    time_elapsed    | 11579    |\n|    total_timesteps | 684927   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.154    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 684800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1743.849009081771\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 6968     |\n|    fps             | 59       |\n|    time_elapsed    | 11584    |\n|    total_timesteps | 685188   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0625  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 685120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 6972     |\n|    fps             | 59       |\n|    time_elapsed    | 11589    |\n|    total_timesteps | 685444   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.137   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 685376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 6976     |\n|    fps             | 59       |\n|    time_elapsed    | 11593    |\n|    total_timesteps | 685705   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 1.17e-05 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 685632   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 6980     |\n|    fps             | 59       |\n|    time_elapsed    | 11597    |\n|    total_timesteps | 685960   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0153   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 685888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1741.8456812578013\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 6984     |\n|    fps             | 59       |\n|    time_elapsed    | 11601    |\n|    total_timesteps | 686216   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0702   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 686144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 6988     |\n|    fps             | 59       |\n|    time_elapsed    | 11606    |\n|    total_timesteps | 686472   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0705  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 686400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 6992     |\n|    fps             | 59       |\n|    time_elapsed    | 11610    |\n|    total_timesteps | 686728   |\n| train/             |          |\n|    actor_loss      | 841      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0243   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 686656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 6996     |\n|    fps             | 59       |\n|    time_elapsed    | 11614    |\n|    total_timesteps | 686987   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.00687 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 686912   |\n---------------------------------\nNew best mean reward across all envs: -1739.7497701959435\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 7000     |\n|    fps             | 59       |\n|    time_elapsed    | 11619    |\n|    total_timesteps | 687243   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0686  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 687168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 7004     |\n|    fps             | 59       |\n|    time_elapsed    | 11623    |\n|    total_timesteps | 687505   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0694   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 687424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 7008     |\n|    fps             | 59       |\n|    time_elapsed    | 11627    |\n|    total_timesteps | 687762   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 12.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0886   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 687680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1737.957995740884\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 7012     |\n|    fps             | 59       |\n|    time_elapsed    | 11632    |\n|    total_timesteps | 688024   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.00459 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 687936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 7016     |\n|    fps             | 59       |\n|    time_elapsed    | 11636    |\n|    total_timesteps | 688281   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.159    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 688192   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 7020     |\n|    fps             | 59       |\n|    time_elapsed    | 11640    |\n|    total_timesteps | 688540   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0841  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 688448   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 7024     |\n|    fps             | 59       |\n|    time_elapsed    | 11644    |\n|    total_timesteps | 688802   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0324   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 688704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1735.9903668147606\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 7028     |\n|    fps             | 59       |\n|    time_elapsed    | 11649    |\n|    total_timesteps | 689061   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0602  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 688960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 7032     |\n|    fps             | 59       |\n|    time_elapsed    | 11653    |\n|    total_timesteps | 689323   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 689216   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 7036     |\n|    fps             | 59       |\n|    time_elapsed    | 11657    |\n|    total_timesteps | 689586   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 689472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 7040     |\n|    fps             | 59       |\n|    time_elapsed    | 11661    |\n|    total_timesteps | 689846   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.00646 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 689728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1734.2401772469625\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 7044     |\n|    fps             | 59       |\n|    time_elapsed    | 11666    |\n|    total_timesteps | 690107   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0222   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 689984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 7048     |\n|    fps             | 59       |\n|    time_elapsed    | 11670    |\n|    total_timesteps | 690363   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0214   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 690240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 7052     |\n|    fps             | 59       |\n|    time_elapsed    | 11675    |\n|    total_timesteps | 690631   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.00425  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 690560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 7056     |\n|    fps             | 59       |\n|    time_elapsed    | 11680    |\n|    total_timesteps | 690897   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0145  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 690816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1732.8321164000226\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 7060     |\n|    fps             | 59       |\n|    time_elapsed    | 11684    |\n|    total_timesteps | 691161   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0995  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 691072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -909     |\n| time/              |          |\n|    episodes        | 7064     |\n|    fps             | 59       |\n|    time_elapsed    | 11688    |\n|    total_timesteps | 691435   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0888  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 691328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -916     |\n| time/              |          |\n|    episodes        | 7068     |\n|    fps             | 59       |\n|    time_elapsed    | 11693    |\n|    total_timesteps | 691691   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.075   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 691584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -922     |\n| time/              |          |\n|    episodes        | 7072     |\n|    fps             | 59       |\n|    time_elapsed    | 11697    |\n|    total_timesteps | 691962   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.219    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 691840   |\n---------------------------------\nNew best mean reward across all envs: -1731.3210123655026\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -921     |\n| time/              |          |\n|    episodes        | 7076     |\n|    fps             | 59       |\n|    time_elapsed    | 11701    |\n|    total_timesteps | 692219   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00237  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 692096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 7080     |\n|    fps             | 59       |\n|    time_elapsed    | 11706    |\n|    total_timesteps | 692480   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 692352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -920     |\n| time/              |          |\n|    episodes        | 7084     |\n|    fps             | 59       |\n|    time_elapsed    | 11711    |\n|    total_timesteps | 692741   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0278  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 692672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1729.4867848340746\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 7088     |\n|    fps             | 59       |\n|    time_elapsed    | 11715    |\n|    total_timesteps | 693004   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.072    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 692928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -928     |\n| time/              |          |\n|    episodes        | 7092     |\n|    fps             | 59       |\n|    time_elapsed    | 11719    |\n|    total_timesteps | 693264   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0157   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 693184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 7096     |\n|    fps             | 59       |\n|    time_elapsed    | 11724    |\n|    total_timesteps | 693529   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0338   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 693440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 7100     |\n|    fps             | 59       |\n|    time_elapsed    | 11728    |\n|    total_timesteps | 693792   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0307  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 693696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1727.5870800942444\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -937     |\n| time/              |          |\n|    episodes        | 7104     |\n|    fps             | 59       |\n|    time_elapsed    | 11732    |\n|    total_timesteps | 694053   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.162    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 693952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -943     |\n| time/              |          |\n|    episodes        | 7108     |\n|    fps             | 59       |\n|    time_elapsed    | 11737    |\n|    total_timesteps | 694323   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.116    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 694208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 7112     |\n|    fps             | 59       |\n|    time_elapsed    | 11741    |\n|    total_timesteps | 694582   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.244    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 694464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -925     |\n| time/              |          |\n|    episodes        | 7116     |\n|    fps             | 59       |\n|    time_elapsed    | 11746    |\n|    total_timesteps | 694839   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.165    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 694720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1725.6370803830594\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -932     |\n| time/              |          |\n|    episodes        | 7120     |\n|    fps             | 59       |\n|    time_elapsed    | 11750    |\n|    total_timesteps | 695100   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.116   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 694976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -928     |\n| time/              |          |\n|    episodes        | 7124     |\n|    fps             | 59       |\n|    time_elapsed    | 11755    |\n|    total_timesteps | 695368   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0772   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 695296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -925     |\n| time/              |          |\n|    episodes        | 7128     |\n|    fps             | 59       |\n|    time_elapsed    | 11760    |\n|    total_timesteps | 695632   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0799  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 695552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -914     |\n| time/              |          |\n|    episodes        | 7132     |\n|    fps             | 59       |\n|    time_elapsed    | 11764    |\n|    total_timesteps | 695893   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0405  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 695808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1723.8751519963832\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -912     |\n| time/              |          |\n|    episodes        | 7136     |\n|    fps             | 59       |\n|    time_elapsed    | 11768    |\n|    total_timesteps | 696161   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.147    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 696064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -905     |\n| time/              |          |\n|    episodes        | 7140     |\n|    fps             | 59       |\n|    time_elapsed    | 11772    |\n|    total_timesteps | 696419   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 696320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 7144     |\n|    fps             | 59       |\n|    time_elapsed    | 11777    |\n|    total_timesteps | 696680   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 696576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -901     |\n| time/              |          |\n|    episodes        | 7148     |\n|    fps             | 59       |\n|    time_elapsed    | 11781    |\n|    total_timesteps | 696937   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.103    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 696832   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1721.9308886651515\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 7152     |\n|    fps             | 59       |\n|    time_elapsed    | 11785    |\n|    total_timesteps | 697193   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0647   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 697088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 7156     |\n|    fps             | 59       |\n|    time_elapsed    | 11790    |\n|    total_timesteps | 697458   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0427  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 697344   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 7160     |\n|    fps             | 59       |\n|    time_elapsed    | 11794    |\n|    total_timesteps | 697719   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.124    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 697600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 7164     |\n|    fps             | 59       |\n|    time_elapsed    | 11799    |\n|    total_timesteps | 697982   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.209   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 697856   |\n---------------------------------\nNew best mean reward across all envs: -1719.6698149598024\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 7168     |\n|    fps             | 59       |\n|    time_elapsed    | 11804    |\n|    total_timesteps | 698248   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0707   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 698176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 7172     |\n|    fps             | 59       |\n|    time_elapsed    | 11809    |\n|    total_timesteps | 698516   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 698432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 7176     |\n|    fps             | 59       |\n|    time_elapsed    | 11813    |\n|    total_timesteps | 698777   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 698688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1718.1621449184784\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 7180     |\n|    fps             | 59       |\n|    time_elapsed    | 11817    |\n|    total_timesteps | 699058   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0656  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 698944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7184     |\n|    fps             | 59       |\n|    time_elapsed    | 11822    |\n|    total_timesteps | 699327   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0102  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 699200   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 7188     |\n|    fps             | 59       |\n|    time_elapsed    | 11826    |\n|    total_timesteps | 699584   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0688   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 699456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 7192     |\n|    fps             | 59       |\n|    time_elapsed    | 11831    |\n|    total_timesteps | 699855   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0279   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 699776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1716.4893703738044\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 7196     |\n|    fps             | 59       |\n|    time_elapsed    | 11836    |\n|    total_timesteps | 700122   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0667  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 700032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 7200     |\n|    fps             | 59       |\n|    time_elapsed    | 11840    |\n|    total_timesteps | 700391   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0943   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 700288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7204     |\n|    fps             | 59       |\n|    time_elapsed    | 11845    |\n|    total_timesteps | 700647   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.219   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 700544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 7208     |\n|    fps             | 59       |\n|    time_elapsed    | 11849    |\n|    total_timesteps | 700907   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.14     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 700800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1714.948697062454\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 7212     |\n|    fps             | 59       |\n|    time_elapsed    | 11853    |\n|    total_timesteps | 701167   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0995  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 701056   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 7216     |\n|    fps             | 59       |\n|    time_elapsed    | 11858    |\n|    total_timesteps | 701433   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 701312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 7220     |\n|    fps             | 59       |\n|    time_elapsed    | 11863    |\n|    total_timesteps | 701714   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.208    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 701632   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 7224     |\n|    fps             | 59       |\n|    time_elapsed    | 11867    |\n|    total_timesteps | 701977   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 701888   |\n---------------------------------\nNew best mean reward across all envs: -1713.2781007055228\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 7228     |\n|    fps             | 59       |\n|    time_elapsed    | 11872    |\n|    total_timesteps | 702247   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0429  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 702144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 7232     |\n|    fps             | 59       |\n|    time_elapsed    | 11876    |\n|    total_timesteps | 702509   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 702400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 7236     |\n|    fps             | 59       |\n|    time_elapsed    | 11881    |\n|    total_timesteps | 702776   |\n| train/             |          |\n|    actor_loss      | 840      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.058   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 702656   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1711.4464006396338\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -884     |\n| time/              |          |\n|    episodes        | 7240     |\n|    fps             | 59       |\n|    time_elapsed    | 11885    |\n|    total_timesteps | 703033   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0296   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 702912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 7244     |\n|    fps             | 59       |\n|    time_elapsed    | 11890    |\n|    total_timesteps | 703291   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00868  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 703168   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 7248     |\n|    fps             | 59       |\n|    time_elapsed    | 11895    |\n|    total_timesteps | 703565   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0251  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 703488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.4     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 7252     |\n|    fps             | 59       |\n|    time_elapsed    | 11899    |\n|    total_timesteps | 703831   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0454  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 703744   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1709.8155203324052\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 7256     |\n|    fps             | 59       |\n|    time_elapsed    | 11904    |\n|    total_timesteps | 704088   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0339  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 704000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.4     |\n|    ep_rew_mean     | -914     |\n| time/              |          |\n|    episodes        | 7260     |\n|    fps             | 59       |\n|    time_elapsed    | 11908    |\n|    total_timesteps | 704355   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.038   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 704256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 7264     |\n|    fps             | 59       |\n|    time_elapsed    | 11912    |\n|    total_timesteps | 704613   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00817  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 704512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -924     |\n| time/              |          |\n|    episodes        | 7268     |\n|    fps             | 59       |\n|    time_elapsed    | 11917    |\n|    total_timesteps | 704876   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0492   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 704768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1708.210839140309\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -930     |\n| time/              |          |\n|    episodes        | 7272     |\n|    fps             | 59       |\n|    time_elapsed    | 11921    |\n|    total_timesteps | 705133   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0517   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 705024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 7276     |\n|    fps             | 59       |\n|    time_elapsed    | 11926    |\n|    total_timesteps | 705405   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.204   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 705280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 7280     |\n|    fps             | 59       |\n|    time_elapsed    | 11930    |\n|    total_timesteps | 705662   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.105   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 705536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -915     |\n| time/              |          |\n|    episodes        | 7284     |\n|    fps             | 59       |\n|    time_elapsed    | 11934    |\n|    total_timesteps | 705918   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 705792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1706.5898614282487\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -919     |\n| time/              |          |\n|    episodes        | 7288     |\n|    fps             | 59       |\n|    time_elapsed    | 11939    |\n|    total_timesteps | 706178   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.185   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 706112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 7292     |\n|    fps             | 59       |\n|    time_elapsed    | 11944    |\n|    total_timesteps | 706442   |\n| train/             |          |\n|    actor_loss      | 843      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.215   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 706368   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -915     |\n| time/              |          |\n|    episodes        | 7296     |\n|    fps             | 59       |\n|    time_elapsed    | 11948    |\n|    total_timesteps | 706711   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0778  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 706624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 7300     |\n|    fps             | 59       |\n|    time_elapsed    | 11953    |\n|    total_timesteps | 706969   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.052    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 706880   |\n---------------------------------\nNew best mean reward across all envs: -1704.8136082310332\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 7304     |\n|    fps             | 59       |\n|    time_elapsed    | 11957    |\n|    total_timesteps | 707228   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.211   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 707136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 7308     |\n|    fps             | 59       |\n|    time_elapsed    | 11961    |\n|    total_timesteps | 707488   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 707392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 7312     |\n|    fps             | 59       |\n|    time_elapsed    | 11966    |\n|    total_timesteps | 707745   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0134   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 707648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1703.0651323017362\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -897     |\n| time/              |          |\n|    episodes        | 7316     |\n|    fps             | 59       |\n|    time_elapsed    | 11970    |\n|    total_timesteps | 708002   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.141   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 707904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 7320     |\n|    fps             | 59       |\n|    time_elapsed    | 11974    |\n|    total_timesteps | 708265   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 708160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 7324     |\n|    fps             | 59       |\n|    time_elapsed    | 11979    |\n|    total_timesteps | 708525   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.205    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 708416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 7328     |\n|    fps             | 59       |\n|    time_elapsed    | 11983    |\n|    total_timesteps | 708781   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.102    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 708672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1701.117428176312\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 7332     |\n|    fps             | 59       |\n|    time_elapsed    | 11987    |\n|    total_timesteps | 709041   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0312  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 708928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 7336     |\n|    fps             | 59       |\n|    time_elapsed    | 11992    |\n|    total_timesteps | 709305   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 709184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 7340     |\n|    fps             | 59       |\n|    time_elapsed    | 11996    |\n|    total_timesteps | 709568   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.089    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 709440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 7344     |\n|    fps             | 59       |\n|    time_elapsed    | 12001    |\n|    total_timesteps | 709831   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0563  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 709760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1699.4613641998474\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 7348     |\n|    fps             | 59       |\n|    time_elapsed    | 12006    |\n|    total_timesteps | 710087   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0014   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 710016   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -886     |\n| time/              |          |\n|    episodes        | 7352     |\n|    fps             | 59       |\n|    time_elapsed    | 12010    |\n|    total_timesteps | 710345   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0748   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 710272   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 7356     |\n|    fps             | 59       |\n|    time_elapsed    | 12014    |\n|    total_timesteps | 710602   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0861   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 710528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -876     |\n| time/              |          |\n|    episodes        | 7360     |\n|    fps             | 59       |\n|    time_elapsed    | 12019    |\n|    total_timesteps | 710864   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 710784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1697.5780329108584\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 7364     |\n|    fps             | 59       |\n|    time_elapsed    | 12023    |\n|    total_timesteps | 711122   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00746 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 711040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 7368     |\n|    fps             | 59       |\n|    time_elapsed    | 12028    |\n|    total_timesteps | 711382   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0229  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 711296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 7372     |\n|    fps             | 59       |\n|    time_elapsed    | 12032    |\n|    total_timesteps | 711640   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.042    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 711552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 7376     |\n|    fps             | 59       |\n|    time_elapsed    | 12036    |\n|    total_timesteps | 711896   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.042   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 711808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1695.9735256476354\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 7380     |\n|    fps             | 59       |\n|    time_elapsed    | 12041    |\n|    total_timesteps | 712159   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.134    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 712064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 7384     |\n|    fps             | 59       |\n|    time_elapsed    | 12045    |\n|    total_timesteps | 712417   |\n| train/             |          |\n|    actor_loss      | 837      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0561   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 712320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 7388     |\n|    fps             | 59       |\n|    time_elapsed    | 12049    |\n|    total_timesteps | 712674   |\n| train/             |          |\n|    actor_loss      | 844      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0723   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 712576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 7392     |\n|    fps             | 59       |\n|    time_elapsed    | 12054    |\n|    total_timesteps | 712932   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.00619  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 712832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1693.9728173555516\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 7396     |\n|    fps             | 59       |\n|    time_elapsed    | 12058    |\n|    total_timesteps | 713188   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0121   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 713088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 7400     |\n|    fps             | 59       |\n|    time_elapsed    | 12062    |\n|    total_timesteps | 713449   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0453   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 713344   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 7404     |\n|    fps             | 59       |\n|    time_elapsed    | 12067    |\n|    total_timesteps | 713708   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 713600   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 7408     |\n|    fps             | 59       |\n|    time_elapsed    | 12071    |\n|    total_timesteps | 713966   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0965  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 713856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1692.2372304079174\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 7412     |\n|    fps             | 59       |\n|    time_elapsed    | 12076    |\n|    total_timesteps | 714224   |\n| train/             |          |\n|    actor_loss      | 836      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0202  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 714112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 7416     |\n|    fps             | 59       |\n|    time_elapsed    | 12080    |\n|    total_timesteps | 714480   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0598  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 714368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 7420     |\n|    fps             | 59       |\n|    time_elapsed    | 12084    |\n|    total_timesteps | 714738   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.16    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 714624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1690.4913525947534\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 7424     |\n|    fps             | 59       |\n|    time_elapsed    | 12089    |\n|    total_timesteps | 715012   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0123  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 714944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 7428     |\n|    fps             | 59       |\n|    time_elapsed    | 12094    |\n|    total_timesteps | 715268   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.109   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 715200   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 7432     |\n|    fps             | 59       |\n|    time_elapsed    | 12098    |\n|    total_timesteps | 715524   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0413   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 715456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 7436     |\n|    fps             | 59       |\n|    time_elapsed    | 12103    |\n|    total_timesteps | 715780   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0268   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 715712   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1688.7608340602653\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 7440     |\n|    fps             | 59       |\n|    time_elapsed    | 12107    |\n|    total_timesteps | 716036   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.082    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 715968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 7444     |\n|    fps             | 59       |\n|    time_elapsed    | 12111    |\n|    total_timesteps | 716292   |\n| train/             |          |\n|    actor_loss      | 826      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 716224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 7448     |\n|    fps             | 59       |\n|    time_elapsed    | 12116    |\n|    total_timesteps | 716556   |\n| train/             |          |\n|    actor_loss      | 838      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0103  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 716480   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 7452     |\n|    fps             | 59       |\n|    time_elapsed    | 12120    |\n|    total_timesteps | 716812   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 716736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1687.0819499132376\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 7456     |\n|    fps             | 59       |\n|    time_elapsed    | 12124    |\n|    total_timesteps | 717069   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0729  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 716992   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 7460     |\n|    fps             | 59       |\n|    time_elapsed    | 12129    |\n|    total_timesteps | 717330   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0147   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 717248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 7464     |\n|    fps             | 59       |\n|    time_elapsed    | 12133    |\n|    total_timesteps | 717585   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.139    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 717504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 7468     |\n|    fps             | 59       |\n|    time_elapsed    | 12137    |\n|    total_timesteps | 717841   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0297   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 717760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1685.3670902772576\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 7472     |\n|    fps             | 59       |\n|    time_elapsed    | 12142    |\n|    total_timesteps | 718103   |\n| train/             |          |\n|    actor_loss      | 834      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0272   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 718016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 7476     |\n|    fps             | 59       |\n|    time_elapsed    | 12146    |\n|    total_timesteps | 718368   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 718272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 7480     |\n|    fps             | 59       |\n|    time_elapsed    | 12150    |\n|    total_timesteps | 718624   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.053   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 718528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 7484     |\n|    fps             | 59       |\n|    time_elapsed    | 12155    |\n|    total_timesteps | 718879   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0682   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 718784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1683.6917262668956\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 7488     |\n|    fps             | 59       |\n|    time_elapsed    | 12159    |\n|    total_timesteps | 719134   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 719040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 7492     |\n|    fps             | 59       |\n|    time_elapsed    | 12163    |\n|    total_timesteps | 719399   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.109    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 719296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 7496     |\n|    fps             | 59       |\n|    time_elapsed    | 12168    |\n|    total_timesteps | 719665   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0502   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 719552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 7500     |\n|    fps             | 59       |\n|    time_elapsed    | 12172    |\n|    total_timesteps | 719920   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 719808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1681.9734698809925\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 7504     |\n|    fps             | 59       |\n|    time_elapsed    | 12176    |\n|    total_timesteps | 720176   |\n| train/             |          |\n|    actor_loss      | 832      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.092   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 720064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 7508     |\n|    fps             | 59       |\n|    time_elapsed    | 12181    |\n|    total_timesteps | 720439   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0808   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 720320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 7512     |\n|    fps             | 59       |\n|    time_elapsed    | 12185    |\n|    total_timesteps | 720697   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.117   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 720576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 7516     |\n|    fps             | 59       |\n|    time_elapsed    | 12189    |\n|    total_timesteps | 720954   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.222   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 720832   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1680.254051531558\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 7520     |\n|    fps             | 59       |\n|    time_elapsed    | 12194    |\n|    total_timesteps | 721227   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0715  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 721152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 7524     |\n|    fps             | 59       |\n|    time_elapsed    | 12199    |\n|    total_timesteps | 721489   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0888  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 721408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 7528     |\n|    fps             | 59       |\n|    time_elapsed    | 12203    |\n|    total_timesteps | 721746   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.124    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 721664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1678.7538788035451\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 7532     |\n|    fps             | 59       |\n|    time_elapsed    | 12207    |\n|    total_timesteps | 722006   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 721920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 7536     |\n|    fps             | 59       |\n|    time_elapsed    | 12212    |\n|    total_timesteps | 722266   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0735   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 722176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 7540     |\n|    fps             | 59       |\n|    time_elapsed    | 12216    |\n|    total_timesteps | 722527   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0898   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 722432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 7544     |\n|    fps             | 59       |\n|    time_elapsed    | 12220    |\n|    total_timesteps | 722803   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.109    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 722688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1677.0583398200163\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7548     |\n|    fps             | 59       |\n|    time_elapsed    | 12225    |\n|    total_timesteps | 723070   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0316   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 722944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7552     |\n|    fps             | 59       |\n|    time_elapsed    | 12230    |\n|    total_timesteps | 723332   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.107    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 723264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 7556     |\n|    fps             | 59       |\n|    time_elapsed    | 12234    |\n|    total_timesteps | 723595   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0693   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 723520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7560     |\n|    fps             | 59       |\n|    time_elapsed    | 12239    |\n|    total_timesteps | 723853   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.07    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 723776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1675.3073618503347\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 7564     |\n|    fps             | 59       |\n|    time_elapsed    | 12243    |\n|    total_timesteps | 724109   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.147    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 724032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 7568     |\n|    fps             | 59       |\n|    time_elapsed    | 12247    |\n|    total_timesteps | 724366   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00239  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 724288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 7572     |\n|    fps             | 59       |\n|    time_elapsed    | 12252    |\n|    total_timesteps | 724623   |\n| train/             |          |\n|    actor_loss      | 816      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 724544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 7576     |\n|    fps             | 59       |\n|    time_elapsed    | 12256    |\n|    total_timesteps | 724887   |\n| train/             |          |\n|    actor_loss      | 835      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0324   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 724800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1673.545571569422\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 7580     |\n|    fps             | 59       |\n|    time_elapsed    | 12260    |\n|    total_timesteps | 725154   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0708  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 725056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 7584     |\n|    fps             | 59       |\n|    time_elapsed    | 12265    |\n|    total_timesteps | 725415   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0515   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 725312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 7588     |\n|    fps             | 59       |\n|    time_elapsed    | 12269    |\n|    total_timesteps | 725678   |\n| train/             |          |\n|    actor_loss      | 833      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0794   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 725568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 7592     |\n|    fps             | 59       |\n|    time_elapsed    | 12274    |\n|    total_timesteps | 725947   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0089  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 725824   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1671.8973357940395\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 7596     |\n|    fps             | 59       |\n|    time_elapsed    | 12279    |\n|    total_timesteps | 726210   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0553  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 726144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 7600     |\n|    fps             | 59       |\n|    time_elapsed    | 12283    |\n|    total_timesteps | 726472   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0207  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 726400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 7604     |\n|    fps             | 59       |\n|    time_elapsed    | 12287    |\n|    total_timesteps | 726734   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0579  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 726656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 7608     |\n|    fps             | 59       |\n|    time_elapsed    | 12292    |\n|    total_timesteps | 726989   |\n| train/             |          |\n|    actor_loss      | 830      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.084    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 726912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1670.157738575023\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 7612     |\n|    fps             | 59       |\n|    time_elapsed    | 12296    |\n|    total_timesteps | 727245   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.17    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 727168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 7616     |\n|    fps             | 59       |\n|    time_elapsed    | 12300    |\n|    total_timesteps | 727499   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0777   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 727424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 7620     |\n|    fps             | 59       |\n|    time_elapsed    | 12305    |\n|    total_timesteps | 727762   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.072    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 727680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1668.603152848463\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 7624     |\n|    fps             | 59       |\n|    time_elapsed    | 12309    |\n|    total_timesteps | 728020   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00569 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 727936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 7628     |\n|    fps             | 59       |\n|    time_elapsed    | 12313    |\n|    total_timesteps | 728276   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.00349 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 728192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 7632     |\n|    fps             | 59       |\n|    time_elapsed    | 12318    |\n|    total_timesteps | 728543   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0164  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 728448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 7636     |\n|    fps             | 59       |\n|    time_elapsed    | 12322    |\n|    total_timesteps | 728815   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.00673 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 728704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1667.0092098024245\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 7640     |\n|    fps             | 59       |\n|    time_elapsed    | 12327    |\n|    total_timesteps | 729074   |\n| train/             |          |\n|    actor_loss      | 816      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00273 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 728960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 7644     |\n|    fps             | 59       |\n|    time_elapsed    | 12331    |\n|    total_timesteps | 729339   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0287   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 729216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 7648     |\n|    fps             | 59       |\n|    time_elapsed    | 12336    |\n|    total_timesteps | 729607   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.133    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 729536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 7652     |\n|    fps             | 59       |\n|    time_elapsed    | 12341    |\n|    total_timesteps | 729884   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00817 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 729792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1665.7267172079974\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 7656     |\n|    fps             | 59       |\n|    time_elapsed    | 12345    |\n|    total_timesteps | 730162   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00275  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 730048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 7660     |\n|    fps             | 59       |\n|    time_elapsed    | 12349    |\n|    total_timesteps | 730432   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0908  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 730304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -876     |\n| time/              |          |\n|    episodes        | 7664     |\n|    fps             | 59       |\n|    time_elapsed    | 12355    |\n|    total_timesteps | 730700   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0692   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 730624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 7668     |\n|    fps             | 59       |\n|    time_elapsed    | 12359    |\n|    total_timesteps | 730981   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.195   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 730880   |\n---------------------------------\nNew best mean reward across all envs: -1664.4954409897764\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.4     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 7672     |\n|    fps             | 59       |\n|    time_elapsed    | 12364    |\n|    total_timesteps | 731260   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.183    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 731136   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.5     |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 7676     |\n|    fps             | 59       |\n|    time_elapsed    | 12369    |\n|    total_timesteps | 731537   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0975   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 731456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.7     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 7680     |\n|    fps             | 59       |\n|    time_elapsed    | 12373    |\n|    total_timesteps | 731820   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0216  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 731712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1663.4605917528502\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.8     |\n|    ep_rew_mean     | -949     |\n| time/              |          |\n|    episodes        | 7684     |\n|    fps             | 59       |\n|    time_elapsed    | 12378    |\n|    total_timesteps | 732093   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0213   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 731968   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.9     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 7688     |\n|    fps             | 59       |\n|    time_elapsed    | 12383    |\n|    total_timesteps | 732365   |\n| train/             |          |\n|    actor_loss      | 824      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0223  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 732288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67       |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 7692     |\n|    fps             | 59       |\n|    time_elapsed    | 12387    |\n|    total_timesteps | 732643   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.125    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 732544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -970     |\n| time/              |          |\n|    episodes        | 7696     |\n|    fps             | 59       |\n|    time_elapsed    | 12392    |\n|    total_timesteps | 732917   |\n| train/             |          |\n|    actor_loss      | 816      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 732800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1662.288169230331\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.2     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 7700     |\n|    fps             | 59       |\n|    time_elapsed    | 12397    |\n|    total_timesteps | 733193   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0511  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 733120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.4     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 7704     |\n|    fps             | 59       |\n|    time_elapsed    | 12402    |\n|    total_timesteps | 733478   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 733376   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -999     |\n| time/              |          |\n|    episodes        | 7708     |\n|    fps             | 59       |\n|    time_elapsed    | 12406    |\n|    total_timesteps | 733744   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.192    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 733632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1661.02901986533\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.7     |\n|    ep_rew_mean     | -995     |\n| time/              |          |\n|    episodes        | 7712     |\n|    fps             | 59       |\n|    time_elapsed    | 12410    |\n|    total_timesteps | 734012   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.122   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 733888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 7716      |\n|    fps             | 59        |\n|    time_elapsed    | 12416     |\n|    total_timesteps | 734310    |\n| train/             |           |\n|    actor_loss      | 809       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0518   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 734208    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 7720      |\n|    fps             | 59        |\n|    time_elapsed    | 12420     |\n|    total_timesteps | 734573    |\n| train/             |           |\n|    actor_loss      | 813       |\n|    critic_loss     | 11.1      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.152    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 734464    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 7724      |\n|    fps             | 59        |\n|    time_elapsed    | 12425     |\n|    total_timesteps | 734853    |\n| train/             |           |\n|    actor_loss      | 798       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.012     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 734784    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1659.9116864817197\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.5      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 7728      |\n|    fps             | 59        |\n|    time_elapsed    | 12430     |\n|    total_timesteps | 735124    |\n| train/             |           |\n|    actor_loss      | 817       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0637   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 735040    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 7732      |\n|    fps             | 59        |\n|    time_elapsed    | 12434     |\n|    total_timesteps | 735411    |\n| train/             |           |\n|    actor_loss      | 812       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 0.0572    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 735296    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 7736      |\n|    fps             | 59        |\n|    time_elapsed    | 12439     |\n|    total_timesteps | 735686    |\n| train/             |           |\n|    actor_loss      | 827       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.105    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 735616    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.8      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7740      |\n|    fps             | 59        |\n|    time_elapsed    | 12444     |\n|    total_timesteps | 735959    |\n| train/             |           |\n|    actor_loss      | 817       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.31      |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 735872    |\n----------------------------------\nargv[0]=\nNew best mean reward across all envs: -1658.9092678313907\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69        |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7744      |\n|    fps             | 59        |\n|    time_elapsed    | 12448     |\n|    total_timesteps | 736235    |\n| train/             |           |\n|    actor_loss      | 822       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0308   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 736128    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7748      |\n|    fps             | 59        |\n|    time_elapsed    | 12453     |\n|    total_timesteps | 736500    |\n| train/             |           |\n|    actor_loss      | 810       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.0209    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 736384    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7752      |\n|    fps             | 59        |\n|    time_elapsed    | 12458     |\n|    total_timesteps | 736776    |\n| train/             |           |\n|    actor_loss      | 814       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.00401  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 736704    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1657.689097192641\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 7756      |\n|    fps             | 59        |\n|    time_elapsed    | 12462     |\n|    total_timesteps | 737036    |\n| train/             |           |\n|    actor_loss      | 818       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.00644  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 736960    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.05e+03 |\n| time/              |           |\n|    episodes        | 7760      |\n|    fps             | 59        |\n|    time_elapsed    | 12467     |\n|    total_timesteps | 737305    |\n| train/             |           |\n|    actor_loss      | 825       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.0972    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 737216    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 7764      |\n|    fps             | 59        |\n|    time_elapsed    | 12471     |\n|    total_timesteps | 737590    |\n| train/             |           |\n|    actor_loss      | 823       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0673   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 737472    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7768      |\n|    fps             | 59        |\n|    time_elapsed    | 12476     |\n|    total_timesteps | 737875    |\n| train/             |           |\n|    actor_loss      | 814       |\n|    critic_loss     | 11.4      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.127     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 737792    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1656.8012190406566\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7772      |\n|    fps             | 59        |\n|    time_elapsed    | 12481     |\n|    total_timesteps | 738148    |\n| train/             |           |\n|    actor_loss      | 811       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.0215   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 738048    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7776      |\n|    fps             | 59        |\n|    time_elapsed    | 12485     |\n|    total_timesteps | 738428    |\n| train/             |           |\n|    actor_loss      | 824       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0339   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 738304    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69        |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7780      |\n|    fps             | 59        |\n|    time_elapsed    | 12491     |\n|    total_timesteps | 738718    |\n| train/             |           |\n|    actor_loss      | 806       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.0451   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 738624    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1655.824481311639\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 7784      |\n|    fps             | 59        |\n|    time_elapsed    | 12495     |\n|    total_timesteps | 739001    |\n| train/             |           |\n|    actor_loss      | 817       |\n|    critic_loss     | 12        |\n|    ent_coef        | 0.213     |\n|    ent_coef_loss   | 0.206     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 738880    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 69.1      |\n|    ep_rew_mean     | -1.07e+03 |\n| time/              |           |\n|    episodes        | 7788      |\n|    fps             | 59        |\n|    time_elapsed    | 12500     |\n|    total_timesteps | 739273    |\n| train/             |           |\n|    actor_loss      | 827       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0354    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 739200    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.9      |\n|    ep_rew_mean     | -1.06e+03 |\n| time/              |           |\n|    episodes        | 7792      |\n|    fps             | 59        |\n|    time_elapsed    | 12505     |\n|    total_timesteps | 739529    |\n| train/             |           |\n|    actor_loss      | 822       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.046    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 739456    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.7      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 7796      |\n|    fps             | 59        |\n|    time_elapsed    | 12509     |\n|    total_timesteps | 739788    |\n| train/             |           |\n|    actor_loss      | 819       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | -0.192    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 739712    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1654.1193762344985\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.6      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 7800      |\n|    fps             | 59        |\n|    time_elapsed    | 12513     |\n|    total_timesteps | 740050    |\n| train/             |           |\n|    actor_loss      | 819       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | 0.121     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 739968    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.3      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 7804      |\n|    fps             | 59        |\n|    time_elapsed    | 12518     |\n|    total_timesteps | 740306    |\n| train/             |           |\n|    actor_loss      | 815       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.214     |\n|    ent_coef_loss   | 0.118     |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 740224    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.4      |\n|    ep_rew_mean     | -1.03e+03 |\n| time/              |           |\n|    episodes        | 7808      |\n|    fps             | 59        |\n|    time_elapsed    | 12522     |\n|    total_timesteps | 740584    |\n| train/             |           |\n|    actor_loss      | 825       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | 0.0306    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 740480    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.3      |\n|    ep_rew_mean     | -1.04e+03 |\n| time/              |           |\n|    episodes        | 7812      |\n|    fps             | 59        |\n|    time_elapsed    | 12526     |\n|    total_timesteps | 740844    |\n| train/             |           |\n|    actor_loss      | 831       |\n|    critic_loss     | 11.8      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.0717   |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 740736    |\n----------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1652.6742246153974\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 7816      |\n|    fps             | 59        |\n|    time_elapsed    | 12531     |\n|    total_timesteps | 741116    |\n| train/             |           |\n|    actor_loss      | 819       |\n|    critic_loss     | 11.5      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | 0.0354    |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 740992    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 68.1      |\n|    ep_rew_mean     | -1.02e+03 |\n| time/              |           |\n|    episodes        | 7820      |\n|    fps             | 59        |\n|    time_elapsed    | 12536     |\n|    total_timesteps | 741384    |\n| train/             |           |\n|    actor_loss      | 815       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.215     |\n|    ent_coef_loss   | -0.00711  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 741312    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 67.9      |\n|    ep_rew_mean     | -1.01e+03 |\n| time/              |           |\n|    episodes        | 7824      |\n|    fps             | 59        |\n|    time_elapsed    | 12540     |\n|    total_timesteps | 741647    |\n| train/             |           |\n|    actor_loss      | 816       |\n|    critic_loss     | 11.7      |\n|    ent_coef        | 0.217     |\n|    ent_coef_loss   | 9.29e-05  |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 741568    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.8     |\n|    ep_rew_mean     | -1e+03   |\n| time/              |          |\n|    episodes        | 7828     |\n|    fps             | 59       |\n|    time_elapsed    | 12545    |\n|    total_timesteps | 741909   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.114    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 741824   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1651.2306528453748\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -980     |\n| time/              |          |\n|    episodes        | 7832     |\n|    fps             | 59       |\n|    time_elapsed    | 12549    |\n|    total_timesteps | 742165   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.125    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 742080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -974     |\n| time/              |          |\n|    episodes        | 7836     |\n|    fps             | 59       |\n|    time_elapsed    | 12553    |\n|    total_timesteps | 742434   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.029    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 742336   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.3     |\n|    ep_rew_mean     | -973     |\n| time/              |          |\n|    episodes        | 7840     |\n|    fps             | 59       |\n|    time_elapsed    | 12557    |\n|    total_timesteps | 742691   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.29     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 742592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.2     |\n|    ep_rew_mean     | -964     |\n| time/              |          |\n|    episodes        | 7844     |\n|    fps             | 59       |\n|    time_elapsed    | 12562    |\n|    total_timesteps | 742959   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.036    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 742848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1649.8001893103442\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.2     |\n|    ep_rew_mean     | -965     |\n| time/              |          |\n|    episodes        | 7848     |\n|    fps             | 59       |\n|    time_elapsed    | 12566    |\n|    total_timesteps | 743222   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0509  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 743104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -961     |\n| time/              |          |\n|    episodes        | 7852     |\n|    fps             | 59       |\n|    time_elapsed    | 12570    |\n|    total_timesteps | 743487   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0134  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 743360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -959     |\n| time/              |          |\n|    episodes        | 7856     |\n|    fps             | 59       |\n|    time_elapsed    | 12575    |\n|    total_timesteps | 743743   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0872  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 743616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1648.4240132232844\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67       |\n|    ep_rew_mean     | -963     |\n| time/              |          |\n|    episodes        | 7860     |\n|    fps             | 59       |\n|    time_elapsed    | 12580    |\n|    total_timesteps | 744008   |\n| train/             |          |\n|    actor_loss      | 828      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0954   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 743936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.9     |\n|    ep_rew_mean     | -944     |\n| time/              |          |\n|    episodes        | 7864     |\n|    fps             | 59       |\n|    time_elapsed    | 12584    |\n|    total_timesteps | 744279   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.00138  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 744192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.7     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 7868     |\n|    fps             | 59       |\n|    time_elapsed    | 12588    |\n|    total_timesteps | 744547   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.145    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 744448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.7     |\n|    ep_rew_mean     | -931     |\n| time/              |          |\n|    episodes        | 7872     |\n|    fps             | 59       |\n|    time_elapsed    | 12593    |\n|    total_timesteps | 744815   |\n| train/             |          |\n|    actor_loss      | 827      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0552   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 744704   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1647.0212287454756\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.5     |\n|    ep_rew_mean     | -913     |\n| time/              |          |\n|    episodes        | 7876     |\n|    fps             | 59       |\n|    time_elapsed    | 12597    |\n|    total_timesteps | 745078   |\n| train/             |          |\n|    actor_loss      | 820      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0796  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 744960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 7880     |\n|    fps             | 59       |\n|    time_elapsed    | 12601    |\n|    total_timesteps | 745341   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0273  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 745216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 7884     |\n|    fps             | 59       |\n|    time_elapsed    | 12605    |\n|    total_timesteps | 745598   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.103    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 745472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 7888     |\n|    fps             | 59       |\n|    time_elapsed    | 12610    |\n|    total_timesteps | 745854   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.282   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 745728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1645.515445094509\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 7892     |\n|    fps             | 59       |\n|    time_elapsed    | 12614    |\n|    total_timesteps | 746110   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00604 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 745984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 7896     |\n|    fps             | 59       |\n|    time_elapsed    | 12618    |\n|    total_timesteps | 746367   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0667   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 746240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -900     |\n| time/              |          |\n|    episodes        | 7900     |\n|    fps             | 59       |\n|    time_elapsed    | 12623    |\n|    total_timesteps | 746641   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0206  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 746560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -905     |\n| time/              |          |\n|    episodes        | 7904     |\n|    fps             | 59       |\n|    time_elapsed    | 12628    |\n|    total_timesteps | 746913   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.107    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 746816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1644.0253961656913\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -894     |\n| time/              |          |\n|    episodes        | 7908     |\n|    fps             | 59       |\n|    time_elapsed    | 12632    |\n|    total_timesteps | 747190   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.042    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 747072   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 7912     |\n|    fps             | 59       |\n|    time_elapsed    | 12637    |\n|    total_timesteps | 747446   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.197   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 747328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 7916     |\n|    fps             | 59       |\n|    time_elapsed    | 12641    |\n|    total_timesteps | 747702   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.172   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 747584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 7920     |\n|    fps             | 59       |\n|    time_elapsed    | 12646    |\n|    total_timesteps | 747970   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0762   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 747904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1642.6026535415772\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 7924     |\n|    fps             | 59       |\n|    time_elapsed    | 12651    |\n|    total_timesteps | 748233   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 748160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -907     |\n| time/              |          |\n|    episodes        | 7928     |\n|    fps             | 59       |\n|    time_elapsed    | 12655    |\n|    total_timesteps | 748508   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.172    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 748416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -906     |\n| time/              |          |\n|    episodes        | 7932     |\n|    fps             | 59       |\n|    time_elapsed    | 12659    |\n|    total_timesteps | 748775   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.273   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 748672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1641.2313090221505\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -902     |\n| time/              |          |\n|    episodes        | 7936     |\n|    fps             | 59       |\n|    time_elapsed    | 12664    |\n|    total_timesteps | 749031   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.00495 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 748928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -896     |\n| time/              |          |\n|    episodes        | 7940     |\n|    fps             | 59       |\n|    time_elapsed    | 12668    |\n|    total_timesteps | 749293   |\n| train/             |          |\n|    actor_loss      | 822      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0706   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 749184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -891     |\n| time/              |          |\n|    episodes        | 7944     |\n|    fps             | 59       |\n|    time_elapsed    | 12673    |\n|    total_timesteps | 749549   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00439  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 749440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 7948     |\n|    fps             | 59       |\n|    time_elapsed    | 12677    |\n|    total_timesteps | 749811   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0206   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 749696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1639.6748785415693\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 7952     |\n|    fps             | 59       |\n|    time_elapsed    | 12682    |\n|    total_timesteps | 750077   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.192   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 749952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 7956     |\n|    fps             | 59       |\n|    time_elapsed    | 12686    |\n|    total_timesteps | 750332   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 750208   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 7960     |\n|    fps             | 59       |\n|    time_elapsed    | 12690    |\n|    total_timesteps | 750588   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0229   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 750464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 7964     |\n|    fps             | 59       |\n|    time_elapsed    | 12695    |\n|    total_timesteps | 750849   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.212   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 750784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1638.0854488494676\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 7968     |\n|    fps             | 59       |\n|    time_elapsed    | 12700    |\n|    total_timesteps | 751107   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0495  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 751040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 7972     |\n|    fps             | 59       |\n|    time_elapsed    | 12704    |\n|    total_timesteps | 751363   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 751296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 7976     |\n|    fps             | 59       |\n|    time_elapsed    | 12708    |\n|    total_timesteps | 751630   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.294    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 751552   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 7980     |\n|    fps             | 59       |\n|    time_elapsed    | 12713    |\n|    total_timesteps | 751889   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0424   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 751808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1636.4693854277293\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 7984     |\n|    fps             | 59       |\n|    time_elapsed    | 12717    |\n|    total_timesteps | 752159   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0702  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 752064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 7988     |\n|    fps             | 59       |\n|    time_elapsed    | 12722    |\n|    total_timesteps | 752415   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0872  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 752320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 7992     |\n|    fps             | 59       |\n|    time_elapsed    | 12727    |\n|    total_timesteps | 752682   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0405   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 752576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 7996     |\n|    fps             | 59       |\n|    time_elapsed    | 12731    |\n|    total_timesteps | 752941   |\n| train/             |          |\n|    actor_loss      | 817      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.172    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 752832   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1634.9263067433499\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 8000     |\n|    fps             | 59       |\n|    time_elapsed    | 12736    |\n|    total_timesteps | 753216   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0663  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 753088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 8004     |\n|    fps             | 59       |\n|    time_elapsed    | 12741    |\n|    total_timesteps | 753490   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.126    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 753408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8008     |\n|    fps             | 59       |\n|    time_elapsed    | 12745    |\n|    total_timesteps | 753753   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0443  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 753664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1633.4385372444865\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 8012     |\n|    fps             | 59       |\n|    time_elapsed    | 12750    |\n|    total_timesteps | 754009   |\n| train/             |          |\n|    actor_loss      | 829      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.11    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 753920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 8016     |\n|    fps             | 59       |\n|    time_elapsed    | 12754    |\n|    total_timesteps | 754279   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0412   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 754176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 8020     |\n|    fps             | 59       |\n|    time_elapsed    | 12759    |\n|    total_timesteps | 754547   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0187  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 754432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8024     |\n|    fps             | 59       |\n|    time_elapsed    | 12763    |\n|    total_timesteps | 754806   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.079   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 754688   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1632.106576302271\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 8028     |\n|    fps             | 59       |\n|    time_elapsed    | 12768    |\n|    total_timesteps | 755078   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0446  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 755008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 8032     |\n|    fps             | 59       |\n|    time_elapsed    | 12773    |\n|    total_timesteps | 755334   |\n| train/             |          |\n|    actor_loss      | 818      |\n|    critic_loss     | 12.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.221    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 755264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 8036     |\n|    fps             | 59       |\n|    time_elapsed    | 12777    |\n|    total_timesteps | 755599   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.069   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 755520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 8040     |\n|    fps             | 59       |\n|    time_elapsed    | 12781    |\n|    total_timesteps | 755855   |\n| train/             |          |\n|    actor_loss      | 831      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 755776   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1630.6655506467073\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 8044     |\n|    fps             | 59       |\n|    time_elapsed    | 12786    |\n|    total_timesteps | 756118   |\n| train/             |          |\n|    actor_loss      | 803      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0222   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 756032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 8048     |\n|    fps             | 59       |\n|    time_elapsed    | 12790    |\n|    total_timesteps | 756379   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.282    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 756288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8052     |\n|    fps             | 59       |\n|    time_elapsed    | 12794    |\n|    total_timesteps | 756635   |\n| train/             |          |\n|    actor_loss      | 823      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0913  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 756544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 8056     |\n|    fps             | 59       |\n|    time_elapsed    | 12798    |\n|    total_timesteps | 756898   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0108  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 756800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1629.1601219847662\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 8060     |\n|    fps             | 59       |\n|    time_elapsed    | 12803    |\n|    total_timesteps | 757158   |\n| train/             |          |\n|    actor_loss      | 807      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.146    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 757056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8064     |\n|    fps             | 59       |\n|    time_elapsed    | 12807    |\n|    total_timesteps | 757414   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.205   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 757312   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 8068     |\n|    fps             | 59       |\n|    time_elapsed    | 12811    |\n|    total_timesteps | 757669   |\n| train/             |          |\n|    actor_loss      | 825      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.133    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 757568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 8072     |\n|    fps             | 59       |\n|    time_elapsed    | 12816    |\n|    total_timesteps | 757937   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00446  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 757824   |\n---------------------------------\nNew best mean reward across all envs: -1627.811735263074\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8076     |\n|    fps             | 59       |\n|    time_elapsed    | 12820    |\n|    total_timesteps | 758199   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.152   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 758080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 8080     |\n|    fps             | 59       |\n|    time_elapsed    | 12826    |\n|    total_timesteps | 758465   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0404   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 758400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8084     |\n|    fps             | 59       |\n|    time_elapsed    | 12830    |\n|    total_timesteps | 758721   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 758656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 8088     |\n|    fps             | 59       |\n|    time_elapsed    | 12834    |\n|    total_timesteps | 758977   |\n| train/             |          |\n|    actor_loss      | 800      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0783   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 758912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1626.114506971325\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 8092     |\n|    fps             | 59       |\n|    time_elapsed    | 12839    |\n|    total_timesteps | 759258   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.00563 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 759168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 8096     |\n|    fps             | 59       |\n|    time_elapsed    | 12843    |\n|    total_timesteps | 759514   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0505   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 759424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 8100     |\n|    fps             | 59       |\n|    time_elapsed    | 12847    |\n|    total_timesteps | 759784   |\n| train/             |          |\n|    actor_loss      | 821      |\n|    critic_loss     | 12       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0611   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 759680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1624.6518712073237\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 8104     |\n|    fps             | 59       |\n|    time_elapsed    | 12851    |\n|    total_timesteps | 760040   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0614  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 759936   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 8108     |\n|    fps             | 59       |\n|    time_elapsed    | 12856    |\n|    total_timesteps | 760296   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0287   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 760192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -866     |\n| time/              |          |\n|    episodes        | 8112     |\n|    fps             | 59       |\n|    time_elapsed    | 12860    |\n|    total_timesteps | 760555   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0218   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 760448   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 8116     |\n|    fps             | 59       |\n|    time_elapsed    | 12864    |\n|    total_timesteps | 760811   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00608 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 760704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1623.3362905955846\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8120     |\n|    fps             | 59       |\n|    time_elapsed    | 12869    |\n|    total_timesteps | 761067   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00113  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 760960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 8124     |\n|    fps             | 59       |\n|    time_elapsed    | 12873    |\n|    total_timesteps | 761322   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0992   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 761216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8128     |\n|    fps             | 59       |\n|    time_elapsed    | 12877    |\n|    total_timesteps | 761582   |\n| train/             |          |\n|    actor_loss      | 803      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0697   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 761472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 8132     |\n|    fps             | 59       |\n|    time_elapsed    | 12881    |\n|    total_timesteps | 761841   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0238   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 761728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1621.7837524005574\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8136     |\n|    fps             | 59       |\n|    time_elapsed    | 12886    |\n|    total_timesteps | 762097   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.1     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 761984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 8140     |\n|    fps             | 59       |\n|    time_elapsed    | 12890    |\n|    total_timesteps | 762362   |\n| train/             |          |\n|    actor_loss      | 813      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0643   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 762240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 8144     |\n|    fps             | 59       |\n|    time_elapsed    | 12895    |\n|    total_timesteps | 762632   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0346   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 762560   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 8148     |\n|    fps             | 59       |\n|    time_elapsed    | 12899    |\n|    total_timesteps | 762891   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00518  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 762816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1620.285036722729\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 8152     |\n|    fps             | 59       |\n|    time_elapsed    | 12903    |\n|    total_timesteps | 763147   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0722   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 763072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 8156     |\n|    fps             | 59       |\n|    time_elapsed    | 12908    |\n|    total_timesteps | 763409   |\n| train/             |          |\n|    actor_loss      | 807      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0853  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 763328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 8160     |\n|    fps             | 59       |\n|    time_elapsed    | 12912    |\n|    total_timesteps | 763669   |\n| train/             |          |\n|    actor_loss      | 819      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0394  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 763584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 8164     |\n|    fps             | 59       |\n|    time_elapsed    | 12916    |\n|    total_timesteps | 763926   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0934   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 763840   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1618.7380608129727\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 8168     |\n|    fps             | 59       |\n|    time_elapsed    | 12920    |\n|    total_timesteps | 764182   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0935   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 764096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 8172     |\n|    fps             | 59       |\n|    time_elapsed    | 12925    |\n|    total_timesteps | 764438   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 764352   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 8176     |\n|    fps             | 59       |\n|    time_elapsed    | 12929    |\n|    total_timesteps | 764697   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 764608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 8180     |\n|    fps             | 59       |\n|    time_elapsed    | 12933    |\n|    total_timesteps | 764952   |\n| train/             |          |\n|    actor_loss      | 809      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0555  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 764864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1617.254445585538\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 8184     |\n|    fps             | 59       |\n|    time_elapsed    | 12938    |\n|    total_timesteps | 765208   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 765120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 8188     |\n|    fps             | 59       |\n|    time_elapsed    | 12942    |\n|    total_timesteps | 765464   |\n| train/             |          |\n|    actor_loss      | 814      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.06    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 765376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 8192     |\n|    fps             | 59       |\n|    time_elapsed    | 12946    |\n|    total_timesteps | 765723   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0251   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 765632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 8196     |\n|    fps             | 59       |\n|    time_elapsed    | 12950    |\n|    total_timesteps | 765979   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 765888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1615.8505604304526\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 8200     |\n|    fps             | 59       |\n|    time_elapsed    | 12955    |\n|    total_timesteps | 766235   |\n| train/             |          |\n|    actor_loss      | 794      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 766144   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 8204     |\n|    fps             | 59       |\n|    time_elapsed    | 12959    |\n|    total_timesteps | 766492   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 766400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8208     |\n|    fps             | 59       |\n|    time_elapsed    | 12963    |\n|    total_timesteps | 766748   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.068   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 766656   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1614.556175640026\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8212     |\n|    fps             | 59       |\n|    time_elapsed    | 12968    |\n|    total_timesteps | 767010   |\n| train/             |          |\n|    actor_loss      | 811      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0739  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 766912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 8216     |\n|    fps             | 59       |\n|    time_elapsed    | 12972    |\n|    total_timesteps | 767266   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0531   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 767168   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 8220     |\n|    fps             | 59       |\n|    time_elapsed    | 12976    |\n|    total_timesteps | 767522   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.00579  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 767424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 8224     |\n|    fps             | 59       |\n|    time_elapsed    | 12981    |\n|    total_timesteps | 767777   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0467   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 767680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1612.9903312166632\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8228     |\n|    fps             | 59       |\n|    time_elapsed    | 12985    |\n|    total_timesteps | 768041   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.045   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 767936   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8232     |\n|    fps             | 59       |\n|    time_elapsed    | 12989    |\n|    total_timesteps | 768295   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0366  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 768192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 8236     |\n|    fps             | 59       |\n|    time_elapsed    | 12994    |\n|    total_timesteps | 768552   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.00326 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 768448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 8240     |\n|    fps             | 59       |\n|    time_elapsed    | 12998    |\n|    total_timesteps | 768817   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0464   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 768704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1611.3186683534054\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 8244     |\n|    fps             | 59       |\n|    time_elapsed    | 13002    |\n|    total_timesteps | 769074   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0584  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 768960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 8248     |\n|    fps             | 59       |\n|    time_elapsed    | 13006    |\n|    total_timesteps | 769334   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 769216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 8252     |\n|    fps             | 59       |\n|    time_elapsed    | 13012    |\n|    total_timesteps | 769606   |\n| train/             |          |\n|    actor_loss      | 815      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.00163 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 769536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8256     |\n|    fps             | 59       |\n|    time_elapsed    | 13016    |\n|    total_timesteps | 769869   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0021   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 769792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1609.730265864913\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8260     |\n|    fps             | 59       |\n|    time_elapsed    | 13020    |\n|    total_timesteps | 770125   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0195  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 770048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 8264     |\n|    fps             | 59       |\n|    time_elapsed    | 13025    |\n|    total_timesteps | 770385   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0823   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 770304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -805     |\n| time/              |          |\n|    episodes        | 8268     |\n|    fps             | 59       |\n|    time_elapsed    | 13029    |\n|    total_timesteps | 770648   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.128   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 770560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 8272     |\n|    fps             | 59       |\n|    time_elapsed    | 13033    |\n|    total_timesteps | 770906   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0772  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 770816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1608.0092244013488\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 8276     |\n|    fps             | 59       |\n|    time_elapsed    | 13038    |\n|    total_timesteps | 771167   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 771072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 8280     |\n|    fps             | 59       |\n|    time_elapsed    | 13042    |\n|    total_timesteps | 771430   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.00153 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 771328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 8284     |\n|    fps             | 59       |\n|    time_elapsed    | 13046    |\n|    total_timesteps | 771690   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0633   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 771584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 8288     |\n|    fps             | 59       |\n|    time_elapsed    | 13051    |\n|    total_timesteps | 771946   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.057   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 771840   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1606.4601250113176\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 8292     |\n|    fps             | 59       |\n|    time_elapsed    | 13055    |\n|    total_timesteps | 772201   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.23    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 772096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 8296     |\n|    fps             | 59       |\n|    time_elapsed    | 13059    |\n|    total_timesteps | 772457   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0316   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 772352   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 8300     |\n|    fps             | 59       |\n|    time_elapsed    | 13063    |\n|    total_timesteps | 772714   |\n| train/             |          |\n|    actor_loss      | 795      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 772608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 8304     |\n|    fps             | 59       |\n|    time_elapsed    | 13068    |\n|    total_timesteps | 772975   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0373   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 772864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1604.9719138533032\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 8308     |\n|    fps             | 59       |\n|    time_elapsed    | 13072    |\n|    total_timesteps | 773231   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0754  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 773120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -762     |\n| time/              |          |\n|    episodes        | 8312     |\n|    fps             | 59       |\n|    time_elapsed    | 13077    |\n|    total_timesteps | 773487   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0904  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 773376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 8316     |\n|    fps             | 59       |\n|    time_elapsed    | 13081    |\n|    total_timesteps | 773755   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0126  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 773632   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1603.4493276613161\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 8320     |\n|    fps             | 59       |\n|    time_elapsed    | 13086    |\n|    total_timesteps | 774015   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.124    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 773888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 8324     |\n|    fps             | 59       |\n|    time_elapsed    | 13090    |\n|    total_timesteps | 774270   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00641  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 774144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 8328     |\n|    fps             | 59       |\n|    time_elapsed    | 13095    |\n|    total_timesteps | 774532   |\n| train/             |          |\n|    actor_loss      | 800      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 774464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 8332     |\n|    fps             | 59       |\n|    time_elapsed    | 13099    |\n|    total_timesteps | 774790   |\n| train/             |          |\n|    actor_loss      | 807      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.258    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 774720   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1601.9990770574595\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 8336     |\n|    fps             | 59       |\n|    time_elapsed    | 13104    |\n|    total_timesteps | 775061   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0278   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 774976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 8340     |\n|    fps             | 59       |\n|    time_elapsed    | 13108    |\n|    total_timesteps | 775320   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0502  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 775232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 8344     |\n|    fps             | 59       |\n|    time_elapsed    | 13112    |\n|    total_timesteps | 775585   |\n| train/             |          |\n|    actor_loss      | 795      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00477 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 775488   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 8348     |\n|    fps             | 59       |\n|    time_elapsed    | 13117    |\n|    total_timesteps | 775842   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.044   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 775744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1600.782047270929\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 8352     |\n|    fps             | 59       |\n|    time_elapsed    | 13121    |\n|    total_timesteps | 776103   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0207  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 776000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 8356     |\n|    fps             | 59       |\n|    time_elapsed    | 13125    |\n|    total_timesteps | 776360   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.081    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 776256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 8360     |\n|    fps             | 59       |\n|    time_elapsed    | 13130    |\n|    total_timesteps | 776616   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00146  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 776512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 8364     |\n|    fps             | 59       |\n|    time_elapsed    | 13134    |\n|    total_timesteps | 776890   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.154    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 776768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1599.4953019762725\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 8368     |\n|    fps             | 59       |\n|    time_elapsed    | 13139    |\n|    total_timesteps | 777146   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.141   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 777024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 8372     |\n|    fps             | 59       |\n|    time_elapsed    | 13143    |\n|    total_timesteps | 777406   |\n| train/             |          |\n|    actor_loss      | 802      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.137    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 777280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 8376     |\n|    fps             | 59       |\n|    time_elapsed    | 13147    |\n|    total_timesteps | 777662   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0895  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 777536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 8380     |\n|    fps             | 59       |\n|    time_elapsed    | 13152    |\n|    total_timesteps | 777918   |\n| train/             |          |\n|    actor_loss      | 800      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0833   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 777792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1598.1300392115218\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8384     |\n|    fps             | 59       |\n|    time_elapsed    | 13157    |\n|    total_timesteps | 778177   |\n| train/             |          |\n|    actor_loss      | 808      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00517  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 778112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8388     |\n|    fps             | 59       |\n|    time_elapsed    | 13161    |\n|    total_timesteps | 778438   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0494   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 778368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 8392     |\n|    fps             | 59       |\n|    time_elapsed    | 13165    |\n|    total_timesteps | 778701   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0939   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 778624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 8396     |\n|    fps             | 59       |\n|    time_elapsed    | 13170    |\n|    total_timesteps | 778957   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0492  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 778880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1596.924277085031\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 8400     |\n|    fps             | 59       |\n|    time_elapsed    | 13174    |\n|    total_timesteps | 779213   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0575  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 779136   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8404     |\n|    fps             | 59       |\n|    time_elapsed    | 13179    |\n|    total_timesteps | 779471   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0313  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 779392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -873     |\n| time/              |          |\n|    episodes        | 8408     |\n|    fps             | 59       |\n|    time_elapsed    | 13183    |\n|    total_timesteps | 779731   |\n| train/             |          |\n|    actor_loss      | 794      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0626   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 779648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 8412     |\n|    fps             | 59       |\n|    time_elapsed    | 13187    |\n|    total_timesteps | 779998   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0822   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 779904   |\n---------------------------------\nNew best mean reward across all envs: -1595.479218481445\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 8416     |\n|    fps             | 59       |\n|    time_elapsed    | 13191    |\n|    total_timesteps | 780255   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.00367 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 780160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 8420     |\n|    fps             | 59       |\n|    time_elapsed    | 13196    |\n|    total_timesteps | 780512   |\n| train/             |          |\n|    actor_loss      | 796      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0435  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 780416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 8424     |\n|    fps             | 59       |\n|    time_elapsed    | 13200    |\n|    total_timesteps | 780772   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 780672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1594.1508012089312\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8428     |\n|    fps             | 59       |\n|    time_elapsed    | 13205    |\n|    total_timesteps | 781029   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0686  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 780928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 8432     |\n|    fps             | 59       |\n|    time_elapsed    | 13209    |\n|    total_timesteps | 781288   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0516  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 781184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 8436     |\n|    fps             | 59       |\n|    time_elapsed    | 13213    |\n|    total_timesteps | 781545   |\n| train/             |          |\n|    actor_loss      | 807      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0563   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 781440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 8440     |\n|    fps             | 59       |\n|    time_elapsed    | 13217    |\n|    total_timesteps | 781801   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.114   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 781696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1592.8238779123676\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8444     |\n|    fps             | 59       |\n|    time_elapsed    | 13222    |\n|    total_timesteps | 782074   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.156   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 781952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8448     |\n|    fps             | 59       |\n|    time_elapsed    | 13226    |\n|    total_timesteps | 782331   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0327  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 782208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 8452     |\n|    fps             | 59       |\n|    time_elapsed    | 13231    |\n|    total_timesteps | 782586   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0822   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 782464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 8456     |\n|    fps             | 59       |\n|    time_elapsed    | 13235    |\n|    total_timesteps | 782843   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 782720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1591.572876227686\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 8460     |\n|    fps             | 59       |\n|    time_elapsed    | 13240    |\n|    total_timesteps | 783106   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 783040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -890     |\n| time/              |          |\n|    episodes        | 8464     |\n|    fps             | 59       |\n|    time_elapsed    | 13244    |\n|    total_timesteps | 783370   |\n| train/             |          |\n|    actor_loss      | 802      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 783296   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 8468     |\n|    fps             | 59       |\n|    time_elapsed    | 13249    |\n|    total_timesteps | 783631   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.125    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 783552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 8472     |\n|    fps             | 59       |\n|    time_elapsed    | 13253    |\n|    total_timesteps | 783902   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0421  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 783808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1590.4702516411733\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 8476     |\n|    fps             | 59       |\n|    time_elapsed    | 13258    |\n|    total_timesteps | 784158   |\n| train/             |          |\n|    actor_loss      | 795      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.183   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 784064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -892     |\n| time/              |          |\n|    episodes        | 8480     |\n|    fps             | 59       |\n|    time_elapsed    | 13262    |\n|    total_timesteps | 784413   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 784320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 8484     |\n|    fps             | 59       |\n|    time_elapsed    | 13266    |\n|    total_timesteps | 784670   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.111   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 784576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -895     |\n| time/              |          |\n|    episodes        | 8488     |\n|    fps             | 59       |\n|    time_elapsed    | 13271    |\n|    total_timesteps | 784926   |\n| train/             |          |\n|    actor_loss      | 803      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.15    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 784832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1589.1417339135546\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -885     |\n| time/              |          |\n|    episodes        | 8492     |\n|    fps             | 59       |\n|    time_elapsed    | 13275    |\n|    total_timesteps | 785182   |\n| train/             |          |\n|    actor_loss      | 805      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0252   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 785088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.8      |\n|    ep_rew_mean     | -880      |\n| time/              |           |\n|    episodes        | 8496      |\n|    fps             | 59        |\n|    time_elapsed    | 13279     |\n|    total_timesteps | 785439    |\n| train/             |           |\n|    actor_loss      | 788       |\n|    critic_loss     | 11.6      |\n|    ent_coef        | 0.212     |\n|    ent_coef_loss   | -0.000971 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 785344    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 8500     |\n|    fps             | 59       |\n|    time_elapsed    | 13283    |\n|    total_timesteps | 785694   |\n| train/             |          |\n|    actor_loss      | 795      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0454  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 785600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 8504     |\n|    fps             | 59       |\n|    time_elapsed    | 13288    |\n|    total_timesteps | 785952   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.121    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 785856   |\n---------------------------------\nNew best mean reward across all envs: -1587.7581362999038\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 8508     |\n|    fps             | 59       |\n|    time_elapsed    | 13292    |\n|    total_timesteps | 786214   |\n| train/             |          |\n|    actor_loss      | 796      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0545   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 786112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 8512     |\n|    fps             | 59       |\n|    time_elapsed    | 13297    |\n|    total_timesteps | 786477   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0275  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 786368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 8516     |\n|    fps             | 59       |\n|    time_elapsed    | 13301    |\n|    total_timesteps | 786739   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0555  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 786624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8520     |\n|    fps             | 59       |\n|    time_elapsed    | 13305    |\n|    total_timesteps | 786999   |\n| train/             |          |\n|    actor_loss      | 796      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 786880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1586.2997529848851\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 8524     |\n|    fps             | 59       |\n|    time_elapsed    | 13310    |\n|    total_timesteps | 787260   |\n| train/             |          |\n|    actor_loss      | 807      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0341  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 787136   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 8528     |\n|    fps             | 59       |\n|    time_elapsed    | 13315    |\n|    total_timesteps | 787524   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0358   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 787456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 8532     |\n|    fps             | 59       |\n|    time_elapsed    | 13319    |\n|    total_timesteps | 787780   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.151    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 787712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1584.968115533215\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 8536     |\n|    fps             | 59       |\n|    time_elapsed    | 13324    |\n|    total_timesteps | 788036   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00709  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 787968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 8540     |\n|    fps             | 59       |\n|    time_elapsed    | 13328    |\n|    total_timesteps | 788293   |\n| train/             |          |\n|    actor_loss      | 806      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0171   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 788224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 8544     |\n|    fps             | 59       |\n|    time_elapsed    | 13332    |\n|    total_timesteps | 788549   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.138   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 788480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 8548     |\n|    fps             | 59       |\n|    time_elapsed    | 13337    |\n|    total_timesteps | 788805   |\n| train/             |          |\n|    actor_loss      | 800      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.123    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 788736   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1583.63098710237\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 8552     |\n|    fps             | 59       |\n|    time_elapsed    | 13341    |\n|    total_timesteps | 789064   |\n| train/             |          |\n|    actor_loss      | 812      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0622   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 788992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 8556     |\n|    fps             | 59       |\n|    time_elapsed    | 13345    |\n|    total_timesteps | 789329   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0784  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 789248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8560     |\n|    fps             | 59       |\n|    time_elapsed    | 13350    |\n|    total_timesteps | 789592   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0947  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 789504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 8564     |\n|    fps             | 59       |\n|    time_elapsed    | 13354    |\n|    total_timesteps | 789862   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0693  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 789760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1582.3660836049835\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 8568     |\n|    fps             | 59       |\n|    time_elapsed    | 13359    |\n|    total_timesteps | 790120   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.075   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 790016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 8572     |\n|    fps             | 59       |\n|    time_elapsed    | 13363    |\n|    total_timesteps | 790380   |\n| train/             |          |\n|    actor_loss      | 802      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0722   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 790272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 8576     |\n|    fps             | 59       |\n|    time_elapsed    | 13367    |\n|    total_timesteps | 790637   |\n| train/             |          |\n|    actor_loss      | 803      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0875   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 790528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 8580     |\n|    fps             | 59       |\n|    time_elapsed    | 13371    |\n|    total_timesteps | 790901   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0353   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 790784   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1581.0688340947352\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 8584     |\n|    fps             | 59       |\n|    time_elapsed    | 13376    |\n|    total_timesteps | 791159   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0598   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 791040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 8588     |\n|    fps             | 59       |\n|    time_elapsed    | 13380    |\n|    total_timesteps | 791415   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.00683 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 791296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8592     |\n|    fps             | 59       |\n|    time_elapsed    | 13385    |\n|    total_timesteps | 791679   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00825  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 791552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 8596     |\n|    fps             | 59       |\n|    time_elapsed    | 13390    |\n|    total_timesteps | 791948   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0776   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 791872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1579.945111422116\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 8600     |\n|    fps             | 59       |\n|    time_elapsed    | 13394    |\n|    total_timesteps | 792203   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0643   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 792128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8604     |\n|    fps             | 59       |\n|    time_elapsed    | 13398    |\n|    total_timesteps | 792459   |\n| train/             |          |\n|    actor_loss      | 794      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0688  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 792384   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8608     |\n|    fps             | 59       |\n|    time_elapsed    | 13403    |\n|    total_timesteps | 792715   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0863  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 792640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 8612     |\n|    fps             | 59       |\n|    time_elapsed    | 13407    |\n|    total_timesteps | 792972   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.121    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 792896   |\n---------------------------------\nNew best mean reward across all envs: -1578.5874828088472\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8616     |\n|    fps             | 59       |\n|    time_elapsed    | 13411    |\n|    total_timesteps | 793242   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00194  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 793152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 8620     |\n|    fps             | 59       |\n|    time_elapsed    | 13416    |\n|    total_timesteps | 793500   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0824   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 793408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 8624     |\n|    fps             | 59       |\n|    time_elapsed    | 13420    |\n|    total_timesteps | 793761   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0136   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 793664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1577.211422218832\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 8628     |\n|    fps             | 59       |\n|    time_elapsed    | 13424    |\n|    total_timesteps | 794021   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0249   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 793920   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8632     |\n|    fps             | 59       |\n|    time_elapsed    | 13429    |\n|    total_timesteps | 794284   |\n| train/             |          |\n|    actor_loss      | 794      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0319   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 794176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 8636     |\n|    fps             | 59       |\n|    time_elapsed    | 13433    |\n|    total_timesteps | 794540   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0957   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 794432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 8640     |\n|    fps             | 59       |\n|    time_elapsed    | 13437    |\n|    total_timesteps | 794799   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0722  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 794688   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1575.6887377399103\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 8644     |\n|    fps             | 59       |\n|    time_elapsed    | 13442    |\n|    total_timesteps | 795056   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0393   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 794944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 8648     |\n|    fps             | 59       |\n|    time_elapsed    | 13446    |\n|    total_timesteps | 795313   |\n| train/             |          |\n|    actor_loss      | 804      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0506  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 795200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 8652     |\n|    fps             | 59       |\n|    time_elapsed    | 13450    |\n|    total_timesteps | 795570   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0236   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 795456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 8656     |\n|    fps             | 59       |\n|    time_elapsed    | 13454    |\n|    total_timesteps | 795826   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0745  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 795712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1574.2283867713047\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 8660     |\n|    fps             | 59       |\n|    time_elapsed    | 13459    |\n|    total_timesteps | 796091   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0359  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 795968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 8664     |\n|    fps             | 59       |\n|    time_elapsed    | 13464    |\n|    total_timesteps | 796364   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.207    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 796288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 8668     |\n|    fps             | 59       |\n|    time_elapsed    | 13468    |\n|    total_timesteps | 796621   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 796544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 8672     |\n|    fps             | 59       |\n|    time_elapsed    | 13472    |\n|    total_timesteps | 796890   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 796800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1572.816498797203\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 8676     |\n|    fps             | 59       |\n|    time_elapsed    | 13477    |\n|    total_timesteps | 797149   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0279   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 797056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 8680     |\n|    fps             | 59       |\n|    time_elapsed    | 13481    |\n|    total_timesteps | 797406   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.19    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 797312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 8684     |\n|    fps             | 59       |\n|    time_elapsed    | 13486    |\n|    total_timesteps | 797665   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.139   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 797568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 8688     |\n|    fps             | 59       |\n|    time_elapsed    | 13490    |\n|    total_timesteps | 797925   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0681  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 797824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1571.3960324770637\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 8692     |\n|    fps             | 59       |\n|    time_elapsed    | 13494    |\n|    total_timesteps | 798181   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0392   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 798080   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 8696     |\n|    fps             | 59       |\n|    time_elapsed    | 13498    |\n|    total_timesteps | 798439   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0274   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 798336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 8700     |\n|    fps             | 59       |\n|    time_elapsed    | 13503    |\n|    total_timesteps | 798710   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0439   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 798592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 8704     |\n|    fps             | 59       |\n|    time_elapsed    | 13507    |\n|    total_timesteps | 798967   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 798848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1570.004236774546\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 8708     |\n|    fps             | 59       |\n|    time_elapsed    | 13511    |\n|    total_timesteps | 799223   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.171   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 799104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 8712     |\n|    fps             | 59       |\n|    time_elapsed    | 13516    |\n|    total_timesteps | 799479   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0159   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 799360   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 8716     |\n|    fps             | 59       |\n|    time_elapsed    | 13520    |\n|    total_timesteps | 799737   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.218    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 799616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -778     |\n| time/              |          |\n|    episodes        | 8720     |\n|    fps             | 59       |\n|    time_elapsed    | 13524    |\n|    total_timesteps | 799998   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.037   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 799872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1568.7152286294108\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 8724     |\n|    fps             | 59       |\n|    time_elapsed    | 13529    |\n|    total_timesteps | 800255   |\n| train/             |          |\n|    actor_loss      | 799      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 800128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -791     |\n| time/              |          |\n|    episodes        | 8728     |\n|    fps             | 59       |\n|    time_elapsed    | 13534    |\n|    total_timesteps | 800513   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.133    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 800448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 8732     |\n|    fps             | 59       |\n|    time_elapsed    | 13538    |\n|    total_timesteps | 800778   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.218    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 800704   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1567.4955307577716\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 8736     |\n|    fps             | 59       |\n|    time_elapsed    | 13542    |\n|    total_timesteps | 801034   |\n| train/             |          |\n|    actor_loss      | 810      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.053    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 800960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -801     |\n| time/              |          |\n|    episodes        | 8740     |\n|    fps             | 59       |\n|    time_elapsed    | 13547    |\n|    total_timesteps | 801291   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0226  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 801216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 8744     |\n|    fps             | 59       |\n|    time_elapsed    | 13551    |\n|    total_timesteps | 801552   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0268   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 801472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 8748     |\n|    fps             | 59       |\n|    time_elapsed    | 13555    |\n|    total_timesteps | 801811   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.194    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 801728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1566.3157547783674\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8752     |\n|    fps             | 59       |\n|    time_elapsed    | 13560    |\n|    total_timesteps | 802073   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0923  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 801984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 8756     |\n|    fps             | 59       |\n|    time_elapsed    | 13564    |\n|    total_timesteps | 802329   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.164    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 802240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 8760     |\n|    fps             | 59       |\n|    time_elapsed    | 13568    |\n|    total_timesteps | 802585   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0124  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 802496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 8764     |\n|    fps             | 59       |\n|    time_elapsed    | 13573    |\n|    total_timesteps | 802844   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0284   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 802752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1565.0932878672352\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 8768     |\n|    fps             | 59       |\n|    time_elapsed    | 13577    |\n|    total_timesteps | 803100   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0671   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 803008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 8772     |\n|    fps             | 59       |\n|    time_elapsed    | 13581    |\n|    total_timesteps | 803362   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0208   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 803264   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 8776     |\n|    fps             | 59       |\n|    time_elapsed    | 13586    |\n|    total_timesteps | 803618   |\n| train/             |          |\n|    actor_loss      | 796      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.17     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 803520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 8780     |\n|    fps             | 59       |\n|    time_elapsed    | 13590    |\n|    total_timesteps | 803874   |\n| train/             |          |\n|    actor_loss      | 798      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.134   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 803776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1563.7995597181346\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 8784     |\n|    fps             | 59       |\n|    time_elapsed    | 13594    |\n|    total_timesteps | 804131   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.243   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 804032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 8788     |\n|    fps             | 59       |\n|    time_elapsed    | 13599    |\n|    total_timesteps | 804394   |\n| train/             |          |\n|    actor_loss      | 801      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0274  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 804288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 8792     |\n|    fps             | 59       |\n|    time_elapsed    | 13603    |\n|    total_timesteps | 804650   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0356  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 804544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 8796     |\n|    fps             | 59       |\n|    time_elapsed    | 13607    |\n|    total_timesteps | 804914   |\n| train/             |          |\n|    actor_loss      | 796      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0903   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 804800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1562.3920686085435\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 8800     |\n|    fps             | 59       |\n|    time_elapsed    | 13612    |\n|    total_timesteps | 805172   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.254   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 805056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8804     |\n|    fps             | 59       |\n|    time_elapsed    | 13616    |\n|    total_timesteps | 805428   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0682   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 805312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 8808     |\n|    fps             | 59       |\n|    time_elapsed    | 13620    |\n|    total_timesteps | 805684   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.048   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 805568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8812     |\n|    fps             | 59       |\n|    time_elapsed    | 13625    |\n|    total_timesteps | 805947   |\n| train/             |          |\n|    actor_loss      | 800      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0238  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 805824   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1561.2193264863838\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 8816     |\n|    fps             | 59       |\n|    time_elapsed    | 13629    |\n|    total_timesteps | 806205   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0398   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 806080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 8820     |\n|    fps             | 59       |\n|    time_elapsed    | 13634    |\n|    total_timesteps | 806465   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0618   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 806400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 8824     |\n|    fps             | 59       |\n|    time_elapsed    | 13638    |\n|    total_timesteps | 806727   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.00403  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 806656   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 8828     |\n|    fps             | 59       |\n|    time_elapsed    | 13643    |\n|    total_timesteps | 806983   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0309   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 806912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1559.6818469711634\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8832     |\n|    fps             | 59       |\n|    time_elapsed    | 13647    |\n|    total_timesteps | 807237   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.052   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 807168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -838     |\n| time/              |          |\n|    episodes        | 8836     |\n|    fps             | 59       |\n|    time_elapsed    | 13651    |\n|    total_timesteps | 807493   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0203   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 807424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 8840     |\n|    fps             | 59       |\n|    time_elapsed    | 13656    |\n|    total_timesteps | 807756   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.162   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 807680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1558.512172709788\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 8844     |\n|    fps             | 59       |\n|    time_elapsed    | 13660    |\n|    total_timesteps | 808020   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0492   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 807936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8848     |\n|    fps             | 59       |\n|    time_elapsed    | 13664    |\n|    total_timesteps | 808278   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 808192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 8852     |\n|    fps             | 59       |\n|    time_elapsed    | 13669    |\n|    total_timesteps | 808539   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.236    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 808448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 8856     |\n|    fps             | 59       |\n|    time_elapsed    | 13673    |\n|    total_timesteps | 808795   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0892  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 808704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1557.182820909003\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 8860     |\n|    fps             | 59       |\n|    time_elapsed    | 13677    |\n|    total_timesteps | 809058   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00326  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 808960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 8864     |\n|    fps             | 59       |\n|    time_elapsed    | 13682    |\n|    total_timesteps | 809317   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0189  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 809216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 8868     |\n|    fps             | 59       |\n|    time_elapsed    | 13686    |\n|    total_timesteps | 809572   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0361  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 809472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -821     |\n| time/              |          |\n|    episodes        | 8872     |\n|    fps             | 59       |\n|    time_elapsed    | 13690    |\n|    total_timesteps | 809830   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 809728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1556.0489084685223\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 8876     |\n|    fps             | 59       |\n|    time_elapsed    | 13694    |\n|    total_timesteps | 810088   |\n| train/             |          |\n|    actor_loss      | 797      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0688   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 809984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 8880     |\n|    fps             | 59       |\n|    time_elapsed    | 13699    |\n|    total_timesteps | 810347   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0416   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 810240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 8884     |\n|    fps             | 59       |\n|    time_elapsed    | 13703    |\n|    total_timesteps | 810603   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0213  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 810496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 8888     |\n|    fps             | 59       |\n|    time_elapsed    | 13707    |\n|    total_timesteps | 810862   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0379   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 810752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1554.5973349975118\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 8892     |\n|    fps             | 59       |\n|    time_elapsed    | 13712    |\n|    total_timesteps | 811119   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 811008   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 8896     |\n|    fps             | 59       |\n|    time_elapsed    | 13716    |\n|    total_timesteps | 811375   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.075    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 811264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 8900     |\n|    fps             | 59       |\n|    time_elapsed    | 13720    |\n|    total_timesteps | 811631   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0808  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 811520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 8904     |\n|    fps             | 59       |\n|    time_elapsed    | 13725    |\n|    total_timesteps | 811889   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.157   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 811776   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1553.4607961723545\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 8908     |\n|    fps             | 59       |\n|    time_elapsed    | 13729    |\n|    total_timesteps | 812146   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0238   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 812032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 8912     |\n|    fps             | 59       |\n|    time_elapsed    | 13733    |\n|    total_timesteps | 812403   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.161    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 812288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 8916     |\n|    fps             | 59       |\n|    time_elapsed    | 13738    |\n|    total_timesteps | 812659   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0467   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 812544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 8920     |\n|    fps             | 59       |\n|    time_elapsed    | 13742    |\n|    total_timesteps | 812917   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0233   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 812800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1552.108094452283\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 8924     |\n|    fps             | 59       |\n|    time_elapsed    | 13746    |\n|    total_timesteps | 813180   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.173    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 813056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 8928     |\n|    fps             | 59       |\n|    time_elapsed    | 13751    |\n|    total_timesteps | 813451   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0273   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 813376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 8932     |\n|    fps             | 59       |\n|    time_elapsed    | 13756    |\n|    total_timesteps | 813706   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.127   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 813632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -829     |\n| time/              |          |\n|    episodes        | 8936     |\n|    fps             | 59       |\n|    time_elapsed    | 13760    |\n|    total_timesteps | 813974   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.00889 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 813888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1550.9628267717555\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 8940     |\n|    fps             | 59       |\n|    time_elapsed    | 13764    |\n|    total_timesteps | 814236   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.00693 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 814144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 8944     |\n|    fps             | 59       |\n|    time_elapsed    | 13769    |\n|    total_timesteps | 814497   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0456   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 814400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 8948     |\n|    fps             | 59       |\n|    time_elapsed    | 13773    |\n|    total_timesteps | 814754   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 814656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1549.9039476696805\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 8952     |\n|    fps             | 59       |\n|    time_elapsed    | 13777    |\n|    total_timesteps | 815014   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.249    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 814912   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8956     |\n|    fps             | 59       |\n|    time_elapsed    | 13782    |\n|    total_timesteps | 815280   |\n| train/             |          |\n|    actor_loss      | 795      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0253   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 815168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 8960     |\n|    fps             | 59       |\n|    time_elapsed    | 13786    |\n|    total_timesteps | 815538   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.047    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 815424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8964     |\n|    fps             | 59       |\n|    time_elapsed    | 13790    |\n|    total_timesteps | 815807   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.328   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 815680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1548.8456999918685\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 8968     |\n|    fps             | 59       |\n|    time_elapsed    | 13795    |\n|    total_timesteps | 816064   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0474  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 815936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8972     |\n|    fps             | 59       |\n|    time_elapsed    | 13799    |\n|    total_timesteps | 816319   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0445  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 816192   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 8976     |\n|    fps             | 59       |\n|    time_elapsed    | 13803    |\n|    total_timesteps | 816576   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0856   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 816448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 8980     |\n|    fps             | 59       |\n|    time_elapsed    | 13808    |\n|    total_timesteps | 816834   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0909  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 816768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1547.60325156277\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 8984     |\n|    fps             | 59       |\n|    time_elapsed    | 13813    |\n|    total_timesteps | 817097   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 817024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 8988     |\n|    fps             | 59       |\n|    time_elapsed    | 13817    |\n|    total_timesteps | 817358   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0603  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 817280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 8992     |\n|    fps             | 59       |\n|    time_elapsed    | 13821    |\n|    total_timesteps | 817615   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.2      |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 817536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 8996     |\n|    fps             | 59       |\n|    time_elapsed    | 13826    |\n|    total_timesteps | 817871   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0882   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 817792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1546.2967834091346\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9000     |\n|    fps             | 59       |\n|    time_elapsed    | 13830    |\n|    total_timesteps | 818131   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.036    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 818048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 9004     |\n|    fps             | 59       |\n|    time_elapsed    | 13834    |\n|    total_timesteps | 818396   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.074    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 818304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 9008     |\n|    fps             | 59       |\n|    time_elapsed    | 13839    |\n|    total_timesteps | 818652   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.123   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 818560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 9012     |\n|    fps             | 59       |\n|    time_elapsed    | 13843    |\n|    total_timesteps | 818912   |\n| train/             |          |\n|    actor_loss      | 790      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0162   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 818816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1545.1423758091018\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 9016     |\n|    fps             | 59       |\n|    time_elapsed    | 13847    |\n|    total_timesteps | 819187   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0197   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 819072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 9020     |\n|    fps             | 59       |\n|    time_elapsed    | 13852    |\n|    total_timesteps | 819450   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 819328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -883     |\n| time/              |          |\n|    episodes        | 9024     |\n|    fps             | 59       |\n|    time_elapsed    | 13856    |\n|    total_timesteps | 819706   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0898   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 819584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 9028     |\n|    fps             | 59       |\n|    time_elapsed    | 13861    |\n|    total_timesteps | 819973   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 819904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1544.2483743244225\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -887     |\n| time/              |          |\n|    episodes        | 9032     |\n|    fps             | 59       |\n|    time_elapsed    | 13866    |\n|    total_timesteps | 820230   |\n| train/             |          |\n|    actor_loss      | 786      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.152    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 820160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -885     |\n| time/              |          |\n|    episodes        | 9036     |\n|    fps             | 59       |\n|    time_elapsed    | 13870    |\n|    total_timesteps | 820487   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 820416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 9040     |\n|    fps             | 59       |\n|    time_elapsed    | 13874    |\n|    total_timesteps | 820746   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0835  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 820672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1542.9357654651833\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 9044     |\n|    fps             | 59       |\n|    time_elapsed    | 13878    |\n|    total_timesteps | 821005   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.144    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 820928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 9048     |\n|    fps             | 59       |\n|    time_elapsed    | 13883    |\n|    total_timesteps | 821264   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.215   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 821184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 9052     |\n|    fps             | 59       |\n|    time_elapsed    | 13887    |\n|    total_timesteps | 821519   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0153   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 821440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 9056     |\n|    fps             | 59       |\n|    time_elapsed    | 13891    |\n|    total_timesteps | 821777   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.00547 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 821696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1541.6551931956153\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 9060     |\n|    fps             | 59       |\n|    time_elapsed    | 13896    |\n|    total_timesteps | 822033   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0927   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 821952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9064     |\n|    fps             | 59       |\n|    time_elapsed    | 13900    |\n|    total_timesteps | 822290   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0235  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 822208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 9068     |\n|    fps             | 59       |\n|    time_elapsed    | 13904    |\n|    total_timesteps | 822552   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.198   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 822464   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 9072     |\n|    fps             | 59       |\n|    time_elapsed    | 13909    |\n|    total_timesteps | 822815   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 822720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1540.6802249886084\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 9076     |\n|    fps             | 59       |\n|    time_elapsed    | 13913    |\n|    total_timesteps | 823071   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 822976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 9080     |\n|    fps             | 59       |\n|    time_elapsed    | 13917    |\n|    total_timesteps | 823330   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.18    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 823232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9084     |\n|    fps             | 59       |\n|    time_elapsed    | 13922    |\n|    total_timesteps | 823587   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.162    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 823488   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 9088     |\n|    fps             | 59       |\n|    time_elapsed    | 13926    |\n|    total_timesteps | 823845   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0611  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 823744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1539.236968882197\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 9092     |\n|    fps             | 59       |\n|    time_elapsed    | 13930    |\n|    total_timesteps | 824101   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0863   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 824000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 9096     |\n|    fps             | 59       |\n|    time_elapsed    | 13935    |\n|    total_timesteps | 824357   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0544  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 824256   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 9100     |\n|    fps             | 59       |\n|    time_elapsed    | 13939    |\n|    total_timesteps | 824618   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0458  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 824512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 9104     |\n|    fps             | 59       |\n|    time_elapsed    | 13943    |\n|    total_timesteps | 824881   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0594   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 824768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1538.1492819559883\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 9108     |\n|    fps             | 59       |\n|    time_elapsed    | 13947    |\n|    total_timesteps | 825139   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 825024   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 9112     |\n|    fps             | 59       |\n|    time_elapsed    | 13952    |\n|    total_timesteps | 825400   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0517  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 825280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 9116     |\n|    fps             | 59       |\n|    time_elapsed    | 13957    |\n|    total_timesteps | 825675   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00512 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 825600   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 9120     |\n|    fps             | 59       |\n|    time_elapsed    | 13961    |\n|    total_timesteps | 825931   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0685   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 825856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1537.1633913277835\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -870     |\n| time/              |          |\n|    episodes        | 9124     |\n|    fps             | 59       |\n|    time_elapsed    | 13965    |\n|    total_timesteps | 826206   |\n| train/             |          |\n|    actor_loss      | 791      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 826112   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 9128     |\n|    fps             | 59       |\n|    time_elapsed    | 13970    |\n|    total_timesteps | 826467   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0404  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 826368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 9132     |\n|    fps             | 59       |\n|    time_elapsed    | 13974    |\n|    total_timesteps | 826722   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0357  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 826624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 9136     |\n|    fps             | 59       |\n|    time_elapsed    | 13978    |\n|    total_timesteps | 826978   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0131   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 826880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1535.9800891892637\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9140     |\n|    fps             | 59       |\n|    time_elapsed    | 13983    |\n|    total_timesteps | 827240   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0227  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 827136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 9144     |\n|    fps             | 59       |\n|    time_elapsed    | 13987    |\n|    total_timesteps | 827500   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0539   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 827392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -859     |\n| time/              |          |\n|    episodes        | 9148     |\n|    fps             | 59       |\n|    time_elapsed    | 13991    |\n|    total_timesteps | 827760   |\n| train/             |          |\n|    actor_loss      | 788      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0341  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 827648   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1534.8623107293765\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 9152     |\n|    fps             | 59       |\n|    time_elapsed    | 13996    |\n|    total_timesteps | 828027   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0782   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 827904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 9156     |\n|    fps             | 59       |\n|    time_elapsed    | 14000    |\n|    total_timesteps | 828287   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0605   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 828160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -860     |\n| time/              |          |\n|    episodes        | 9160     |\n|    fps             | 59       |\n|    time_elapsed    | 14005    |\n|    total_timesteps | 828556   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.089    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 828480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 9164     |\n|    fps             | 59       |\n|    time_elapsed    | 14009    |\n|    total_timesteps | 828831   |\n| train/             |          |\n|    actor_loss      | 792      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0542  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 828736   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1533.7319330182977\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 9168     |\n|    fps             | 59       |\n|    time_elapsed    | 14014    |\n|    total_timesteps | 829092   |\n| train/             |          |\n|    actor_loss      | 794      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0615  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 828992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 9172     |\n|    fps             | 59       |\n|    time_elapsed    | 14018    |\n|    total_timesteps | 829348   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0559   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 829248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9176     |\n|    fps             | 59       |\n|    time_elapsed    | 14023    |\n|    total_timesteps | 829620   |\n| train/             |          |\n|    actor_loss      | 793      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 829504   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 9180     |\n|    fps             | 59       |\n|    time_elapsed    | 14027    |\n|    total_timesteps | 829885   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.024    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 829760   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1532.7067854693375\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 9184     |\n|    fps             | 59       |\n|    time_elapsed    | 14032    |\n|    total_timesteps | 830159   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.074    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 830080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 9188     |\n|    fps             | 59       |\n|    time_elapsed    | 14036    |\n|    total_timesteps | 830415   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0262  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 830336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -882     |\n| time/              |          |\n|    episodes        | 9192     |\n|    fps             | 59       |\n|    time_elapsed    | 14041    |\n|    total_timesteps | 830674   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0717  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 830592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 9196     |\n|    fps             | 59       |\n|    time_elapsed    | 14045    |\n|    total_timesteps | 830932   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.056   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 830848   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1531.4909010826655\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 9200     |\n|    fps             | 59       |\n|    time_elapsed    | 14049    |\n|    total_timesteps | 831195   |\n| train/             |          |\n|    actor_loss      | 789      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.111    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 831104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -865     |\n| time/              |          |\n|    episodes        | 9204     |\n|    fps             | 59       |\n|    time_elapsed    | 14054    |\n|    total_timesteps | 831457   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.105   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 831360   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -864     |\n| time/              |          |\n|    episodes        | 9208     |\n|    fps             | 59       |\n|    time_elapsed    | 14058    |\n|    total_timesteps | 831719   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0196  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 831616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 9212     |\n|    fps             | 59       |\n|    time_elapsed    | 14062    |\n|    total_timesteps | 831975   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0783   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 831872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1530.2734821022807\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 9216     |\n|    fps             | 59       |\n|    time_elapsed    | 14067    |\n|    total_timesteps | 832251   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0263  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 832128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -853     |\n| time/              |          |\n|    episodes        | 9220     |\n|    fps             | 59       |\n|    time_elapsed    | 14072    |\n|    total_timesteps | 832513   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 832448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 9224     |\n|    fps             | 59       |\n|    time_elapsed    | 14076    |\n|    total_timesteps | 832770   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0126  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 832704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1529.2231511570392\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 9228     |\n|    fps             | 59       |\n|    time_elapsed    | 14081    |\n|    total_timesteps | 833031   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0761  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 832960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 9232     |\n|    fps             | 59       |\n|    time_elapsed    | 14085    |\n|    total_timesteps | 833305   |\n| train/             |          |\n|    actor_loss      | 785      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.116    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 833216   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 9236     |\n|    fps             | 59       |\n|    time_elapsed    | 14089    |\n|    total_timesteps | 833565   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00464  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 833472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 9240     |\n|    fps             | 59       |\n|    time_elapsed    | 14093    |\n|    total_timesteps | 833824   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0578   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 833728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1528.0320375131557\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 9244     |\n|    fps             | 59       |\n|    time_elapsed    | 14098    |\n|    total_timesteps | 834081   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0577   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 833984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 9248     |\n|    fps             | 59       |\n|    time_elapsed    | 14102    |\n|    total_timesteps | 834338   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.014    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 834240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 9252     |\n|    fps             | 59       |\n|    time_elapsed    | 14106    |\n|    total_timesteps | 834595   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0423  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 834496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 9256     |\n|    fps             | 59       |\n|    time_elapsed    | 14111    |\n|    total_timesteps | 834859   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0652   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 834752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1526.9734242972236\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.7     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 9260     |\n|    fps             | 59       |\n|    time_elapsed    | 14115    |\n|    total_timesteps | 835123   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0322  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 835008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 9264     |\n|    fps             | 59       |\n|    time_elapsed    | 14119    |\n|    total_timesteps | 835382   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 835264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 9268     |\n|    fps             | 59       |\n|    time_elapsed    | 14124    |\n|    total_timesteps | 835637   |\n| train/             |          |\n|    actor_loss      | 787      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 835520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -845     |\n| time/              |          |\n|    episodes        | 9272     |\n|    fps             | 59       |\n|    time_elapsed    | 14128    |\n|    total_timesteps | 835895   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.156   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 835776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1525.7811231464534\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 9276     |\n|    fps             | 59       |\n|    time_elapsed    | 14133    |\n|    total_timesteps | 836162   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.00356  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 836096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 9280     |\n|    fps             | 59       |\n|    time_elapsed    | 14137    |\n|    total_timesteps | 836418   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.069   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 836352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -833     |\n| time/              |          |\n|    episodes        | 9284     |\n|    fps             | 59       |\n|    time_elapsed    | 14142    |\n|    total_timesteps | 836675   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0729   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 836608   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 9288     |\n|    fps             | 59       |\n|    time_elapsed    | 14146    |\n|    total_timesteps | 836931   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.000413 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 836864   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1524.6437378780906\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -839     |\n| time/              |          |\n|    episodes        | 9292     |\n|    fps             | 59       |\n|    time_elapsed    | 14150    |\n|    total_timesteps | 837189   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 837120   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -848     |\n| time/              |          |\n|    episodes        | 9296     |\n|    fps             | 59       |\n|    time_elapsed    | 14155    |\n|    total_timesteps | 837452   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0818   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 837376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9300     |\n|    fps             | 59       |\n|    time_elapsed    | 14159    |\n|    total_timesteps | 837708   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0548  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 837632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -850     |\n| time/              |          |\n|    episodes        | 9304     |\n|    fps             | 59       |\n|    time_elapsed    | 14163    |\n|    total_timesteps | 837974   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.065   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 837888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1523.5746714616178\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 9308     |\n|    fps             | 59       |\n|    time_elapsed    | 14168    |\n|    total_timesteps | 838233   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.062   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 838144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -841     |\n| time/              |          |\n|    episodes        | 9312     |\n|    fps             | 59       |\n|    time_elapsed    | 14172    |\n|    total_timesteps | 838489   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00424  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 838400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 9316     |\n|    fps             | 59       |\n|    time_elapsed    | 14176    |\n|    total_timesteps | 838748   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00346  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 838656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1522.4004490314871\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 9320     |\n|    fps             | 59       |\n|    time_elapsed    | 14181    |\n|    total_timesteps | 839015   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0776   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 838912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 9324     |\n|    fps             | 59       |\n|    time_elapsed    | 14185    |\n|    total_timesteps | 839271   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0122  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 839168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 9328     |\n|    fps             | 59       |\n|    time_elapsed    | 14189    |\n|    total_timesteps | 839535   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0253  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 839424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 9332     |\n|    fps             | 59       |\n|    time_elapsed    | 14194    |\n|    total_timesteps | 839791   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.102    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 839680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1521.2669218558563\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 9336     |\n|    fps             | 59       |\n|    time_elapsed    | 14198    |\n|    total_timesteps | 840051   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0451  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 839936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 9340     |\n|    fps             | 59       |\n|    time_elapsed    | 14202    |\n|    total_timesteps | 840311   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0491   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 840192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9344     |\n|    fps             | 59       |\n|    time_elapsed    | 14207    |\n|    total_timesteps | 840569   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0348  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 840448   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -837     |\n| time/              |          |\n|    episodes        | 9348     |\n|    fps             | 59       |\n|    time_elapsed    | 14211    |\n|    total_timesteps | 840825   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 840704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1520.086136400456\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 9352     |\n|    fps             | 59       |\n|    time_elapsed    | 14215    |\n|    total_timesteps | 841083   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0465   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 840960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9356     |\n|    fps             | 59       |\n|    time_elapsed    | 14220    |\n|    total_timesteps | 841340   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0543   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 841216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -846     |\n| time/              |          |\n|    episodes        | 9360     |\n|    fps             | 59       |\n|    time_elapsed    | 14225    |\n|    total_timesteps | 841619   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.134    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 841536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -851     |\n| time/              |          |\n|    episodes        | 9364     |\n|    fps             | 59       |\n|    time_elapsed    | 14229    |\n|    total_timesteps | 841876   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.139    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 841792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1519.2485976332505\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -847     |\n| time/              |          |\n|    episodes        | 9368     |\n|    fps             | 59       |\n|    time_elapsed    | 14234    |\n|    total_timesteps | 842139   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0241  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 842048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -852     |\n| time/              |          |\n|    episodes        | 9372     |\n|    fps             | 59       |\n|    time_elapsed    | 14238    |\n|    total_timesteps | 842406   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0252  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 842304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9376     |\n|    fps             | 59       |\n|    time_elapsed    | 14242    |\n|    total_timesteps | 842665   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0659   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 842560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9380     |\n|    fps             | 59       |\n|    time_elapsed    | 14247    |\n|    total_timesteps | 842923   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.113    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 842816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1517.913143282306\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -849     |\n| time/              |          |\n|    episodes        | 9384     |\n|    fps             | 59       |\n|    time_elapsed    | 14251    |\n|    total_timesteps | 843178   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0937  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 843072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -868     |\n| time/              |          |\n|    episodes        | 9388     |\n|    fps             | 59       |\n|    time_elapsed    | 14255    |\n|    total_timesteps | 843455   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.14    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 843328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 9392     |\n|    fps             | 59       |\n|    time_elapsed    | 14260    |\n|    total_timesteps | 843714   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 843648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 9396     |\n|    fps             | 59       |\n|    time_elapsed    | 14265    |\n|    total_timesteps | 843976   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0395   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 843904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1517.0918002030837\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -854     |\n| time/              |          |\n|    episodes        | 9400     |\n|    fps             | 59       |\n|    time_elapsed    | 14269    |\n|    total_timesteps | 844232   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0206   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 844160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9404     |\n|    fps             | 59       |\n|    time_elapsed    | 14273    |\n|    total_timesteps | 844494   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0552   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 844416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -861     |\n| time/              |          |\n|    episodes        | 9408     |\n|    fps             | 59       |\n|    time_elapsed    | 14278    |\n|    total_timesteps | 844753   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0885  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 844672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1515.9906505993433\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -877     |\n| time/              |          |\n|    episodes        | 9412     |\n|    fps             | 59       |\n|    time_elapsed    | 14282    |\n|    total_timesteps | 845018   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0633   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 844928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -881     |\n| time/              |          |\n|    episodes        | 9416     |\n|    fps             | 59       |\n|    time_elapsed    | 14286    |\n|    total_timesteps | 845276   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.00382  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 845184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 9420     |\n|    fps             | 59       |\n|    time_elapsed    | 14291    |\n|    total_timesteps | 845532   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0595   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 845440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 9424     |\n|    fps             | 59       |\n|    time_elapsed    | 14295    |\n|    total_timesteps | 845789   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0192   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 845696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1514.8908740452714\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 9428     |\n|    fps             | 59       |\n|    time_elapsed    | 14299    |\n|    total_timesteps | 846046   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0465  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 845952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -858     |\n| time/              |          |\n|    episodes        | 9432     |\n|    fps             | 59       |\n|    time_elapsed    | 14304    |\n|    total_timesteps | 846303   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0276  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 846208   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -871     |\n| time/              |          |\n|    episodes        | 9436     |\n|    fps             | 59       |\n|    time_elapsed    | 14308    |\n|    total_timesteps | 846573   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.14     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 846464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -869     |\n| time/              |          |\n|    episodes        | 9440     |\n|    fps             | 59       |\n|    time_elapsed    | 14312    |\n|    total_timesteps | 846829   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 846720   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1513.7616391758907\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -863     |\n| time/              |          |\n|    episodes        | 9444     |\n|    fps             | 59       |\n|    time_elapsed    | 14317    |\n|    total_timesteps | 847088   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0883  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 846976   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -878     |\n| time/              |          |\n|    episodes        | 9448     |\n|    fps             | 59       |\n|    time_elapsed    | 14321    |\n|    total_timesteps | 847355   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0137   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 847232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 9452     |\n|    fps             | 59       |\n|    time_elapsed    | 14326    |\n|    total_timesteps | 847618   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0573  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 847552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -880     |\n| time/              |          |\n|    episodes        | 9456     |\n|    fps             | 59       |\n|    time_elapsed    | 14330    |\n|    total_timesteps | 847874   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0286  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 847808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1512.9416832179409\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -867     |\n| time/              |          |\n|    episodes        | 9460     |\n|    fps             | 59       |\n|    time_elapsed    | 14335    |\n|    total_timesteps | 848145   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0195  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 848064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 9464     |\n|    fps             | 59       |\n|    time_elapsed    | 14339    |\n|    total_timesteps | 848408   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.038    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 848320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -893     |\n| time/              |          |\n|    episodes        | 9468     |\n|    fps             | 59       |\n|    time_elapsed    | 14343    |\n|    total_timesteps | 848687   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.127    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 848576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -905     |\n| time/              |          |\n|    episodes        | 9472     |\n|    fps             | 59       |\n|    time_elapsed    | 14348    |\n|    total_timesteps | 848955   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.053    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 848832   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1512.1570915701013\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.6     |\n|    ep_rew_mean     | -911     |\n| time/              |          |\n|    episodes        | 9476     |\n|    fps             | 59       |\n|    time_elapsed    | 14353    |\n|    total_timesteps | 849223   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0423  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 849152   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -929     |\n| time/              |          |\n|    episodes        | 9480     |\n|    fps             | 59       |\n|    time_elapsed    | 14358    |\n|    total_timesteps | 849510   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.15    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 849408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -922     |\n| time/              |          |\n|    episodes        | 9484     |\n|    fps             | 59       |\n|    time_elapsed    | 14362    |\n|    total_timesteps | 849767   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.274   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 849664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1511.29260952098\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -909     |\n| time/              |          |\n|    episodes        | 9488     |\n|    fps             | 59       |\n|    time_elapsed    | 14366    |\n|    total_timesteps | 850035   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0209   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 849920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -917     |\n| time/              |          |\n|    episodes        | 9492     |\n|    fps             | 59       |\n|    time_elapsed    | 14370    |\n|    total_timesteps | 850301   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0835  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 850176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66       |\n|    ep_rew_mean     | -923     |\n| time/              |          |\n|    episodes        | 9496     |\n|    fps             | 59       |\n|    time_elapsed    | 14376    |\n|    total_timesteps | 850578   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0913  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 850496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -933     |\n| time/              |          |\n|    episodes        | 9500     |\n|    fps             | 59       |\n|    time_elapsed    | 14380    |\n|    total_timesteps | 850859   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 850752   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1510.4248608032356\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.3     |\n|    ep_rew_mean     | -928     |\n| time/              |          |\n|    episodes        | 9504     |\n|    fps             | 59       |\n|    time_elapsed    | 14385    |\n|    total_timesteps | 851129   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0206   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 851008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.5     |\n|    ep_rew_mean     | -934     |\n| time/              |          |\n|    episodes        | 9508     |\n|    fps             | 59       |\n|    time_elapsed    | 14390    |\n|    total_timesteps | 851403   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0647  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 851328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.7     |\n|    ep_rew_mean     | -935     |\n| time/              |          |\n|    episodes        | 9512     |\n|    fps             | 59       |\n|    time_elapsed    | 14394    |\n|    total_timesteps | 851691   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0195   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 851584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.8     |\n|    ep_rew_mean     | -927     |\n| time/              |          |\n|    episodes        | 9516     |\n|    fps             | 59       |\n|    time_elapsed    | 14399    |\n|    total_timesteps | 851959   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0611   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 851840   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1509.5646251193832\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -942     |\n| time/              |          |\n|    episodes        | 9520     |\n|    fps             | 59       |\n|    time_elapsed    | 14404    |\n|    total_timesteps | 852243   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.114    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 852160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.2     |\n|    ep_rew_mean     | -946     |\n| time/              |          |\n|    episodes        | 9524     |\n|    fps             | 59       |\n|    time_elapsed    | 14408    |\n|    total_timesteps | 852514   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00393  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 852416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -954     |\n| time/              |          |\n|    episodes        | 9528     |\n|    fps             | 59       |\n|    time_elapsed    | 14412    |\n|    total_timesteps | 852792   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0152  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 852672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1508.7149917817758\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.6     |\n|    ep_rew_mean     | -961     |\n| time/              |          |\n|    episodes        | 9532     |\n|    fps             | 59       |\n|    time_elapsed    | 14418    |\n|    total_timesteps | 853060   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.11    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 852992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.6     |\n|    ep_rew_mean     | -947     |\n| time/              |          |\n|    episodes        | 9536     |\n|    fps             | 59       |\n|    time_elapsed    | 14422    |\n|    total_timesteps | 853334   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0137  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 853248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.9     |\n|    ep_rew_mean     | -968     |\n| time/              |          |\n|    episodes        | 9540     |\n|    fps             | 59       |\n|    time_elapsed    | 14427    |\n|    total_timesteps | 853622   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.043    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 853504   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.9     |\n|    ep_rew_mean     | -962     |\n| time/              |          |\n|    episodes        | 9544     |\n|    fps             | 59       |\n|    time_elapsed    | 14431    |\n|    total_timesteps | 853882   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0405   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 853760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1507.7745762446418\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68       |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 9548     |\n|    fps             | 59       |\n|    time_elapsed    | 14436    |\n|    total_timesteps | 854152   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.188    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 854080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68.1     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 9552     |\n|    fps             | 59       |\n|    time_elapsed    | 14440    |\n|    total_timesteps | 854427   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0569  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 854336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68.2     |\n|    ep_rew_mean     | -953     |\n| time/              |          |\n|    episodes        | 9556     |\n|    fps             | 59       |\n|    time_elapsed    | 14445    |\n|    total_timesteps | 854699   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0295   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 854592   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68.2     |\n|    ep_rew_mean     | -957     |\n| time/              |          |\n|    episodes        | 9560     |\n|    fps             | 59       |\n|    time_elapsed    | 14449    |\n|    total_timesteps | 854970   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0565  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 854848   |\n---------------------------------\nNew best mean reward across all envs: -1506.837269056516\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68.2     |\n|    ep_rew_mean     | -941     |\n| time/              |          |\n|    episodes        | 9564     |\n|    fps             | 59       |\n|    time_elapsed    | 14454    |\n|    total_timesteps | 855228   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0224   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 855104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 68       |\n|    ep_rew_mean     | -921     |\n| time/              |          |\n|    episodes        | 9568     |\n|    fps             | 59       |\n|    time_elapsed    | 14458    |\n|    total_timesteps | 855486   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 855360   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.9     |\n|    ep_rew_mean     | -910     |\n| time/              |          |\n|    episodes        | 9572     |\n|    fps             | 59       |\n|    time_elapsed    | 14462    |\n|    total_timesteps | 855743   |\n| train/             |          |\n|    actor_loss      | 779      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 855616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1505.54490182454\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.8     |\n|    ep_rew_mean     | -903     |\n| time/              |          |\n|    episodes        | 9576     |\n|    fps             | 59       |\n|    time_elapsed    | 14466    |\n|    total_timesteps | 856000   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 855872   |\n---------------------------------\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -888     |\n| time/              |          |\n|    episodes        | 9580     |\n|    fps             | 59       |\n|    time_elapsed    | 14471    |\n|    total_timesteps | 856260   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0555  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 856192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.5     |\n|    ep_rew_mean     | -889     |\n| time/              |          |\n|    episodes        | 9584     |\n|    fps             | 59       |\n|    time_elapsed    | 14476    |\n|    total_timesteps | 856518   |\n| train/             |          |\n|    actor_loss      | 774      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0774  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 856448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.4     |\n|    ep_rew_mean     | -879     |\n| time/              |          |\n|    episodes        | 9588     |\n|    fps             | 59       |\n|    time_elapsed    | 14480    |\n|    total_timesteps | 856773   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00997  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 856704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1504.5015130259023\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.4     |\n|    ep_rew_mean     | -875     |\n| time/              |          |\n|    episodes        | 9592     |\n|    fps             | 59       |\n|    time_elapsed    | 14485    |\n|    total_timesteps | 857039   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0262  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 856960   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.3     |\n|    ep_rew_mean     | -874     |\n| time/              |          |\n|    episodes        | 9596     |\n|    fps             | 59       |\n|    time_elapsed    | 14489    |\n|    total_timesteps | 857312   |\n| train/             |          |\n|    actor_loss      | 784      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0265  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 857216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.2     |\n|    ep_rew_mean     | -872     |\n| time/              |          |\n|    episodes        | 9600     |\n|    fps             | 59       |\n|    time_elapsed    | 14493    |\n|    total_timesteps | 857578   |\n| train/             |          |\n|    actor_loss      | 783      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.133   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 857472   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 67.1     |\n|    ep_rew_mean     | -862     |\n| time/              |          |\n|    episodes        | 9604     |\n|    fps             | 59       |\n|    time_elapsed    | 14498    |\n|    total_timesteps | 857835   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0735  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 857728   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1503.452091453861\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.9     |\n|    ep_rew_mean     | -856     |\n| time/              |          |\n|    episodes        | 9608     |\n|    fps             | 59       |\n|    time_elapsed    | 14502    |\n|    total_timesteps | 858090   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.015   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 857984   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.8     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9612     |\n|    fps             | 59       |\n|    time_elapsed    | 14507    |\n|    total_timesteps | 858370   |\n| train/             |          |\n|    actor_loss      | 780      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.095    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 858304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.7     |\n|    ep_rew_mean     | -857     |\n| time/              |          |\n|    episodes        | 9616     |\n|    fps             | 59       |\n|    time_elapsed    | 14511    |\n|    total_timesteps | 858626   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0916   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 858560   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.4     |\n|    ep_rew_mean     | -836     |\n| time/              |          |\n|    episodes        | 9620     |\n|    fps             | 59       |\n|    time_elapsed    | 14516    |\n|    total_timesteps | 858881   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 858816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1502.357830241095\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.2     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 9624     |\n|    fps             | 59       |\n|    time_elapsed    | 14520    |\n|    total_timesteps | 859138   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.168   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 859072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 66.1     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 9628     |\n|    fps             | 59       |\n|    time_elapsed    | 14524    |\n|    total_timesteps | 859399   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.132    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 859328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.9     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 9632     |\n|    fps             | 59       |\n|    time_elapsed    | 14529    |\n|    total_timesteps | 859653   |\n| train/             |          |\n|    actor_loss      | 781      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0318   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 859584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.8     |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 9636     |\n|    fps             | 59       |\n|    time_elapsed    | 14533    |\n|    total_timesteps | 859910   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.11    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 859840   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1501.2238057589998\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 9640     |\n|    fps             | 59       |\n|    time_elapsed    | 14537    |\n|    total_timesteps | 860167   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0287  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 860096   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 9644     |\n|    fps             | 59       |\n|    time_elapsed    | 14542    |\n|    total_timesteps | 860424   |\n| train/             |          |\n|    actor_loss      | 775      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.062   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 860352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 9648     |\n|    fps             | 59       |\n|    time_elapsed    | 14546    |\n|    total_timesteps | 860686   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.144    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 860608   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 9652     |\n|    fps             | 59       |\n|    time_elapsed    | 14550    |\n|    total_timesteps | 860943   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0261  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 860864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1500.1457507505256\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 9656     |\n|    fps             | 59       |\n|    time_elapsed    | 14555    |\n|    total_timesteps | 861198   |\n| train/             |          |\n|    actor_loss      | 777      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.00332 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 861120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 9660     |\n|    fps             | 59       |\n|    time_elapsed    | 14559    |\n|    total_timesteps | 861461   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0879  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 861376   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 9664     |\n|    fps             | 59       |\n|    time_elapsed    | 14563    |\n|    total_timesteps | 861719   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0684   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 861632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -817     |\n| time/              |          |\n|    episodes        | 9668     |\n|    fps             | 59       |\n|    time_elapsed    | 14568    |\n|    total_timesteps | 861976   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 861888   |\n---------------------------------\nNew best mean reward across all envs: -1499.045952256637\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 9672     |\n|    fps             | 59       |\n|    time_elapsed    | 14572    |\n|    total_timesteps | 862233   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.169   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 862144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 9676     |\n|    fps             | 59       |\n|    time_elapsed    | 14576    |\n|    total_timesteps | 862490   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.201   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 862400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 9680     |\n|    fps             | 59       |\n|    time_elapsed    | 14581    |\n|    total_timesteps | 862747   |\n| train/             |          |\n|    actor_loss      | 778      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0216   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 862656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1497.9193054037696\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 9684     |\n|    fps             | 59       |\n|    time_elapsed    | 14585    |\n|    total_timesteps | 863006   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0732  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 862912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 9688     |\n|    fps             | 59       |\n|    time_elapsed    | 14589    |\n|    total_timesteps | 863261   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 863168   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 9692     |\n|    fps             | 59       |\n|    time_elapsed    | 14594    |\n|    total_timesteps | 863517   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0122  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 863424   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 9696     |\n|    fps             | 59       |\n|    time_elapsed    | 14598    |\n|    total_timesteps | 863773   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.088   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 863680   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1496.8035213804694\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 9700     |\n|    fps             | 59       |\n|    time_elapsed    | 14602    |\n|    total_timesteps | 864031   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 863936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 9704     |\n|    fps             | 59       |\n|    time_elapsed    | 14607    |\n|    total_timesteps | 864287   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.0282  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 864192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 9708     |\n|    fps             | 59       |\n|    time_elapsed    | 14611    |\n|    total_timesteps | 864553   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.242   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 864448   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 9712     |\n|    fps             | 59       |\n|    time_elapsed    | 14615    |\n|    total_timesteps | 864810   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.2      |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 864704   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1495.762730717262\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 9716     |\n|    fps             | 59       |\n|    time_elapsed    | 14620    |\n|    total_timesteps | 865073   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.127    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 864960   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 9720     |\n|    fps             | 59       |\n|    time_elapsed    | 14624    |\n|    total_timesteps | 865329   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 865216   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 9724     |\n|    fps             | 59       |\n|    time_elapsed    | 14628    |\n|    total_timesteps | 865585   |\n| train/             |          |\n|    actor_loss      | 776      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 865472   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 9728     |\n|    fps             | 59       |\n|    time_elapsed    | 14633    |\n|    total_timesteps | 865850   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 865728   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1494.6163702730844\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.5      |\n|    ep_rew_mean     | -806      |\n| time/              |           |\n|    episodes        | 9732      |\n|    fps             | 59        |\n|    time_elapsed    | 14637     |\n|    total_timesteps | 866106    |\n| train/             |           |\n|    actor_loss      | 755       |\n|    critic_loss     | 11.3      |\n|    ent_coef        | 0.216     |\n|    ent_coef_loss   | -0.000566 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 865984    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 9736     |\n|    fps             | 59       |\n|    time_elapsed    | 14641    |\n|    total_timesteps | 866362   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.22     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 866240   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 9740     |\n|    fps             | 59       |\n|    time_elapsed    | 14646    |\n|    total_timesteps | 866619   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.114    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 866496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 9744     |\n|    fps             | 59       |\n|    time_elapsed    | 14651    |\n|    total_timesteps | 866885   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0348   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 866816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1493.547549203238\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 9748     |\n|    fps             | 59       |\n|    time_elapsed    | 14655    |\n|    total_timesteps | 867146   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0824   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 867072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 9752     |\n|    fps             | 59       |\n|    time_elapsed    | 14659    |\n|    total_timesteps | 867403   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0596  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 867328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 9756     |\n|    fps             | 59       |\n|    time_elapsed    | 14663    |\n|    total_timesteps | 867659   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.043   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 867584   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 9760     |\n|    fps             | 59       |\n|    time_elapsed    | 14668    |\n|    total_timesteps | 867921   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0219  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 867840   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1492.471997327308\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 9764     |\n|    fps             | 59       |\n|    time_elapsed    | 14672    |\n|    total_timesteps | 868178   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.141   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 868096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -809     |\n| time/              |          |\n|    episodes        | 9768     |\n|    fps             | 59       |\n|    time_elapsed    | 14677    |\n|    total_timesteps | 868439   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 868352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 9772     |\n|    fps             | 59       |\n|    time_elapsed    | 14681    |\n|    total_timesteps | 868695   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0143  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 868608   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 9776     |\n|    fps             | 59       |\n|    time_elapsed    | 14685    |\n|    total_timesteps | 868952   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.222    |\n|    ent_coef_loss   | 0.179    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 868864   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1491.4860894500812\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -827     |\n| time/              |          |\n|    episodes        | 9780     |\n|    fps             | 59       |\n|    time_elapsed    | 14690    |\n|    total_timesteps | 869207   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.0963  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 869120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 9784     |\n|    fps             | 59       |\n|    time_elapsed    | 14694    |\n|    total_timesteps | 869472   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0903   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 869376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9788     |\n|    fps             | 59       |\n|    time_elapsed    | 14698    |\n|    total_timesteps | 869728   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0601  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 869632   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 9792     |\n|    fps             | 59       |\n|    time_elapsed    | 14703    |\n|    total_timesteps | 869984   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0606  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 869888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1490.3953616851838\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 9796     |\n|    fps             | 59       |\n|    time_elapsed    | 14707    |\n|    total_timesteps | 870245   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0125  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 870144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9800     |\n|    fps             | 59       |\n|    time_elapsed    | 14711    |\n|    total_timesteps | 870506   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0447   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 870400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 9804     |\n|    fps             | 59       |\n|    time_elapsed    | 14716    |\n|    total_timesteps | 870764   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0808   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 870656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1489.348512361455\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 9808     |\n|    fps             | 59       |\n|    time_elapsed    | 14720    |\n|    total_timesteps | 871025   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 870912   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -828     |\n| time/              |          |\n|    episodes        | 9812     |\n|    fps             | 59       |\n|    time_elapsed    | 14724    |\n|    total_timesteps | 871288   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0353  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 871168   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 9816     |\n|    fps             | 59       |\n|    time_elapsed    | 14729    |\n|    total_timesteps | 871546   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00499  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 871424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -835     |\n| time/              |          |\n|    episodes        | 9820     |\n|    fps             | 59       |\n|    time_elapsed    | 14734    |\n|    total_timesteps | 871823   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0181   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 871744   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1488.4440216812193\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -834     |\n| time/              |          |\n|    episodes        | 9824     |\n|    fps             | 59       |\n|    time_elapsed    | 14738    |\n|    total_timesteps | 872082   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.11     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 872000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 9828     |\n|    fps             | 59       |\n|    time_elapsed    | 14742    |\n|    total_timesteps | 872349   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0651  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 872256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9832     |\n|    fps             | 59       |\n|    time_elapsed    | 14747    |\n|    total_timesteps | 872605   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0154   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 872512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 9836     |\n|    fps             | 59       |\n|    time_elapsed    | 14751    |\n|    total_timesteps | 872863   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0293  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 872768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1487.2253120184876\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 9840     |\n|    fps             | 59       |\n|    time_elapsed    | 14755    |\n|    total_timesteps | 873129   |\n| train/             |          |\n|    actor_loss      | 773      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.134    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 873024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -825     |\n| time/              |          |\n|    episodes        | 9844     |\n|    fps             | 59       |\n|    time_elapsed    | 14760    |\n|    total_timesteps | 873388   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.00568 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 873280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -822     |\n| time/              |          |\n|    episodes        | 9848     |\n|    fps             | 59       |\n|    time_elapsed    | 14764    |\n|    total_timesteps | 873645   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0661  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 873536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 9852     |\n|    fps             | 59       |\n|    time_elapsed    | 14768    |\n|    total_timesteps | 873905   |\n| train/             |          |\n|    actor_loss      | 771      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 873792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1486.1114787075562\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 9856     |\n|    fps             | 59       |\n|    time_elapsed    | 14773    |\n|    total_timesteps | 874166   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0095   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 874048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -805     |\n| time/              |          |\n|    episodes        | 9860     |\n|    fps             | 59       |\n|    time_elapsed    | 14777    |\n|    total_timesteps | 874423   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0269  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 874304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 9864     |\n|    fps             | 59       |\n|    time_elapsed    | 14781    |\n|    total_timesteps | 874684   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0521   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 874560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 9868     |\n|    fps             | 59       |\n|    time_elapsed    | 14786    |\n|    total_timesteps | 874940   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.00899  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 874816   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1484.9947647592696\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 9872     |\n|    fps             | 59       |\n|    time_elapsed    | 14791    |\n|    total_timesteps | 875202   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.037   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 875136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 9876     |\n|    fps             | 59       |\n|    time_elapsed    | 14795    |\n|    total_timesteps | 875475   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00372 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 875392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 9880     |\n|    fps             | 59       |\n|    time_elapsed    | 14800    |\n|    total_timesteps | 875748   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0184   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 875648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1484.0738659725014\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 9884     |\n|    fps             | 59       |\n|    time_elapsed    | 14804    |\n|    total_timesteps | 876016   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.105   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 875904   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 9888     |\n|    fps             | 59       |\n|    time_elapsed    | 14808    |\n|    total_timesteps | 876274   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0936   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 876160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.5     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 9892     |\n|    fps             | 59       |\n|    time_elapsed    | 14813    |\n|    total_timesteps | 876530   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.07     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 876416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 9896     |\n|    fps             | 59       |\n|    time_elapsed    | 14817    |\n|    total_timesteps | 876788   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0236   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 876672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1482.9460971003396\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 9900     |\n|    fps             | 59       |\n|    time_elapsed    | 14821    |\n|    total_timesteps | 877045   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0217   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 876928   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 9904     |\n|    fps             | 59       |\n|    time_elapsed    | 14826    |\n|    total_timesteps | 877301   |\n| train/             |          |\n|    actor_loss      | 750      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.094   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 877184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 9908     |\n|    fps             | 59       |\n|    time_elapsed    | 14830    |\n|    total_timesteps | 877558   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0955   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 877440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 9912     |\n|    fps             | 59       |\n|    time_elapsed    | 14835    |\n|    total_timesteps | 877828   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0761  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 877760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1481.8425250361386\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 9916     |\n|    fps             | 59       |\n|    time_elapsed    | 14839    |\n|    total_timesteps | 878089   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.011    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 878016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 9920     |\n|    fps             | 59       |\n|    time_elapsed    | 14844    |\n|    total_timesteps | 878351   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0609  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 878272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 9924     |\n|    fps             | 59       |\n|    time_elapsed    | 14848    |\n|    total_timesteps | 878618   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0668  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 878528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 9928     |\n|    fps             | 59       |\n|    time_elapsed    | 14853    |\n|    total_timesteps | 878874   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0558  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 878784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1481.0395324770698\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 9932     |\n|    fps             | 59       |\n|    time_elapsed    | 14857    |\n|    total_timesteps | 879130   |\n| train/             |          |\n|    actor_loss      | 747      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 879040   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 9936     |\n|    fps             | 59       |\n|    time_elapsed    | 14861    |\n|    total_timesteps | 879389   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0439   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 879296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 9940     |\n|    fps             | 59       |\n|    time_elapsed    | 14866    |\n|    total_timesteps | 879658   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0479  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 879552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 9944     |\n|    fps             | 59       |\n|    time_elapsed    | 14870    |\n|    total_timesteps | 879914   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.00472  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 879808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1480.0329119449816\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 9948     |\n|    fps             | 59       |\n|    time_elapsed    | 14874    |\n|    total_timesteps | 880169   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0651   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 880064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -824     |\n| time/              |          |\n|    episodes        | 9952     |\n|    fps             | 59       |\n|    time_elapsed    | 14879    |\n|    total_timesteps | 880425   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.032    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 880320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 9956     |\n|    fps             | 59       |\n|    time_elapsed    | 14883    |\n|    total_timesteps | 880695   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0736  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 880576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.4     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 9960     |\n|    fps             | 59       |\n|    time_elapsed    | 14888    |\n|    total_timesteps | 880963   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.138    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 880896   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1479.1146525392162\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -842     |\n| time/              |          |\n|    episodes        | 9964     |\n|    fps             | 59       |\n|    time_elapsed    | 14893    |\n|    total_timesteps | 881219   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.166   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 881152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -844     |\n| time/              |          |\n|    episodes        | 9968     |\n|    fps             | 59       |\n|    time_elapsed    | 14897    |\n|    total_timesteps | 881475   |\n| train/             |          |\n|    actor_loss      | 782      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.204    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 881408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -855     |\n| time/              |          |\n|    episodes        | 9972     |\n|    fps             | 59       |\n|    time_elapsed    | 14902    |\n|    total_timesteps | 881731   |\n| train/             |          |\n|    actor_loss      | 772      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00528 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 881664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -843     |\n| time/              |          |\n|    episodes        | 9976     |\n|    fps             | 59       |\n|    time_elapsed    | 14906    |\n|    total_timesteps | 881987   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0551   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 881920   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1478.008086969399\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -840     |\n| time/              |          |\n|    episodes        | 9980     |\n|    fps             | 59       |\n|    time_elapsed    | 14910    |\n|    total_timesteps | 882246   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0733  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 882176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -832     |\n| time/              |          |\n|    episodes        | 9984     |\n|    fps             | 59       |\n|    time_elapsed    | 14915    |\n|    total_timesteps | 882502   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0415   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 882432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9988     |\n|    fps             | 59       |\n|    time_elapsed    | 14919    |\n|    total_timesteps | 882758   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0791   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 882688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1476.9371071725332\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -831     |\n| time/              |          |\n|    episodes        | 9992     |\n|    fps             | 59       |\n|    time_elapsed    | 14924    |\n|    total_timesteps | 883014   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00167  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 882944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 9996     |\n|    fps             | 59       |\n|    time_elapsed    | 14928    |\n|    total_timesteps | 883270   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 883200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -826     |\n| time/              |          |\n|    episodes        | 10000    |\n|    fps             | 59       |\n|    time_elapsed    | 14932    |\n|    total_timesteps | 883527   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0018  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 883456   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -833     |\n| time/              |          |\n|    episodes        | 10004    |\n|    fps             | 59       |\n|    time_elapsed    | 14937    |\n|    total_timesteps | 883788   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0513   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 883712   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1475.7680778290799\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -830     |\n| time/              |          |\n|    episodes        | 10008    |\n|    fps             | 59       |\n|    time_elapsed    | 14941    |\n|    total_timesteps | 884044   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0935  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 883968   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -823     |\n| time/              |          |\n|    episodes        | 10012    |\n|    fps             | 59       |\n|    time_elapsed    | 14946    |\n|    total_timesteps | 884300   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0904   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 884224   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -818     |\n| time/              |          |\n|    episodes        | 10016    |\n|    fps             | 59       |\n|    time_elapsed    | 14950    |\n|    total_timesteps | 884556   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0501  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 884480   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 10020    |\n|    fps             | 59       |\n|    time_elapsed    | 14954    |\n|    total_timesteps | 884812   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0258  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 884736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1474.6775779934383\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 10024    |\n|    fps             | 59       |\n|    time_elapsed    | 14959    |\n|    total_timesteps | 885068   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0847   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 884992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 10028    |\n|    fps             | 59       |\n|    time_elapsed    | 14963    |\n|    total_timesteps | 885325   |\n| train/             |          |\n|    actor_loss      | 769      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.131   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 885248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10032    |\n|    fps             | 59       |\n|    time_elapsed    | 14968    |\n|    total_timesteps | 885598   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0286   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 885504   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -799     |\n| time/              |          |\n|    episodes        | 10036    |\n|    fps             | 59       |\n|    time_elapsed    | 14972    |\n|    total_timesteps | 885859   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0719  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 885760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1473.6944898185714\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10040    |\n|    fps             | 59       |\n|    time_elapsed    | 14976    |\n|    total_timesteps | 886123   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0849   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 886016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10044    |\n|    fps             | 59       |\n|    time_elapsed    | 14981    |\n|    total_timesteps | 886380   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0377  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 886272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -785     |\n| time/              |          |\n|    episodes        | 10048    |\n|    fps             | 59       |\n|    time_elapsed    | 14985    |\n|    total_timesteps | 886636   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0556   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 886528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10052    |\n|    fps             | 59       |\n|    time_elapsed    | 14990    |\n|    total_timesteps | 886892   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.175    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 886784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1472.553052878599\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -773     |\n| time/              |          |\n|    episodes        | 10056    |\n|    fps             | 59       |\n|    time_elapsed    | 14994    |\n|    total_timesteps | 887148   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0709  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 887040   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10060    |\n|    fps             | 59       |\n|    time_elapsed    | 14998    |\n|    total_timesteps | 887406   |\n| train/             |          |\n|    actor_loss      | 768      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.228    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 887296   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10064    |\n|    fps             | 59       |\n|    time_elapsed    | 15003    |\n|    total_timesteps | 887662   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0152  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 887552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10068    |\n|    fps             | 59       |\n|    time_elapsed    | 15007    |\n|    total_timesteps | 887918   |\n| train/             |          |\n|    actor_loss      | 767      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.146    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 887808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1471.3981468354768\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10072    |\n|    fps             | 59       |\n|    time_elapsed    | 15011    |\n|    total_timesteps | 888174   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0476  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 888064   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10076    |\n|    fps             | 59       |\n|    time_elapsed    | 15016    |\n|    total_timesteps | 888430   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0683  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 888320   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10080    |\n|    fps             | 59       |\n|    time_elapsed    | 15020    |\n|    total_timesteps | 888686   |\n| train/             |          |\n|    actor_loss      | 765      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0103   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 888576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10084    |\n|    fps             | 59       |\n|    time_elapsed    | 15024    |\n|    total_timesteps | 888942   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0115  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 888832   |\n---------------------------------\nNew best mean reward across all envs: -1470.3901300015307\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10088    |\n|    fps             | 59       |\n|    time_elapsed    | 15029    |\n|    total_timesteps | 889198   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0329  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 889088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 10092    |\n|    fps             | 59       |\n|    time_elapsed    | 15033    |\n|    total_timesteps | 889454   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0524  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 889344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10096    |\n|    fps             | 59       |\n|    time_elapsed    | 15037    |\n|    total_timesteps | 889710   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.162    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 889600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10100    |\n|    fps             | 59       |\n|    time_elapsed    | 15042    |\n|    total_timesteps | 889966   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.279   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 889856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1469.4187523299024\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10104    |\n|    fps             | 59       |\n|    time_elapsed    | 15046    |\n|    total_timesteps | 890222   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0528   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 890112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10108    |\n|    fps             | 59       |\n|    time_elapsed    | 15051    |\n|    total_timesteps | 890478   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0554  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 890368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 10112    |\n|    fps             | 59       |\n|    time_elapsed    | 15055    |\n|    total_timesteps | 890738   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.051   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 890624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10116    |\n|    fps             | 59       |\n|    time_elapsed    | 15059    |\n|    total_timesteps | 890994   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.000342 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 890880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1468.344645567493\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 10120    |\n|    fps             | 59       |\n|    time_elapsed    | 15064    |\n|    total_timesteps | 891250   |\n| train/             |          |\n|    actor_loss      | 747      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0514  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 891136   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 10124    |\n|    fps             | 59       |\n|    time_elapsed    | 15068    |\n|    total_timesteps | 891506   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.00361 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 891392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 10128    |\n|    fps             | 59       |\n|    time_elapsed    | 15072    |\n|    total_timesteps | 891763   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.14    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 891648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1467.3506893154185\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 10132    |\n|    fps             | 59       |\n|    time_elapsed    | 15077    |\n|    total_timesteps | 892018   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0205   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 891904   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -784     |\n| time/              |          |\n|    episodes        | 10136    |\n|    fps             | 59       |\n|    time_elapsed    | 15081    |\n|    total_timesteps | 892274   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0649  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 892160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10140    |\n|    fps             | 59       |\n|    time_elapsed    | 15085    |\n|    total_timesteps | 892530   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0869  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 892416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10144    |\n|    fps             | 59       |\n|    time_elapsed    | 15090    |\n|    total_timesteps | 892786   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0189   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 892672   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1466.2518539766506\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 10148    |\n|    fps             | 59       |\n|    time_elapsed    | 15094    |\n|    total_timesteps | 893044   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0864  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 892928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10152    |\n|    fps             | 59       |\n|    time_elapsed    | 15098    |\n|    total_timesteps | 893300   |\n| train/             |          |\n|    actor_loss      | 745      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0207   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 893184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 10156    |\n|    fps             | 59       |\n|    time_elapsed    | 15103    |\n|    total_timesteps | 893556   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00106 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 893440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10160    |\n|    fps             | 59       |\n|    time_elapsed    | 15107    |\n|    total_timesteps | 893812   |\n| train/             |          |\n|    actor_loss      | 766      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.000736 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 893696   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1465.2573226519662\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 10164    |\n|    fps             | 59       |\n|    time_elapsed    | 15111    |\n|    total_timesteps | 894068   |\n| train/             |          |\n|    actor_loss      | 764      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0707   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 893952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -799     |\n| time/              |          |\n|    episodes        | 10168    |\n|    fps             | 59       |\n|    time_elapsed    | 15116    |\n|    total_timesteps | 894324   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0937  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 894208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 10172    |\n|    fps             | 59       |\n|    time_elapsed    | 15120    |\n|    total_timesteps | 894580   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.018   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 894464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10176    |\n|    fps             | 59       |\n|    time_elapsed    | 15124    |\n|    total_timesteps | 894841   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.109   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 894720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1464.1519881179327\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10180    |\n|    fps             | 59       |\n|    time_elapsed    | 15129    |\n|    total_timesteps | 895097   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.154    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 894976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 10184    |\n|    fps             | 59       |\n|    time_elapsed    | 15133    |\n|    total_timesteps | 895358   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00579 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 895232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10188    |\n|    fps             | 59       |\n|    time_elapsed    | 15138    |\n|    total_timesteps | 895615   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 895488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 10192    |\n|    fps             | 59       |\n|    time_elapsed    | 15143    |\n|    total_timesteps | 895879   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.103    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 895808   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1463.2463660039361\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 10196    |\n|    fps             | 59       |\n|    time_elapsed    | 15147    |\n|    total_timesteps | 896136   |\n| train/             |          |\n|    actor_loss      | 754      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0479  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 896064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 10200    |\n|    fps             | 59       |\n|    time_elapsed    | 15151    |\n|    total_timesteps | 896390   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.162   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 896320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 10204    |\n|    fps             | 59       |\n|    time_elapsed    | 15156    |\n|    total_timesteps | 896646   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0258  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 896576   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 10208    |\n|    fps             | 59       |\n|    time_elapsed    | 15160    |\n|    total_timesteps | 896901   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0109  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 896832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1462.24184338042\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10212    |\n|    fps             | 59       |\n|    time_elapsed    | 15165    |\n|    total_timesteps | 897155   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0434  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 897088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10216    |\n|    fps             | 59       |\n|    time_elapsed    | 15169    |\n|    total_timesteps | 897414   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.132    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 897344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10220    |\n|    fps             | 59       |\n|    time_elapsed    | 15173    |\n|    total_timesteps | 897670   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0561  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 897600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10224    |\n|    fps             | 59       |\n|    time_elapsed    | 15178    |\n|    total_timesteps | 897929   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 897856   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1461.1602159859185\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 10228    |\n|    fps             | 59       |\n|    time_elapsed    | 15182    |\n|    total_timesteps | 898185   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0561   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 898112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 10232    |\n|    fps             | 59       |\n|    time_elapsed    | 15186    |\n|    total_timesteps | 898450   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0238  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 898368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10236    |\n|    fps             | 59       |\n|    time_elapsed    | 15190    |\n|    total_timesteps | 898705   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0346  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 898624   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -804     |\n| time/              |          |\n|    episodes        | 10240    |\n|    fps             | 59       |\n|    time_elapsed    | 15195    |\n|    total_timesteps | 898960   |\n| train/             |          |\n|    actor_loss      | 758      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.174   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 898880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1460.2512460184116\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 10244    |\n|    fps             | 59       |\n|    time_elapsed    | 15199    |\n|    total_timesteps | 899216   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.104    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 899136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 10248    |\n|    fps             | 59       |\n|    time_elapsed    | 15203    |\n|    total_timesteps | 899472   |\n| train/             |          |\n|    actor_loss      | 761      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.171   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 899392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 10252    |\n|    fps             | 59       |\n|    time_elapsed    | 15208    |\n|    total_timesteps | 899728   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 899648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -805     |\n| time/              |          |\n|    episodes        | 10256    |\n|    fps             | 59       |\n|    time_elapsed    | 15212    |\n|    total_timesteps | 899984   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.078   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 899904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1459.1678874321792\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 10260    |\n|    fps             | 59       |\n|    time_elapsed    | 15216    |\n|    total_timesteps | 900240   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.164   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 900160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 10264    |\n|    fps             | 59       |\n|    time_elapsed    | 15220    |\n|    total_timesteps | 900495   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0442  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 900416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10268    |\n|    fps             | 59       |\n|    time_elapsed    | 15225    |\n|    total_timesteps | 900750   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.127   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 900672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1458.133194061398\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10272    |\n|    fps             | 59       |\n|    time_elapsed    | 15229    |\n|    total_timesteps | 901006   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0902   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 900928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 10276    |\n|    fps             | 59       |\n|    time_elapsed    | 15233    |\n|    total_timesteps | 901262   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.071   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 901184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 10280    |\n|    fps             | 59       |\n|    time_elapsed    | 15238    |\n|    total_timesteps | 901531   |\n| train/             |          |\n|    actor_loss      | 770      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.254    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 901440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 10284    |\n|    fps             | 59       |\n|    time_elapsed    | 15242    |\n|    total_timesteps | 901793   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0541   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 901696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1457.0514854363387\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10288    |\n|    fps             | 59       |\n|    time_elapsed    | 15246    |\n|    total_timesteps | 902050   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0447  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 901952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 10292    |\n|    fps             | 59       |\n|    time_elapsed    | 15250    |\n|    total_timesteps | 902315   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0363  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 902208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 10296    |\n|    fps             | 59       |\n|    time_elapsed    | 15255    |\n|    total_timesteps | 902571   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0935   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 902464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10300    |\n|    fps             | 59       |\n|    time_elapsed    | 15259    |\n|    total_timesteps | 902828   |\n| train/             |          |\n|    actor_loss      | 746      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 902720   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1456.0962709909975\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10304    |\n|    fps             | 59       |\n|    time_elapsed    | 15263    |\n|    total_timesteps | 903083   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.179    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 902976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10308    |\n|    fps             | 59       |\n|    time_elapsed    | 15268    |\n|    total_timesteps | 903339   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0372  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 903232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 10312    |\n|    fps             | 59       |\n|    time_elapsed    | 15272    |\n|    total_timesteps | 903602   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0515   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 903488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 10316    |\n|    fps             | 59       |\n|    time_elapsed    | 15276    |\n|    total_timesteps | 903857   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00839 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 903744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1455.046626716169\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 10320    |\n|    fps             | 59       |\n|    time_elapsed    | 15280    |\n|    total_timesteps | 904112   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0494  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 904000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10324    |\n|    fps             | 59       |\n|    time_elapsed    | 15285    |\n|    total_timesteps | 904370   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0499   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 904256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 10328    |\n|    fps             | 59       |\n|    time_elapsed    | 15289    |\n|    total_timesteps | 904626   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.146   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 904512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -782     |\n| time/              |          |\n|    episodes        | 10332    |\n|    fps             | 59       |\n|    time_elapsed    | 15293    |\n|    total_timesteps | 904881   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.00362  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 904768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1454.0509624931024\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -778     |\n| time/              |          |\n|    episodes        | 10336    |\n|    fps             | 59       |\n|    time_elapsed    | 15298    |\n|    total_timesteps | 905137   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0201  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 905024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10340    |\n|    fps             | 59       |\n|    time_elapsed    | 15302    |\n|    total_timesteps | 905392   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0767   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 905280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 10344    |\n|    fps             | 59       |\n|    time_elapsed    | 15306    |\n|    total_timesteps | 905648   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0205   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 905536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 10348    |\n|    fps             | 59       |\n|    time_elapsed    | 15310    |\n|    total_timesteps | 905906   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0145   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 905792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1452.9083056459885\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 10352    |\n|    fps             | 59       |\n|    time_elapsed    | 15315    |\n|    total_timesteps | 906171   |\n| train/             |          |\n|    actor_loss      | 763      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.126    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 906048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10356    |\n|    fps             | 59       |\n|    time_elapsed    | 15319    |\n|    total_timesteps | 906428   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0584   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 906304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 10360    |\n|    fps             | 59       |\n|    time_elapsed    | 15323    |\n|    total_timesteps | 906684   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.097   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 906560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10364    |\n|    fps             | 59       |\n|    time_elapsed    | 15328    |\n|    total_timesteps | 906940   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0822   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 906816   |\n---------------------------------\nNew best mean reward across all envs: -1451.9784469319945\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 10368    |\n|    fps             | 59       |\n|    time_elapsed    | 15332    |\n|    total_timesteps | 907195   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0992  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 907072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10372    |\n|    fps             | 59       |\n|    time_elapsed    | 15336    |\n|    total_timesteps | 907451   |\n| train/             |          |\n|    actor_loss      | 750      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.142    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 907328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -782     |\n| time/              |          |\n|    episodes        | 10376    |\n|    fps             | 59       |\n|    time_elapsed    | 15340    |\n|    total_timesteps | 907707   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0738  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 907584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 10380    |\n|    fps             | 59       |\n|    time_elapsed    | 15345    |\n|    total_timesteps | 907963   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0967   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 907840   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1450.9296329098045\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -767     |\n| time/              |          |\n|    episodes        | 10384    |\n|    fps             | 59       |\n|    time_elapsed    | 15349    |\n|    total_timesteps | 908220   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0698  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 908096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -773     |\n| time/              |          |\n|    episodes        | 10388    |\n|    fps             | 59       |\n|    time_elapsed    | 15353    |\n|    total_timesteps | 908476   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0529   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 908352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 10392    |\n|    fps             | 59       |\n|    time_elapsed    | 15358    |\n|    total_timesteps | 908739   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 908672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10396    |\n|    fps             | 59       |\n|    time_elapsed    | 15363    |\n|    total_timesteps | 908994   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0408   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 908928   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1449.865278547168\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10400    |\n|    fps             | 59       |\n|    time_elapsed    | 15367    |\n|    total_timesteps | 909250   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0428  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 909184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10404    |\n|    fps             | 59       |\n|    time_elapsed    | 15371    |\n|    total_timesteps | 909507   |\n| train/             |          |\n|    actor_loss      | 757      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.196   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 909440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 10408    |\n|    fps             | 59       |\n|    time_elapsed    | 15375    |\n|    total_timesteps | 909762   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0747   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 909696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1448.895910224198\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 10412    |\n|    fps             | 59       |\n|    time_elapsed    | 15380    |\n|    total_timesteps | 910021   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.202    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 909952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10416    |\n|    fps             | 59       |\n|    time_elapsed    | 15384    |\n|    total_timesteps | 910284   |\n| train/             |          |\n|    actor_loss      | 754      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0165   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 910208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 10420    |\n|    fps             | 59       |\n|    time_elapsed    | 15388    |\n|    total_timesteps | 910546   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0235  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 910464   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10424    |\n|    fps             | 59       |\n|    time_elapsed    | 15393    |\n|    total_timesteps | 910810   |\n| train/             |          |\n|    actor_loss      | 745      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0534  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 910720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1448.0788591759117\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10428    |\n|    fps             | 59       |\n|    time_elapsed    | 15397    |\n|    total_timesteps | 911066   |\n| train/             |          |\n|    actor_loss      | 759      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0988  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 910976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10432    |\n|    fps             | 59       |\n|    time_elapsed    | 15401    |\n|    total_timesteps | 911322   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.143    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 911232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10436    |\n|    fps             | 59       |\n|    time_elapsed    | 15406    |\n|    total_timesteps | 911575   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.107   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 911488   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10440    |\n|    fps             | 59       |\n|    time_elapsed    | 15410    |\n|    total_timesteps | 911831   |\n| train/             |          |\n|    actor_loss      | 750      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.131   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 911744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1446.93194327026\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10444    |\n|    fps             | 59       |\n|    time_elapsed    | 15414    |\n|    total_timesteps | 912086   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0126   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 912000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -785     |\n| time/              |          |\n|    episodes        | 10448    |\n|    fps             | 59       |\n|    time_elapsed    | 15418    |\n|    total_timesteps | 912342   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.1     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 912256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10452    |\n|    fps             | 59       |\n|    time_elapsed    | 15423    |\n|    total_timesteps | 912598   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 912512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10456    |\n|    fps             | 59       |\n|    time_elapsed    | 15427    |\n|    total_timesteps | 912854   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0125  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 912768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1445.88722293984\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10460    |\n|    fps             | 59       |\n|    time_elapsed    | 15431    |\n|    total_timesteps | 913110   |\n| train/             |          |\n|    actor_loss      | 750      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0688   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 913024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10464    |\n|    fps             | 59       |\n|    time_elapsed    | 15436    |\n|    total_timesteps | 913366   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.00557  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 913280   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 10468    |\n|    fps             | 59       |\n|    time_elapsed    | 15440    |\n|    total_timesteps | 913622   |\n| train/             |          |\n|    actor_loss      | 762      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0958   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 913536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -758     |\n| time/              |          |\n|    episodes        | 10472    |\n|    fps             | 59       |\n|    time_elapsed    | 15444    |\n|    total_timesteps | 913878   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.17     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 913792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1444.812627673965\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 10476    |\n|    fps             | 59       |\n|    time_elapsed    | 15449    |\n|    total_timesteps | 914134   |\n| train/             |          |\n|    actor_loss      | 754      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.127   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 914048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -767     |\n| time/              |          |\n|    episodes        | 10480    |\n|    fps             | 59       |\n|    time_elapsed    | 15453    |\n|    total_timesteps | 914395   |\n| train/             |          |\n|    actor_loss      | 754      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0231   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 914304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10484    |\n|    fps             | 59       |\n|    time_elapsed    | 15457    |\n|    total_timesteps | 914652   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0228   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 914560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10488    |\n|    fps             | 59       |\n|    time_elapsed    | 15462    |\n|    total_timesteps | 914915   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.121   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 914816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1443.8767704830432\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 10492    |\n|    fps             | 59       |\n|    time_elapsed    | 15466    |\n|    total_timesteps | 915171   |\n| train/             |          |\n|    actor_loss      | 760      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0522   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 915072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 10496    |\n|    fps             | 59       |\n|    time_elapsed    | 15470    |\n|    total_timesteps | 915427   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0957   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 915328   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 10500    |\n|    fps             | 59       |\n|    time_elapsed    | 15475    |\n|    total_timesteps | 915691   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0466   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 915584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10504    |\n|    fps             | 59       |\n|    time_elapsed    | 15479    |\n|    total_timesteps | 915947   |\n| train/             |          |\n|    actor_loss      | 747      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00219  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 915840   |\n---------------------------------\nNew best mean reward across all envs: -1442.884457090199\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10508    |\n|    fps             | 59       |\n|    time_elapsed    | 15483    |\n|    total_timesteps | 916210   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0423  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 916096   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -762     |\n| time/              |          |\n|    episodes        | 10512    |\n|    fps             | 59       |\n|    time_elapsed    | 15487    |\n|    total_timesteps | 916469   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.105   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 916352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 10516    |\n|    fps             | 59       |\n|    time_elapsed    | 15492    |\n|    total_timesteps | 916736   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.00827  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 916608   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10520    |\n|    fps             | 59       |\n|    time_elapsed    | 15497    |\n|    total_timesteps | 916993   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.226   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 916928   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1441.870902510017\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10524    |\n|    fps             | 59       |\n|    time_elapsed    | 15501    |\n|    total_timesteps | 917250   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00165 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 917184   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10528    |\n|    fps             | 59       |\n|    time_elapsed    | 15505    |\n|    total_timesteps | 917506   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.032   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 917440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10532    |\n|    fps             | 59       |\n|    time_elapsed    | 15510    |\n|    total_timesteps | 917762   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.139   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 917696   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1440.9342671820064\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 10536    |\n|    fps             | 59       |\n|    time_elapsed    | 15514    |\n|    total_timesteps | 918018   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0207  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 917952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 10540    |\n|    fps             | 59       |\n|    time_elapsed    | 15518    |\n|    total_timesteps | 918274   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 918208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 10544    |\n|    fps             | 59       |\n|    time_elapsed    | 15523    |\n|    total_timesteps | 918530   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0252  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 918464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 10548    |\n|    fps             | 59       |\n|    time_elapsed    | 15527    |\n|    total_timesteps | 918792   |\n| train/             |          |\n|    actor_loss      | 751      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00484  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 918720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1439.9839999484059\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -777     |\n| time/              |          |\n|    episodes        | 10552    |\n|    fps             | 59       |\n|    time_elapsed    | 15531    |\n|    total_timesteps | 919050   |\n| train/             |          |\n|    actor_loss      | 752      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.1      |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 918976   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -773     |\n| time/              |          |\n|    episodes        | 10556    |\n|    fps             | 59       |\n|    time_elapsed    | 15536    |\n|    total_timesteps | 919306   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.444    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 919232   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10560    |\n|    fps             | 59       |\n|    time_elapsed    | 15540    |\n|    total_timesteps | 919563   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.092   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 919488   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -782     |\n| time/              |          |\n|    episodes        | 10564    |\n|    fps             | 59       |\n|    time_elapsed    | 15545    |\n|    total_timesteps | 919827   |\n| train/             |          |\n|    actor_loss      | 755      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0254   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 919744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1439.01230013312\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10568    |\n|    fps             | 59       |\n|    time_elapsed    | 15549    |\n|    total_timesteps | 920084   |\n| train/             |          |\n|    actor_loss      | 750      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0827  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 920000   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 10572    |\n|    fps             | 59       |\n|    time_elapsed    | 15553    |\n|    total_timesteps | 920341   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0681  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 920256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10576    |\n|    fps             | 59       |\n|    time_elapsed    | 15558    |\n|    total_timesteps | 920597   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0338   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 920512   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 10580    |\n|    fps             | 59       |\n|    time_elapsed    | 15562    |\n|    total_timesteps | 920861   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.218   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 920768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1438.1259650269553\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -791     |\n| time/              |          |\n|    episodes        | 10584    |\n|    fps             | 59       |\n|    time_elapsed    | 15566    |\n|    total_timesteps | 921120   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0395   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 921024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10588    |\n|    fps             | 59       |\n|    time_elapsed    | 15571    |\n|    total_timesteps | 921376   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.117   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 921280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -778     |\n| time/              |          |\n|    episodes        | 10592    |\n|    fps             | 59       |\n|    time_elapsed    | 15575    |\n|    total_timesteps | 921632   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.03     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 921536   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -777     |\n| time/              |          |\n|    episodes        | 10596    |\n|    fps             | 59       |\n|    time_elapsed    | 15579    |\n|    total_timesteps | 921894   |\n| train/             |          |\n|    actor_loss      | 748      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 921792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1437.0418282555866\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10600    |\n|    fps             | 59       |\n|    time_elapsed    | 15584    |\n|    total_timesteps | 922155   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 922048   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -773     |\n| time/              |          |\n|    episodes        | 10604    |\n|    fps             | 59       |\n|    time_elapsed    | 15588    |\n|    total_timesteps | 922419   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0222   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 922304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -783     |\n| time/              |          |\n|    episodes        | 10608    |\n|    fps             | 59       |\n|    time_elapsed    | 15593    |\n|    total_timesteps | 922681   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.139   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 922560   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -785     |\n| time/              |          |\n|    episodes        | 10612    |\n|    fps             | 59       |\n|    time_elapsed    | 15597    |\n|    total_timesteps | 922940   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0396   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 922816   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1436.1379081005778\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10616    |\n|    fps             | 59       |\n|    time_elapsed    | 15603    |\n|    total_timesteps | 923208   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.072    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 923136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10620    |\n|    fps             | 59       |\n|    time_elapsed    | 15607    |\n|    total_timesteps | 923463   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 923392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -776     |\n| time/              |          |\n|    episodes        | 10624    |\n|    fps             | 59       |\n|    time_elapsed    | 15612    |\n|    total_timesteps | 923727   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0081   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 923648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10628    |\n|    fps             | 59       |\n|    time_elapsed    | 15616    |\n|    total_timesteps | 923983   |\n| train/             |          |\n|    actor_loss      | 756      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0279   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 923904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1435.196488546901\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -777     |\n| time/              |          |\n|    episodes        | 10632    |\n|    fps             | 59       |\n|    time_elapsed    | 15620    |\n|    total_timesteps | 924241   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0856  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 924160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10636    |\n|    fps             | 59       |\n|    time_elapsed    | 15625    |\n|    total_timesteps | 924501   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 924416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 10640    |\n|    fps             | 59       |\n|    time_elapsed    | 15629    |\n|    total_timesteps | 924757   |\n| train/             |          |\n|    actor_loss      | 746      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.175    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 924672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1434.2210917498628\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 10644    |\n|    fps             | 59       |\n|    time_elapsed    | 15634    |\n|    total_timesteps | 925017   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0878  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 924928   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10648    |\n|    fps             | 59       |\n|    time_elapsed    | 15638    |\n|    total_timesteps | 925281   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0735   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 925184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10652    |\n|    fps             | 59       |\n|    time_elapsed    | 15642    |\n|    total_timesteps | 925537   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0871   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 925440   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10656    |\n|    fps             | 59       |\n|    time_elapsed    | 15647    |\n|    total_timesteps | 925794   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0834  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 925696   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1433.3341455592533\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 10660    |\n|    fps             | 59       |\n|    time_elapsed    | 15651    |\n|    total_timesteps | 926050   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0415  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 925952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -778     |\n| time/              |          |\n|    episodes        | 10664    |\n|    fps             | 59       |\n|    time_elapsed    | 15655    |\n|    total_timesteps | 926308   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0778  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 926208   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10668    |\n|    fps             | 59       |\n|    time_elapsed    | 15660    |\n|    total_timesteps | 926564   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0778   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 926464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 10672    |\n|    fps             | 59       |\n|    time_elapsed    | 15664    |\n|    total_timesteps | 926820   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 926720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1432.2533910247623\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10676    |\n|    fps             | 59       |\n|    time_elapsed    | 15669    |\n|    total_timesteps | 927081   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 926976   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10680    |\n|    fps             | 59       |\n|    time_elapsed    | 15673    |\n|    total_timesteps | 927343   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.22    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 927232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 10684    |\n|    fps             | 59       |\n|    time_elapsed    | 15677    |\n|    total_timesteps | 927598   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0549   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 927488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 10688    |\n|    fps             | 59       |\n|    time_elapsed    | 15682    |\n|    total_timesteps | 927854   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0311   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 927744   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1431.3040369958114\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10692    |\n|    fps             | 59       |\n|    time_elapsed    | 15686    |\n|    total_timesteps | 928110   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0656  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 928000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 10696    |\n|    fps             | 59       |\n|    time_elapsed    | 15691    |\n|    total_timesteps | 928381   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0565  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 928256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -782     |\n| time/              |          |\n|    episodes        | 10700    |\n|    fps             | 59       |\n|    time_elapsed    | 15695    |\n|    total_timesteps | 928637   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0208  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 928512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10704    |\n|    fps             | 59       |\n|    time_elapsed    | 15700    |\n|    total_timesteps | 928911   |\n| train/             |          |\n|    actor_loss      | 753      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.125   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 928832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1430.424953669067\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10708    |\n|    fps             | 59       |\n|    time_elapsed    | 15705    |\n|    total_timesteps | 929167   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 929088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -788     |\n| time/              |          |\n|    episodes        | 10712    |\n|    fps             | 59       |\n|    time_elapsed    | 15709    |\n|    total_timesteps | 929427   |\n| train/             |          |\n|    actor_loss      | 745      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0508   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 929344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -784     |\n| time/              |          |\n|    episodes        | 10716    |\n|    fps             | 59       |\n|    time_elapsed    | 15713    |\n|    total_timesteps | 929683   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.00491 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 929600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10720    |\n|    fps             | 59       |\n|    time_elapsed    | 15717    |\n|    total_timesteps | 929941   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0804  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 929856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1429.639828222328\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 10724    |\n|    fps             | 59       |\n|    time_elapsed    | 15722    |\n|    total_timesteps | 930204   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.118   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 930112   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 10728    |\n|    fps             | 59       |\n|    time_elapsed    | 15726    |\n|    total_timesteps | 930460   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0218  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 930368   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 10732    |\n|    fps             | 59       |\n|    time_elapsed    | 15730    |\n|    total_timesteps | 930716   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.11    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 930624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 10736    |\n|    fps             | 59       |\n|    time_elapsed    | 15734    |\n|    total_timesteps | 930971   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0755  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 930880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1428.784413850237\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -808     |\n| time/              |          |\n|    episodes        | 10740    |\n|    fps             | 59       |\n|    time_elapsed    | 15739    |\n|    total_timesteps | 931242   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0116  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 931136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 10744    |\n|    fps             | 59       |\n|    time_elapsed    | 15743    |\n|    total_timesteps | 931498   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0764   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 931392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -809     |\n| time/              |          |\n|    episodes        | 10748    |\n|    fps             | 59       |\n|    time_elapsed    | 15747    |\n|    total_timesteps | 931764   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0761   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 931648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1427.930955097768\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 10752    |\n|    fps             | 59       |\n|    time_elapsed    | 15752    |\n|    total_timesteps | 932020   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.136    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 931904   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -809     |\n| time/              |          |\n|    episodes        | 10756    |\n|    fps             | 59       |\n|    time_elapsed    | 15756    |\n|    total_timesteps | 932276   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.00607  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 932160   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -804     |\n| time/              |          |\n|    episodes        | 10760    |\n|    fps             | 59       |\n|    time_elapsed    | 15760    |\n|    total_timesteps | 932534   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0328  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 932416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 10764    |\n|    fps             | 59       |\n|    time_elapsed    | 15765    |\n|    total_timesteps | 932808   |\n| train/             |          |\n|    actor_loss      | 754      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.207    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 932736   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1427.0852443940205\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 10768    |\n|    fps             | 59       |\n|    time_elapsed    | 15770    |\n|    total_timesteps | 933081   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0527   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 932992   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 10772    |\n|    fps             | 59       |\n|    time_elapsed    | 15774    |\n|    total_timesteps | 933338   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0097   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 933248   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -810     |\n| time/              |          |\n|    episodes        | 10776    |\n|    fps             | 59       |\n|    time_elapsed    | 15778    |\n|    total_timesteps | 933594   |\n| train/             |          |\n|    actor_loss      | 745      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0229  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 933504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -801     |\n| time/              |          |\n|    episodes        | 10780    |\n|    fps             | 59       |\n|    time_elapsed    | 15783    |\n|    total_timesteps | 933850   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.049    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 933760   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1425.9761978670836\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -805     |\n| time/              |          |\n|    episodes        | 10784    |\n|    fps             | 59       |\n|    time_elapsed    | 15787    |\n|    total_timesteps | 934106   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0817   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 934016   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 10788    |\n|    fps             | 59       |\n|    time_elapsed    | 15791    |\n|    total_timesteps | 934376   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.13    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 934272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 10792    |\n|    fps             | 59       |\n|    time_elapsed    | 15796    |\n|    total_timesteps | 934639   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00771 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 934528   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -819     |\n| time/              |          |\n|    episodes        | 10796    |\n|    fps             | 59       |\n|    time_elapsed    | 15800    |\n|    total_timesteps | 934902   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.157    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 934784   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1425.1555705887597\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -816     |\n| time/              |          |\n|    episodes        | 10800    |\n|    fps             | 59       |\n|    time_elapsed    | 15805    |\n|    total_timesteps | 935171   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0717  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 935104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 10804    |\n|    fps             | 59       |\n|    time_elapsed    | 15809    |\n|    total_timesteps | 935435   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00307 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 935360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -807     |\n| time/              |          |\n|    episodes        | 10808    |\n|    fps             | 59       |\n|    time_elapsed    | 15814    |\n|    total_timesteps | 935695   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0344   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 935616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 10812    |\n|    fps             | 59       |\n|    time_elapsed    | 15818    |\n|    total_timesteps | 935954   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.143   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 935872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1424.2776476310107\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.3     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10816    |\n|    fps             | 59       |\n|    time_elapsed    | 15822    |\n|    total_timesteps | 936209   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0897  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 936128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 10820    |\n|    fps             | 59       |\n|    time_elapsed    | 15827    |\n|    total_timesteps | 936465   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.188   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 936384   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -804     |\n| time/              |          |\n|    episodes        | 10824    |\n|    fps             | 59       |\n|    time_elapsed    | 15831    |\n|    total_timesteps | 936728   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0345  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 936640   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -800     |\n| time/              |          |\n|    episodes        | 10828    |\n|    fps             | 59       |\n|    time_elapsed    | 15835    |\n|    total_timesteps | 936984   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.0505  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 936896   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1423.3906920077125\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 10832    |\n|    fps             | 59       |\n|    time_elapsed    | 15840    |\n|    total_timesteps | 937240   |\n| train/             |          |\n|    actor_loss      | 721      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 937152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10836    |\n|    fps             | 59       |\n|    time_elapsed    | 15844    |\n|    total_timesteps | 937496   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0694   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 937408   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 10840    |\n|    fps             | 59       |\n|    time_elapsed    | 15848    |\n|    total_timesteps | 937757   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.091   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 937664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1422.6002297572782\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -799     |\n| time/              |          |\n|    episodes        | 10844    |\n|    fps             | 59       |\n|    time_elapsed    | 15853    |\n|    total_timesteps | 938013   |\n| train/             |          |\n|    actor_loss      | 728      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0461  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 937920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 10848    |\n|    fps             | 59       |\n|    time_elapsed    | 15857    |\n|    total_timesteps | 938268   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.174    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 938176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10852    |\n|    fps             | 59       |\n|    time_elapsed    | 15861    |\n|    total_timesteps | 938524   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0526  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 938432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 10856    |\n|    fps             | 59       |\n|    time_elapsed    | 15865    |\n|    total_timesteps | 938792   |\n| train/             |          |\n|    actor_loss      | 741      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0257   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 938688   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1421.6868990487483\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 10860    |\n|    fps             | 59       |\n|    time_elapsed    | 15870    |\n|    total_timesteps | 939048   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0751  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 938944   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 10864    |\n|    fps             | 59       |\n|    time_elapsed    | 15874    |\n|    total_timesteps | 939304   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 939200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -785     |\n| time/              |          |\n|    episodes        | 10868    |\n|    fps             | 59       |\n|    time_elapsed    | 15878    |\n|    total_timesteps | 939567   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.137    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 939456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -801     |\n| time/              |          |\n|    episodes        | 10872    |\n|    fps             | 59       |\n|    time_elapsed    | 15883    |\n|    total_timesteps | 939835   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0184   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 939712   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1420.7294678613923\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -806     |\n| time/              |          |\n|    episodes        | 10876    |\n|    fps             | 59       |\n|    time_elapsed    | 15888    |\n|    total_timesteps | 940106   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.101   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 940032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.2     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 10880    |\n|    fps             | 59       |\n|    time_elapsed    | 15892    |\n|    total_timesteps | 940366   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.156   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 940288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65.1     |\n|    ep_rew_mean     | -802     |\n| time/              |          |\n|    episodes        | 10884    |\n|    fps             | 59       |\n|    time_elapsed    | 15896    |\n|    total_timesteps | 940620   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0147   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 940544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 65       |\n|    ep_rew_mean     | -794     |\n| time/              |          |\n|    episodes        | 10888    |\n|    fps             | 59       |\n|    time_elapsed    | 15901    |\n|    total_timesteps | 940876   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.161    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 940800   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1419.7839771408237\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 10892    |\n|    fps             | 59       |\n|    time_elapsed    | 15905    |\n|    total_timesteps | 941131   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.024   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 941056   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.9     |\n|    ep_rew_mean     | -784     |\n| time/              |          |\n|    episodes        | 10896    |\n|    fps             | 59       |\n|    time_elapsed    | 15909    |\n|    total_timesteps | 941389   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 941312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -780     |\n| time/              |          |\n|    episodes        | 10900    |\n|    fps             | 59       |\n|    time_elapsed    | 15913    |\n|    total_timesteps | 941645   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0648  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 941568   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -777     |\n| time/              |          |\n|    episodes        | 10904    |\n|    fps             | 59       |\n|    time_elapsed    | 15918    |\n|    total_timesteps | 941901   |\n| train/             |          |\n|    actor_loss      | 742      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.127    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 941824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1418.7510304169582\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10908    |\n|    fps             | 59       |\n|    time_elapsed    | 15922    |\n|    total_timesteps | 942165   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.129    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 942080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10912    |\n|    fps             | 59       |\n|    time_elapsed    | 15926    |\n|    total_timesteps | 942421   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0447  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 942336   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 10916    |\n|    fps             | 59       |\n|    time_elapsed    | 15931    |\n|    total_timesteps | 942676   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0777   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 942592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -773     |\n| time/              |          |\n|    episodes        | 10920    |\n|    fps             | 59       |\n|    time_elapsed    | 15935    |\n|    total_timesteps | 942935   |\n| train/             |          |\n|    actor_loss      | 744      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.315    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 942848   |\n---------------------------------\nNew best mean reward across all envs: -1417.881946449864\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10924    |\n|    fps             | 59       |\n|    time_elapsed    | 15939    |\n|    total_timesteps | 943199   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.129   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 943104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 10928    |\n|    fps             | 59       |\n|    time_elapsed    | 15944    |\n|    total_timesteps | 943454   |\n| train/             |          |\n|    actor_loss      | 747      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.00362 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 943360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 10932    |\n|    fps             | 59       |\n|    time_elapsed    | 15948    |\n|    total_timesteps | 943709   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.114   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 943616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10936    |\n|    fps             | 59       |\n|    time_elapsed    | 15952    |\n|    total_timesteps | 943977   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.00385  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 943872   |\n---------------------------------\nNew best mean reward across all envs: -1416.9609179519332\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -767     |\n| time/              |          |\n|    episodes        | 10940    |\n|    fps             | 59       |\n|    time_elapsed    | 15957    |\n|    total_timesteps | 944232   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0369   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 944128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -762     |\n| time/              |          |\n|    episodes        | 10944    |\n|    fps             | 59       |\n|    time_elapsed    | 15961    |\n|    total_timesteps | 944488   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0882   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 944384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 10948    |\n|    fps             | 59       |\n|    time_elapsed    | 15965    |\n|    total_timesteps | 944744   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.00455 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 944640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1416.1036474961452\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 10952    |\n|    fps             | 59       |\n|    time_elapsed    | 15969    |\n|    total_timesteps | 945000   |\n| train/             |          |\n|    actor_loss      | 745      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.125    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 944896   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 10956    |\n|    fps             | 59       |\n|    time_elapsed    | 15974    |\n|    total_timesteps | 945256   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.106   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 945152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 10960    |\n|    fps             | 59       |\n|    time_elapsed    | 15978    |\n|    total_timesteps | 945512   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0226  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 945408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 10964    |\n|    fps             | 59       |\n|    time_elapsed    | 15982    |\n|    total_timesteps | 945781   |\n| train/             |          |\n|    actor_loss      | 722      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.103   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 945664   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1415.1750874577683\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 10968    |\n|    fps             | 59       |\n|    time_elapsed    | 15987    |\n|    total_timesteps | 946037   |\n| train/             |          |\n|    actor_loss      | 724      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0613  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 945920   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -758     |\n| time/              |          |\n|    episodes        | 10972    |\n|    fps             | 59       |\n|    time_elapsed    | 15991    |\n|    total_timesteps | 946293   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0354  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 946176   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 10976    |\n|    fps             | 59       |\n|    time_elapsed    | 15995    |\n|    total_timesteps | 946552   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.158    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 946432   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 10980    |\n|    fps             | 59       |\n|    time_elapsed    | 15999    |\n|    total_timesteps | 946812   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0265  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 946688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1414.264247446492\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 10984    |\n|    fps             | 59       |\n|    time_elapsed    | 16005    |\n|    total_timesteps | 947079   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0393  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 947008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -760     |\n| time/              |          |\n|    episodes        | 10988    |\n|    fps             | 59       |\n|    time_elapsed    | 16009    |\n|    total_timesteps | 947339   |\n| train/             |          |\n|    actor_loss      | 724      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0963  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 947264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 10992    |\n|    fps             | 59       |\n|    time_elapsed    | 16013    |\n|    total_timesteps | 947601   |\n| train/             |          |\n|    actor_loss      | 746      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.12    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 947520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 10996    |\n|    fps             | 59       |\n|    time_elapsed    | 16018    |\n|    total_timesteps | 947857   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0399  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 947776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1413.3526233512068\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 11000    |\n|    fps             | 59       |\n|    time_elapsed    | 16022    |\n|    total_timesteps | 948126   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0341   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 948032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.8     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 11004    |\n|    fps             | 59       |\n|    time_elapsed    | 16026    |\n|    total_timesteps | 948382   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 948288   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 11008    |\n|    fps             | 59       |\n|    time_elapsed    | 16030    |\n|    total_timesteps | 948638   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.195   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 948544   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 11012    |\n|    fps             | 59       |\n|    time_elapsed    | 16035    |\n|    total_timesteps | 948894   |\n| train/             |          |\n|    actor_loss      | 723      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0194   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 948800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1412.4670038139898\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 11016    |\n|    fps             | 59       |\n|    time_elapsed    | 16039    |\n|    total_timesteps | 949150   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0268   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 949056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 11020    |\n|    fps             | 59       |\n|    time_elapsed    | 16043    |\n|    total_timesteps | 949406   |\n| train/             |          |\n|    actor_loss      | 728      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.185    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 949312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -758     |\n| time/              |          |\n|    episodes        | 11024    |\n|    fps             | 59       |\n|    time_elapsed    | 16048    |\n|    total_timesteps | 949662   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.124   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 949568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 11028    |\n|    fps             | 59       |\n|    time_elapsed    | 16052    |\n|    total_timesteps | 949926   |\n| train/             |          |\n|    actor_loss      | 728      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.174    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 949824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1411.4494299658984\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 11032    |\n|    fps             | 59       |\n|    time_elapsed    | 16056    |\n|    total_timesteps | 950182   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.138    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 950080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 11036    |\n|    fps             | 59       |\n|    time_elapsed    | 16060    |\n|    total_timesteps | 950438   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.000974 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 950336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11040    |\n|    fps             | 59       |\n|    time_elapsed    | 16065    |\n|    total_timesteps | 950694   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.17     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 950592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -754     |\n| time/              |          |\n|    episodes        | 11044    |\n|    fps             | 59       |\n|    time_elapsed    | 16069    |\n|    total_timesteps | 950950   |\n| train/             |          |\n|    actor_loss      | 738      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0522   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 950848   |\n---------------------------------\nNew best mean reward across all envs: -1410.5148954829915\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -745     |\n| time/              |          |\n|    episodes        | 11048    |\n|    fps             | 59       |\n|    time_elapsed    | 16074    |\n|    total_timesteps | 951205   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.128    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 951104   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -746     |\n| time/              |          |\n|    episodes        | 11052    |\n|    fps             | 59       |\n|    time_elapsed    | 16078    |\n|    total_timesteps | 951462   |\n| train/             |          |\n|    actor_loss      | 725      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.173   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 951360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11056    |\n|    fps             | 59       |\n|    time_elapsed    | 16082    |\n|    total_timesteps | 951718   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0756   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 951616   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11060    |\n|    fps             | 59       |\n|    time_elapsed    | 16086    |\n|    total_timesteps | 951974   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.152   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 951872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1409.6120183987812\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -754     |\n| time/              |          |\n|    episodes        | 11064    |\n|    fps             | 59       |\n|    time_elapsed    | 16091    |\n|    total_timesteps | 952230   |\n| train/             |          |\n|    actor_loss      | 740      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0616  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 952128   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11068    |\n|    fps             | 59       |\n|    time_elapsed    | 16095    |\n|    total_timesteps | 952486   |\n| train/             |          |\n|    actor_loss      | 743      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.148    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 952384   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11072    |\n|    fps             | 59       |\n|    time_elapsed    | 16099    |\n|    total_timesteps | 952742   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0768   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 952640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1408.691780377394\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11076    |\n|    fps             | 59       |\n|    time_elapsed    | 16104    |\n|    total_timesteps | 953001   |\n| train/             |          |\n|    actor_loss      | 749      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0894   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 952896   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11080    |\n|    fps             | 59       |\n|    time_elapsed    | 16108    |\n|    total_timesteps | 953264   |\n| train/             |          |\n|    actor_loss      | 726      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0773  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 953152   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11084    |\n|    fps             | 59       |\n|    time_elapsed    | 16112    |\n|    total_timesteps | 953520   |\n| train/             |          |\n|    actor_loss      | 735      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0589   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 953408   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11088    |\n|    fps             | 59       |\n|    time_elapsed    | 16116    |\n|    total_timesteps | 953776   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0674   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 953664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1407.7376503124756\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -751     |\n| time/              |          |\n|    episodes        | 11092    |\n|    fps             | 59       |\n|    time_elapsed    | 16121    |\n|    total_timesteps | 954032   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0505  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 953920   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11096    |\n|    fps             | 59       |\n|    time_elapsed    | 16125    |\n|    total_timesteps | 954288   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.157   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 954176   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11100    |\n|    fps             | 59       |\n|    time_elapsed    | 16129    |\n|    total_timesteps | 954553   |\n| train/             |          |\n|    actor_loss      | 726      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 954432   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -740     |\n| time/              |          |\n|    episodes        | 11104    |\n|    fps             | 59       |\n|    time_elapsed    | 16134    |\n|    total_timesteps | 954810   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.153    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 954688   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1406.7549836963876\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -741     |\n| time/              |          |\n|    episodes        | 11108    |\n|    fps             | 59       |\n|    time_elapsed    | 16138    |\n|    total_timesteps | 955066   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.223   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 954944   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -732     |\n| time/              |          |\n|    episodes        | 11112    |\n|    fps             | 59       |\n|    time_elapsed    | 16142    |\n|    total_timesteps | 955322   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.0621   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 955200   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -750     |\n| time/              |          |\n|    episodes        | 11116    |\n|    fps             | 59       |\n|    time_elapsed    | 16147    |\n|    total_timesteps | 955588   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.00443 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 955520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11120    |\n|    fps             | 59       |\n|    time_elapsed    | 16152    |\n|    total_timesteps | 955843   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0713  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 955776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1405.9124347823158\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11124    |\n|    fps             | 59       |\n|    time_elapsed    | 16156    |\n|    total_timesteps | 956100   |\n| train/             |          |\n|    actor_loss      | 725      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0406   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 956032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11128    |\n|    fps             | 59       |\n|    time_elapsed    | 16160    |\n|    total_timesteps | 956356   |\n| train/             |          |\n|    actor_loss      | 736      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 956288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11132    |\n|    fps             | 59       |\n|    time_elapsed    | 16165    |\n|    total_timesteps | 956613   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.19    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 956544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11136    |\n|    fps             | 59       |\n|    time_elapsed    | 16169    |\n|    total_timesteps | 956869   |\n| train/             |          |\n|    actor_loss      | 726      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0327   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 956800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1404.9308484300243\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11140    |\n|    fps             | 59       |\n|    time_elapsed    | 16173    |\n|    total_timesteps | 957125   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.0584  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 957056   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11144    |\n|    fps             | 59       |\n|    time_elapsed    | 16178    |\n|    total_timesteps | 957381   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0343   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 957312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11148    |\n|    fps             | 59       |\n|    time_elapsed    | 16182    |\n|    total_timesteps | 957645   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.7     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | -0.119   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 957568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -739     |\n| time/              |          |\n|    episodes        | 11152    |\n|    fps             | 59       |\n|    time_elapsed    | 16186    |\n|    total_timesteps | 957905   |\n| train/             |          |\n|    actor_loss      | 734      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0879   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 957824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1403.93678059974\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -746     |\n| time/              |          |\n|    episodes        | 11156    |\n|    fps             | 59       |\n|    time_elapsed    | 16191    |\n|    total_timesteps | 958171   |\n| train/             |          |\n|    actor_loss      | 737      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0843   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 958080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -735     |\n| time/              |          |\n|    episodes        | 11160    |\n|    fps             | 59       |\n|    time_elapsed    | 16195    |\n|    total_timesteps | 958427   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.00703 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 958336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -734     |\n| time/              |          |\n|    episodes        | 11164    |\n|    fps             | 59       |\n|    time_elapsed    | 16199    |\n|    total_timesteps | 958684   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.0295   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 958592   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -742     |\n| time/              |          |\n|    episodes        | 11168    |\n|    fps             | 59       |\n|    time_elapsed    | 16203    |\n|    total_timesteps | 958939   |\n| train/             |          |\n|    actor_loss      | 724      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0309  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 958848   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1403.129844959114\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11172    |\n|    fps             | 59       |\n|    time_elapsed    | 16208    |\n|    total_timesteps | 959200   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 959104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -737     |\n| time/              |          |\n|    episodes        | 11176    |\n|    fps             | 59       |\n|    time_elapsed    | 16212    |\n|    total_timesteps | 959456   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0494   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 959360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -743     |\n| time/              |          |\n|    episodes        | 11180    |\n|    fps             | 59       |\n|    time_elapsed    | 16216    |\n|    total_timesteps | 959718   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.1      |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 959616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -742     |\n| time/              |          |\n|    episodes        | 11184    |\n|    fps             | 59       |\n|    time_elapsed    | 16221    |\n|    total_timesteps | 959973   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.03    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 959872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1402.1983482987946\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -740     |\n| time/              |          |\n|    episodes        | 11188    |\n|    fps             | 59       |\n|    time_elapsed    | 16225    |\n|    total_timesteps | 960229   |\n| train/             |          |\n|    actor_loss      | 739      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.00737  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 960128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11192    |\n|    fps             | 59       |\n|    time_elapsed    | 16229    |\n|    total_timesteps | 960486   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.05    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 960384   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11196    |\n|    fps             | 59       |\n|    time_elapsed    | 16234    |\n|    total_timesteps | 960742   |\n| train/             |          |\n|    actor_loss      | 722      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | -0.13    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 960640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -734     |\n| time/              |          |\n|    episodes        | 11200    |\n|    fps             | 59       |\n|    time_elapsed    | 16238    |\n|    total_timesteps | 960999   |\n| train/             |          |\n|    actor_loss      | 729      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.207    |\n|    ent_coef_loss   | 0.0482   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 960896   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1401.1859714810387\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11204    |\n|    fps             | 59       |\n|    time_elapsed    | 16242    |\n|    total_timesteps | 961279   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0656   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 961152   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -756     |\n| time/              |          |\n|    episodes        | 11208    |\n|    fps             | 59       |\n|    time_elapsed    | 16247    |\n|    total_timesteps | 961536   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.126    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 961408   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 11212    |\n|    fps             | 59       |\n|    time_elapsed    | 16251    |\n|    total_timesteps | 961792   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0669   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 961664   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1400.559236237859\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -758     |\n| time/              |          |\n|    episodes        | 11216    |\n|    fps             | 59       |\n|    time_elapsed    | 16256    |\n|    total_timesteps | 962054   |\n| train/             |          |\n|    actor_loss      | 732      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0705  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 961984   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -756     |\n| time/              |          |\n|    episodes        | 11220    |\n|    fps             | 59       |\n|    time_elapsed    | 16261    |\n|    total_timesteps | 962309   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.216   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 962240   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 11224    |\n|    fps             | 59       |\n|    time_elapsed    | 16265    |\n|    total_timesteps | 962565   |\n| train/             |          |\n|    actor_loss      | 725      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.115   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 962496   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -765     |\n| time/              |          |\n|    episodes        | 11228    |\n|    fps             | 59       |\n|    time_elapsed    | 16269    |\n|    total_timesteps | 962822   |\n| train/             |          |\n|    actor_loss      | 722      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0633  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 962752   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1399.72259669697\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 11232    |\n|    fps             | 59       |\n|    time_elapsed    | 16274    |\n|    total_timesteps | 963078   |\n| train/             |          |\n|    actor_loss      | 730      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | -0.0462  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 963008   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 11236    |\n|    fps             | 59       |\n|    time_elapsed    | 16278    |\n|    total_timesteps | 963334   |\n| train/             |          |\n|    actor_loss      | 733      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 963264   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.7     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 11240    |\n|    fps             | 59       |\n|    time_elapsed    | 16282    |\n|    total_timesteps | 963590   |\n| train/             |          |\n|    actor_loss      | 723      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.00385  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 963520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 11244    |\n|    fps             | 59       |\n|    time_elapsed    | 16287    |\n|    total_timesteps | 963845   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0486   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 963776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1398.7502751557315\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.6     |\n|    ep_rew_mean     | -767     |\n| time/              |          |\n|    episodes        | 11248    |\n|    fps             | 59       |\n|    time_elapsed    | 16291    |\n|    total_timesteps | 964101   |\n| train/             |          |\n|    actor_loss      | 726      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.02     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 964032   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.5     |\n|    ep_rew_mean     | -767     |\n| time/              |          |\n|    episodes        | 11252    |\n|    fps             | 59       |\n|    time_elapsed    | 16295    |\n|    total_timesteps | 964357   |\n| train/             |          |\n|    actor_loss      | 728      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.186    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 964288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11256    |\n|    fps             | 59       |\n|    time_elapsed    | 16300    |\n|    total_timesteps | 964612   |\n| train/             |          |\n|    actor_loss      | 723      |\n|    critic_loss     | 11.6     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.00785  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 964544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11260    |\n|    fps             | 59       |\n|    time_elapsed    | 16304    |\n|    total_timesteps | 964867   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0907  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 964800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1397.6517750110115\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11264    |\n|    fps             | 59       |\n|    time_elapsed    | 16308    |\n|    total_timesteps | 965123   |\n| train/             |          |\n|    actor_loss      | 722      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 965056   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.4     |\n|    ep_rew_mean     | -746     |\n| time/              |          |\n|    episodes        | 11268    |\n|    fps             | 59       |\n|    time_elapsed    | 16313    |\n|    total_timesteps | 965378   |\n| train/             |          |\n|    actor_loss      | 731      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.161   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 965312   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -739     |\n| time/              |          |\n|    episodes        | 11272    |\n|    fps             | 59       |\n|    time_elapsed    | 16317    |\n|    total_timesteps | 965634   |\n| train/             |          |\n|    actor_loss      | 728      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0278   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 965568   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11276    |\n|    fps             | 59       |\n|    time_elapsed    | 16321    |\n|    total_timesteps | 965890   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0418   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 965824   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1396.7859736819603\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -745     |\n| time/              |          |\n|    episodes        | 11280    |\n|    fps             | 59       |\n|    time_elapsed    | 16326    |\n|    total_timesteps | 966146   |\n| train/             |          |\n|    actor_loss      | 725      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | -0.0275  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 966080   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -745     |\n| time/              |          |\n|    episodes        | 11284    |\n|    fps             | 59       |\n|    time_elapsed    | 16330    |\n|    total_timesteps | 966402   |\n| train/             |          |\n|    actor_loss      | 720      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.118    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 966336   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11288    |\n|    fps             | 59       |\n|    time_elapsed    | 16334    |\n|    total_timesteps | 966658   |\n| train/             |          |\n|    actor_loss      | 724      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.128    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 966592   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11292    |\n|    fps             | 59       |\n|    time_elapsed    | 16338    |\n|    total_timesteps | 966914   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0222  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 966848   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1395.9139495994957\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -750     |\n| time/              |          |\n|    episodes        | 11296    |\n|    fps             | 59       |\n|    time_elapsed    | 16343    |\n|    total_timesteps | 967169   |\n| train/             |          |\n|    actor_loss      | 722      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0154  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 967104   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -751     |\n| time/              |          |\n|    episodes        | 11300    |\n|    fps             | 59       |\n|    time_elapsed    | 16347    |\n|    total_timesteps | 967425   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.21     |\n|    ent_coef_loss   | 0.0197   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 967360   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -735     |\n| time/              |          |\n|    episodes        | 11304    |\n|    fps             | 59       |\n|    time_elapsed    | 16351    |\n|    total_timesteps | 967682   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0531   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 967616   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -736     |\n| time/              |          |\n|    episodes        | 11308    |\n|    fps             | 59       |\n|    time_elapsed    | 16356    |\n|    total_timesteps | 967938   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0565   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 967872   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1394.9754614145631\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -730     |\n| time/              |          |\n|    episodes        | 11312    |\n|    fps             | 59       |\n|    time_elapsed    | 16360    |\n|    total_timesteps | 968193   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0514  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 968128   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -722     |\n| time/              |          |\n|    episodes        | 11316    |\n|    fps             | 59       |\n|    time_elapsed    | 16364    |\n|    total_timesteps | 968449   |\n| train/             |          |\n|    actor_loss      | 725      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.1      |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 968384   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -732     |\n| time/              |          |\n|    episodes        | 11320    |\n|    fps             | 59       |\n|    time_elapsed    | 16369    |\n|    total_timesteps | 968705   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0505   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 968640   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -724     |\n| time/              |          |\n|    episodes        | 11324    |\n|    fps             | 59       |\n|    time_elapsed    | 16373    |\n|    total_timesteps | 968961   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.225   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 968896   |\n---------------------------------\nNew best mean reward across all envs: -1394.1129377884445\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -719     |\n| time/              |          |\n|    episodes        | 11328    |\n|    fps             | 59       |\n|    time_elapsed    | 16377    |\n|    total_timesteps | 969216   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0199  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 969088   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -711     |\n| time/              |          |\n|    episodes        | 11332    |\n|    fps             | 59       |\n|    time_elapsed    | 16381    |\n|    total_timesteps | 969472   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.161    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 969344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -712     |\n| time/              |          |\n|    episodes        | 11336    |\n|    fps             | 59       |\n|    time_elapsed    | 16385    |\n|    total_timesteps | 969728   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.113   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 969600   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 63.9     |\n|    ep_rew_mean     | -723     |\n| time/              |          |\n|    episodes        | 11340    |\n|    fps             | 59       |\n|    time_elapsed    | 16389    |\n|    total_timesteps | 969984   |\n| train/             |          |\n|    actor_loss      | 721      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.361    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 969856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1393.1419690842272\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -723     |\n| time/              |          |\n|    episodes        | 11344    |\n|    fps             | 59       |\n|    time_elapsed    | 16394    |\n|    total_timesteps | 970240   |\n| train/             |          |\n|    actor_loss      | 720      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0614   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 970112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -728     |\n| time/              |          |\n|    episodes        | 11348    |\n|    fps             | 59       |\n|    time_elapsed    | 16398    |\n|    total_timesteps | 970496   |\n| train/             |          |\n|    actor_loss      | 706      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0162  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 970368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -733     |\n| time/              |          |\n|    episodes        | 11352    |\n|    fps             | 59       |\n|    time_elapsed    | 16402    |\n|    total_timesteps | 970752   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0372   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 970624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1392.3769713452773\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11356    |\n|    fps             | 59       |\n|    time_elapsed    | 16407    |\n|    total_timesteps | 971007   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0531   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 970880   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11360    |\n|    fps             | 59       |\n|    time_elapsed    | 16412    |\n|    total_timesteps | 971271   |\n| train/             |          |\n|    actor_loss      | 721      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0618   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 971200   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -756     |\n| time/              |          |\n|    episodes        | 11364    |\n|    fps             | 59       |\n|    time_elapsed    | 16416    |\n|    total_timesteps | 971527   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.221   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 971456   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11368    |\n|    fps             | 59       |\n|    time_elapsed    | 16420    |\n|    total_timesteps | 971783   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.105    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 971712   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1391.5221805264787\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 11372    |\n|    fps             | 59       |\n|    time_elapsed    | 16425    |\n|    total_timesteps | 972040   |\n| train/             |          |\n|    actor_loss      | 704      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.125   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 971968   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11376    |\n|    fps             | 59       |\n|    time_elapsed    | 16429    |\n|    total_timesteps | 972296   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.108    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 972224   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -758     |\n| time/              |          |\n|    episodes        | 11380    |\n|    fps             | 59       |\n|    time_elapsed    | 16433    |\n|    total_timesteps | 972554   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0331  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 972480   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11384    |\n|    fps             | 59       |\n|    time_elapsed    | 16437    |\n|    total_timesteps | 972810   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.237   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 972736   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1390.6471104308685\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11388    |\n|    fps             | 59       |\n|    time_elapsed    | 16442    |\n|    total_timesteps | 973069   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.108   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 972992   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11392    |\n|    fps             | 59       |\n|    time_elapsed    | 16446    |\n|    total_timesteps | 973325   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.117    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 973248   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -760     |\n| time/              |          |\n|    episodes        | 11396    |\n|    fps             | 59       |\n|    time_elapsed    | 16450    |\n|    total_timesteps | 973582   |\n| train/             |          |\n|    actor_loss      | 706      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0531  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 973504   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -763     |\n| time/              |          |\n|    episodes        | 11400    |\n|    fps             | 59       |\n|    time_elapsed    | 16455    |\n|    total_timesteps | 973839   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.141    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 973760   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1389.7913617602096\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 11404    |\n|    fps             | 59       |\n|    time_elapsed    | 16459    |\n|    total_timesteps | 974095   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.171    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 974016   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 11408    |\n|    fps             | 59       |\n|    time_elapsed    | 16463    |\n|    total_timesteps | 974351   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0294  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 974272   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 11412    |\n|    fps             | 59       |\n|    time_elapsed    | 16468    |\n|    total_timesteps | 974609   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0065   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 974528   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 11416    |\n|    fps             | 59       |\n|    time_elapsed    | 16472    |\n|    total_timesteps | 974864   |\n| train/             |          |\n|    actor_loss      | 711      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0507  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 974784   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1388.9109308209988\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -762     |\n| time/              |          |\n|    episodes        | 11420    |\n|    fps             | 59       |\n|    time_elapsed    | 16476    |\n|    total_timesteps | 975122   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.287    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 975040   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -756     |\n| time/              |          |\n|    episodes        | 11424    |\n|    fps             | 59       |\n|    time_elapsed    | 16480    |\n|    total_timesteps | 975378   |\n| train/             |          |\n|    actor_loss      | 720      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.19    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 975296   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 11428    |\n|    fps             | 59       |\n|    time_elapsed    | 16485    |\n|    total_timesteps | 975635   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0809   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 975552   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 11432    |\n|    fps             | 59       |\n|    time_elapsed    | 16489    |\n|    total_timesteps | 975892   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.137   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 975808   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1387.9851354741154\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -764     |\n| time/              |          |\n|    episodes        | 11436    |\n|    fps             | 59       |\n|    time_elapsed    | 16493    |\n|    total_timesteps | 976148   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0169   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 976064   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11440    |\n|    fps             | 59       |\n|    time_elapsed    | 16498    |\n|    total_timesteps | 976405   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.101    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 976320   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 11444    |\n|    fps             | 59       |\n|    time_elapsed    | 16502    |\n|    total_timesteps | 976663   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | -0.0913  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 976576   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 11448    |\n|    fps             | 59       |\n|    time_elapsed    | 16506    |\n|    total_timesteps | 976919   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0596  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 976832   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1387.2143027816098\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -766     |\n| time/              |          |\n|    episodes        | 11452    |\n|    fps             | 59       |\n|    time_elapsed    | 16511    |\n|    total_timesteps | 977175   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.00851  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 977088   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -761     |\n| time/              |          |\n|    episodes        | 11456    |\n|    fps             | 59       |\n|    time_elapsed    | 16515    |\n|    total_timesteps | 977431   |\n| train/             |          |\n|    actor_loss      | 711      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.135    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 977344   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11460    |\n|    fps             | 59       |\n|    time_elapsed    | 16519    |\n|    total_timesteps | 977687   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.08     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 977600   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -754     |\n| time/              |          |\n|    episodes        | 11464    |\n|    fps             | 59       |\n|    time_elapsed    | 16523    |\n|    total_timesteps | 977942   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0452   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 977856   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1386.3099059492672\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11468    |\n|    fps             | 59       |\n|    time_elapsed    | 16528    |\n|    total_timesteps | 978198   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.197    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 978112   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -741     |\n| time/              |          |\n|    episodes        | 11472    |\n|    fps             | 59       |\n|    time_elapsed    | 16532    |\n|    total_timesteps | 978454   |\n| train/             |          |\n|    actor_loss      | 697      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.102   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 978368   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11476    |\n|    fps             | 59       |\n|    time_elapsed    | 16536    |\n|    total_timesteps | 978710   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0732  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 978624   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -746     |\n| time/              |          |\n|    episodes        | 11480    |\n|    fps             | 59       |\n|    time_elapsed    | 16541    |\n|    total_timesteps | 978966   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.242    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 978880   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1385.3751995004986\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -742     |\n| time/              |          |\n|    episodes        | 11484    |\n|    fps             | 59       |\n|    time_elapsed    | 16545    |\n|    total_timesteps | 979222   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.00301 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 979136   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11488    |\n|    fps             | 59       |\n|    time_elapsed    | 16549    |\n|    total_timesteps | 979478   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | -0.144   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 979392   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11492    |\n|    fps             | 59       |\n|    time_elapsed    | 16553    |\n|    total_timesteps | 979734   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.08     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 979648   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11496    |\n|    fps             | 59       |\n|    time_elapsed    | 16558    |\n|    total_timesteps | 979991   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.0103   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 979904   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1384.4937861511964\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -744     |\n| time/              |          |\n|    episodes        | 11500    |\n|    fps             | 59       |\n|    time_elapsed    | 16562    |\n|    total_timesteps | 980248   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0155   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 980160   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -740     |\n| time/              |          |\n|    episodes        | 11504    |\n|    fps             | 59       |\n|    time_elapsed    | 16566    |\n|    total_timesteps | 980507   |\n| train/             |          |\n|    actor_loss      | 704      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.00823  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 980416   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -743     |\n| time/              |          |\n|    episodes        | 11508    |\n|    fps             | 59       |\n|    time_elapsed    | 16571    |\n|    total_timesteps | 980763   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.077   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 980672   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1383.7758040585454\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -754     |\n| time/              |          |\n|    episodes        | 11512    |\n|    fps             | 59       |\n|    time_elapsed    | 16575    |\n|    total_timesteps | 981019   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.8     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.168    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 980928   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11516    |\n|    fps             | 59       |\n|    time_elapsed    | 16579    |\n|    total_timesteps | 981275   |\n| train/             |          |\n|    actor_loss      | 721      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0259  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 981184   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -751     |\n| time/              |          |\n|    episodes        | 11520    |\n|    fps             | 59       |\n|    time_elapsed    | 16584    |\n|    total_timesteps | 981531   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.181    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 981440   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11524    |\n|    fps             | 59       |\n|    time_elapsed    | 16588    |\n|    total_timesteps | 981788   |\n| train/             |          |\n|    actor_loss      | 703      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.0287   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 981696   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1382.8786666607566\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11528    |\n|    fps             | 59       |\n|    time_elapsed    | 16592    |\n|    total_timesteps | 982044   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.16     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 981952   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11532    |\n|    fps             | 59       |\n|    time_elapsed    | 16596    |\n|    total_timesteps | 982300   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.106    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 982208   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -760     |\n| time/              |          |\n|    episodes        | 11536    |\n|    fps             | 59       |\n|    time_elapsed    | 16601    |\n|    total_timesteps | 982557   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.122    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 982464   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -762     |\n| time/              |          |\n|    episodes        | 11540    |\n|    fps             | 59       |\n|    time_elapsed    | 16605    |\n|    total_timesteps | 982813   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0833   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 982720   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\nNew best mean reward across all envs: -1382.0952209552552\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -750     |\n| time/              |          |\n|    episodes        | 11544    |\n|    fps             | 59       |\n|    time_elapsed    | 16609    |\n|    total_timesteps | 983070   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0638  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 982976   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11548    |\n|    fps             | 59       |\n|    time_elapsed    | 16614    |\n|    total_timesteps | 983326   |\n| train/             |          |\n|    actor_loss      | 701      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | 0.0237   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 983232   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11552    |\n|    fps             | 59       |\n|    time_elapsed    | 16618    |\n|    total_timesteps | 983582   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.04    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 983488   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11556    |\n|    fps             | 59       |\n|    time_elapsed    | 16622    |\n|    total_timesteps | 983840   |\n| train/             |          |\n|    actor_loss      | 704      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.221    |\n|    ent_coef_loss   | 0.0465   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 983744   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1381.2032183727908\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11560    |\n|    fps             | 59       |\n|    time_elapsed    | 16627    |\n|    total_timesteps | 984096   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | 0.109    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 984000   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11564    |\n|    fps             | 59       |\n|    time_elapsed    | 16631    |\n|    total_timesteps | 984352   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.219    |\n|    ent_coef_loss   | -0.147   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 984256   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 11568    |\n|    fps             | 59       |\n|    time_elapsed    | 16635    |\n|    total_timesteps | 984613   |\n| train/             |          |\n|    actor_loss      | 703      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.218    |\n|    ent_coef_loss   | 0.0246   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 984512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -772     |\n| time/              |          |\n|    episodes        | 11572    |\n|    fps             | 59       |\n|    time_elapsed    | 16639    |\n|    total_timesteps | 984873   |\n| train/             |          |\n|    actor_loss      | 698      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.201   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 984768   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1380.4353199742127\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -771     |\n| time/              |          |\n|    episodes        | 11576    |\n|    fps             | 59       |\n|    time_elapsed    | 16644    |\n|    total_timesteps | 985133   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.026    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 985024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -774     |\n| time/              |          |\n|    episodes        | 11580    |\n|    fps             | 59       |\n|    time_elapsed    | 16648    |\n|    total_timesteps | 985389   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0755   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 985280   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -791     |\n| time/              |          |\n|    episodes        | 11584    |\n|    fps             | 59       |\n|    time_elapsed    | 16652    |\n|    total_timesteps | 985645   |\n| train/             |          |\n|    actor_loss      | 706      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0582  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 985536   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -785     |\n| time/              |          |\n|    episodes        | 11588    |\n|    fps             | 59       |\n|    time_elapsed    | 16656    |\n|    total_timesteps | 985901   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0707   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 985792   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1379.6877283880235\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -784     |\n| time/              |          |\n|    episodes        | 11592    |\n|    fps             | 59       |\n|    time_elapsed    | 16661    |\n|    total_timesteps | 986157   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0256  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 986048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -778     |\n| time/              |          |\n|    episodes        | 11596    |\n|    fps             | 59       |\n|    time_elapsed    | 16665    |\n|    total_timesteps | 986413   |\n| train/             |          |\n|    actor_loss      | 700      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.00847 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 986304   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -779     |\n| time/              |          |\n|    episodes        | 11600    |\n|    fps             | 59       |\n|    time_elapsed    | 16670    |\n|    total_timesteps | 986670   |\n| train/             |          |\n|    actor_loss      | 706      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.243    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 986560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -781     |\n| time/              |          |\n|    episodes        | 11604    |\n|    fps             | 59       |\n|    time_elapsed    | 16674    |\n|    total_timesteps | 986926   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0649   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 986816   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1378.761890643602\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -770     |\n| time/              |          |\n|    episodes        | 11608    |\n|    fps             | 59       |\n|    time_elapsed    | 16678    |\n|    total_timesteps | 987182   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.228    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 987072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11612    |\n|    fps             | 59       |\n|    time_elapsed    | 16682    |\n|    total_timesteps | 987438   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.103   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 987328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11616    |\n|    fps             | 59       |\n|    time_elapsed    | 16687    |\n|    total_timesteps | 987694   |\n| train/             |          |\n|    actor_loss      | 711      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | 0.172    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 987584   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11620    |\n|    fps             | 59       |\n|    time_elapsed    | 16691    |\n|    total_timesteps | 987950   |\n| train/             |          |\n|    actor_loss      | 710      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.0394  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 987840   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1377.8089718263927\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11624    |\n|    fps             | 59       |\n|    time_elapsed    | 16695    |\n|    total_timesteps | 988207   |\n| train/             |          |\n|    actor_loss      | 706      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.109    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 988096   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -751     |\n| time/              |          |\n|    episodes        | 11628    |\n|    fps             | 59       |\n|    time_elapsed    | 16700    |\n|    total_timesteps | 988463   |\n| train/             |          |\n|    actor_loss      | 720      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.0757   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 988352   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -748     |\n| time/              |          |\n|    episodes        | 11632    |\n|    fps             | 59       |\n|    time_elapsed    | 16704    |\n|    total_timesteps | 988719   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.00732 |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 988608   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11636    |\n|    fps             | 59       |\n|    time_elapsed    | 16708    |\n|    total_timesteps | 988975   |\n| train/             |          |\n|    actor_loss      | 698      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0209   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 988864   |\n---------------------------------\nNew best mean reward across all envs: -1376.9676887202327\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11640    |\n|    fps             | 59       |\n|    time_elapsed    | 16712    |\n|    total_timesteps | 989240   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0795  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 989120   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11644    |\n|    fps             | 59       |\n|    time_elapsed    | 16717    |\n|    total_timesteps | 989497   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0438   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 989376   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11648    |\n|    fps             | 59       |\n|    time_elapsed    | 16721    |\n|    total_timesteps | 989753   |\n| train/             |          |\n|    actor_loss      | 718      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0345   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 989632   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1376.202389543934\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11652    |\n|    fps             | 59       |\n|    time_elapsed    | 16725    |\n|    total_timesteps | 990009   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | 0.167    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 989888   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11656    |\n|    fps             | 59       |\n|    time_elapsed    | 16730    |\n|    total_timesteps | 990266   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.22     |\n|    ent_coef_loss   | -0.0286  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 990144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -753     |\n| time/              |          |\n|    episodes        | 11660    |\n|    fps             | 59       |\n|    time_elapsed    | 16734    |\n|    total_timesteps | 990522   |\n| train/             |          |\n|    actor_loss      | 727      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.217    |\n|    ent_coef_loss   | -0.104   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 990400   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.3     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11664    |\n|    fps             | 59       |\n|    time_elapsed    | 16738    |\n|    total_timesteps | 990778   |\n| train/             |          |\n|    actor_loss      | 703      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.183   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 990656   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1375.3838413121525\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -752     |\n| time/              |          |\n|    episodes        | 11668    |\n|    fps             | 59       |\n|    time_elapsed    | 16743    |\n|    total_timesteps | 991034   |\n| train/             |          |\n|    actor_loss      | 704      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0203   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 990912   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -757     |\n| time/              |          |\n|    episodes        | 11672    |\n|    fps             | 59       |\n|    time_elapsed    | 16747    |\n|    total_timesteps | 991290   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0636   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 991168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -755     |\n| time/              |          |\n|    episodes        | 11676    |\n|    fps             | 59       |\n|    time_elapsed    | 16751    |\n|    total_timesteps | 991546   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0878  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 991424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -749     |\n| time/              |          |\n|    episodes        | 11680    |\n|    fps             | 59       |\n|    time_elapsed    | 16756    |\n|    total_timesteps | 991802   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | 0.0662   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 991680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1374.571438273475\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -746     |\n| time/              |          |\n|    episodes        | 11684    |\n|    fps             | 59       |\n|    time_elapsed    | 16760    |\n|    total_timesteps | 992058   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | -0.0984  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 991936   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -747     |\n| time/              |          |\n|    episodes        | 11688    |\n|    fps             | 59       |\n|    time_elapsed    | 16764    |\n|    total_timesteps | 992314   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0164   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 992192   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -759     |\n| time/              |          |\n|    episodes        | 11692    |\n|    fps             | 59       |\n|    time_elapsed    | 16769    |\n|    total_timesteps | 992579   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.5     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.075    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 992512   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -769     |\n| time/              |          |\n|    episodes        | 11696    |\n|    fps             | 59       |\n|    time_elapsed    | 16774    |\n|    total_timesteps | 992835   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0562  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 992768   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1373.967196121996\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -768     |\n| time/              |          |\n|    episodes        | 11700    |\n|    fps             | 59       |\n|    time_elapsed    | 16778    |\n|    total_timesteps | 993091   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.00806  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 993024   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n----------------------------------\n| rollout/           |           |\n|    ep_len_mean     | 64.2      |\n|    ep_rew_mean     | -771      |\n| time/              |           |\n|    episodes        | 11704     |\n|    fps             | 59        |\n|    time_elapsed    | 16782     |\n|    total_timesteps | 993347    |\n| train/             |           |\n|    actor_loss      | 711       |\n|    critic_loss     | 10.7      |\n|    ent_coef        | 0.211     |\n|    ent_coef_loss   | -0.000341 |\n|    learning_rate   | 0.0003    |\n|    n_updates       | 993280    |\n----------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -775     |\n| time/              |          |\n|    episodes        | 11708    |\n|    fps             | 59       |\n|    time_elapsed    | 16786    |\n|    total_timesteps | 993602   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0523   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 993536   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 11712    |\n|    fps             | 59       |\n|    time_elapsed    | 16791    |\n|    total_timesteps | 993857   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.0212  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 993792   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1373.1382920925735\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -792     |\n| time/              |          |\n|    episodes        | 11716    |\n|    fps             | 59       |\n|    time_elapsed    | 16795    |\n|    total_timesteps | 994113   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | -0.066   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 994048   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[Aargv[0]=\n\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 11720    |\n|    fps             | 59       |\n|    time_elapsed    | 16799    |\n|    total_timesteps | 994369   |\n| train/             |          |\n|    actor_loss      | 708      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0038   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 994304   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -786     |\n| time/              |          |\n|    episodes        | 11724    |\n|    fps             | 59       |\n|    time_elapsed    | 16804    |\n|    total_timesteps | 994625   |\n| train/             |          |\n|    actor_loss      | 707      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.117   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 994560   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -789     |\n| time/              |          |\n|    episodes        | 11728    |\n|    fps             | 59       |\n|    time_elapsed    | 16808    |\n|    total_timesteps | 994881   |\n| train/             |          |\n|    actor_loss      | 711      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0858   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 994816   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1372.3039881490665\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -793     |\n| time/              |          |\n|    episodes        | 11732    |\n|    fps             | 59       |\n|    time_elapsed    | 16812    |\n|    total_timesteps | 995137   |\n| train/             |          |\n|    actor_loss      | 711      |\n|    critic_loss     | 11.3     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0638   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 995072   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.2     |\n|    ep_rew_mean     | -790     |\n| time/              |          |\n|    episodes        | 11736    |\n|    fps             | 59       |\n|    time_elapsed    | 16816    |\n|    total_timesteps | 995393   |\n| train/             |          |\n|    actor_loss      | 713      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0199   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 995328   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -787     |\n| time/              |          |\n|    episodes        | 11740    |\n|    fps             | 59       |\n|    time_elapsed    | 16820    |\n|    total_timesteps | 995648   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | 0.0209   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 995520   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -795     |\n| time/              |          |\n|    episodes        | 11744    |\n|    fps             | 59       |\n|    time_elapsed    | 16824    |\n|    total_timesteps | 995904   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 10.7     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0723  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 995776   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1371.5736808891816\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -799     |\n| time/              |          |\n|    episodes        | 11748    |\n|    fps             | 59       |\n|    time_elapsed    | 16829    |\n|    total_timesteps | 996160   |\n| train/             |          |\n|    actor_loss      | 702      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.211    |\n|    ent_coef_loss   | -0.0935  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 996032   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 11752    |\n|    fps             | 59       |\n|    time_elapsed    | 16833    |\n|    total_timesteps | 996416   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.208    |\n|    ent_coef_loss   | 0.112    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 996288   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -796     |\n| time/              |          |\n|    episodes        | 11756    |\n|    fps             | 59       |\n|    time_elapsed    | 16837    |\n|    total_timesteps | 996672   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.209    |\n|    ent_coef_loss   | 0.147    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 996544   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -798     |\n| time/              |          |\n|    episodes        | 11760    |\n|    fps             | 59       |\n|    time_elapsed    | 16842    |\n|    total_timesteps | 996928   |\n| train/             |          |\n|    actor_loss      | 717      |\n|    critic_loss     | 11.4     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.21     |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 996800   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1370.7642262419895\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -797     |\n| time/              |          |\n|    episodes        | 11764    |\n|    fps             | 59       |\n|    time_elapsed    | 16847    |\n|    total_timesteps | 997185   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0252  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 997120   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -801     |\n| time/              |          |\n|    episodes        | 11768    |\n|    fps             | 59       |\n|    time_elapsed    | 16851    |\n|    total_timesteps | 997442   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 10.9     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.0106  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 997376   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -803     |\n| time/              |          |\n|    episodes        | 11772    |\n|    fps             | 59       |\n|    time_elapsed    | 16855    |\n|    total_timesteps | 997700   |\n| train/             |          |\n|    actor_loss      | 715      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.216    |\n|    ent_coef_loss   | -0.187   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 997632   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -804     |\n| time/              |          |\n|    episodes        | 11776    |\n|    fps             | 59       |\n|    time_elapsed    | 16860    |\n|    total_timesteps | 997957   |\n| train/             |          |\n|    actor_loss      | 701      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.215    |\n|    ent_coef_loss   | -0.0567  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 997888   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1370.0957820736025\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -812     |\n| time/              |          |\n|    episodes        | 11780    |\n|    fps             | 59       |\n|    time_elapsed    | 16864    |\n|    total_timesteps | 998214   |\n| train/             |          |\n|    actor_loss      | 714      |\n|    critic_loss     | 10.7     |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.0581   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 998144   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -813     |\n| time/              |          |\n|    episodes        | 11784    |\n|    fps             | 59       |\n|    time_elapsed    | 16868    |\n|    total_timesteps | 998470   |\n| train/             |          |\n|    actor_loss      | 712      |\n|    critic_loss     | 11.2     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.107   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 998400   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64.1     |\n|    ep_rew_mean     | -820     |\n| time/              |          |\n|    episodes        | 11788    |\n|    fps             | 59       |\n|    time_elapsed    | 16872    |\n|    total_timesteps | 998726   |\n| train/             |          |\n|    actor_loss      | 709      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | 0.0815   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 998656   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -815     |\n| time/              |          |\n|    episodes        | 11792    |\n|    fps             | 59       |\n|    time_elapsed    | 16877    |\n|    total_timesteps | 998982   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 10.7     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0863  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 998912   |\n---------------------------------\nargv[0]=\nNew best mean reward across all envs: -1369.4779488485458\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 11796    |\n|    fps             | 59       |\n|    time_elapsed    | 16881    |\n|    total_timesteps | 999238   |\n| train/             |          |\n|    actor_loss      | 705      |\n|    critic_loss     | 10.8     |\n|    ent_coef        | 0.212    |\n|    ent_coef_loss   | -0.0748  |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 999168   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -814     |\n| time/              |          |\n|    episodes        | 11800    |\n|    fps             | 59       |\n|    time_elapsed    | 16886    |\n|    total_timesteps | 999494   |\n| train/             |          |\n|    actor_loss      | 719      |\n|    critic_loss     | 11.1     |\n|    ent_coef        | 0.214    |\n|    ent_coef_loss   | 0.0431   |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 999424   |\n---------------------------------\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 64       |\n|    ep_rew_mean     | -811     |\n| time/              |          |\n|    episodes        | 11804    |\n|    fps             | 59       |\n|    time_elapsed    | 16890    |\n|    total_timesteps | 999750   |\n| train/             |          |\n|    actor_loss      | 716      |\n|    critic_loss     | 11       |\n|    ent_coef        | 0.213    |\n|    ent_coef_loss   | 0.139    |\n|    learning_rate   | 0.0003   |\n|    n_updates       | 999680   |\n---------------------------------\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\n\u001b[A                             \u001b[A\nargv[0]=\nNew best mean reward across all envs: -1368.7167867434796\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<sb3_contrib.tqc.tqc.TQC at 0x78a60ba87100>"},"metadata":{}}]},{"cell_type":"code","source":"model.save(\"TQC_rocket_landing_v9_manual\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int(3e6), int(1e7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_last_model(check_point_path=\"/kaggle/working/checkpoints\"):\n    import os\n    return os.path.join(check_point_path, sorted(os.listdir(check_point_path),\n       key=lambda x : int(x.split(\"_\")[2]),\n      reverse=True)[0])\ndef copy_last_model(check_point_path=\"/kaggle/working/checkpoints\", dst=\"/kaggle/working/\"):\n    import shutil\n    src = get_last_model(check_point_path)\n    shutil.copy(src, dst)\n    print(f\"Copied the model {src.split('/')[-1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:50:47.481618Z","iopub.execute_input":"2024-05-19T08:50:47.481991Z","iopub.status.idle":"2024-05-19T08:50:47.488379Z","shell.execute_reply.started":"2024-05-19T08:50:47.481964Z","shell.execute_reply":"2024-05-19T08:50:47.487377Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"copy_last_model()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:50:48.051743Z","iopub.execute_input":"2024-05-19T08:50:48.052109Z","iopub.status.idle":"2024-05-19T08:50:48.062647Z","shell.execute_reply.started":"2024-05-19T08:50:48.052080Z","shell.execute_reply":"2024-05-19T08:50:48.061784Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Copied the model rl_model_1000000_steps.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nfrom pathlib import Path\n \ndef copy_latest_file(src_dir, dest_dir):\n    # Get list of files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        print(\"No files found in the source directory.\")\n        return\n \n    # Find the file with the latest timestamp\n    latest_file = max(files, key=lambda x: os.path.getmtime(os.path.join(src_dir, x)))\n    # Construct full file paths\n    src_file = os.path.join(src_dir, latest_file)\n    dest_file = os.path.join(dest_dir, latest_file)\n    # Copy the latest file to the destination directory\n    shutil.copy2(src_file, dest_file)\n    print(f\"Copied {latest_file} to {dest_dir}\")\n \n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:50:40.050587Z","iopub.execute_input":"2024-05-19T08:50:40.051040Z","iopub.status.idle":"2024-05-19T08:50:40.058528Z","shell.execute_reply.started":"2024-05-19T08:50:40.051011Z","shell.execute_reply":"2024-05-19T08:50:40.057519Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Example usage\nsource_directory = '/kaggle/working/checkpoints'\ndestination_directory = '/kaggle/working'\ncopy_latest_file(source_directory, destination_directory)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:50:40.699502Z","iopub.execute_input":"2024-05-19T08:50:40.699863Z","iopub.status.idle":"2024-05-19T08:50:40.724105Z","shell.execute_reply.started":"2024-05-19T08:50:40.699835Z","shell.execute_reply":"2024-05-19T08:50:40.723207Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Copied rl_model_999000_steps.zip to /kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"os.path.getmtime('/kaggle/working/checkpoints/rl_model_999000_steps.zip'), os.path.getmtime('/kaggle/working/checkpoints/rl_model_1000000_steps.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:56:30.758071Z","iopub.execute_input":"2024-05-19T08:56:30.758399Z","iopub.status.idle":"2024-05-19T08:56:30.764535Z","shell.execute_reply.started":"2024-05-19T08:56:30.758376Z","shell.execute_reply":"2024-05-19T08:56:30.763645Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(1716108390.8583019, 1716108347.5085618)"},"metadata":{}}]},{"cell_type":"code","source":"\nos.path.getmtime('/kaggle/working/checkpoints/rl_model_838000_steps.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:57:12.809594Z","iopub.execute_input":"2024-05-19T08:57:12.810266Z","iopub.status.idle":"2024-05-19T08:57:12.815893Z","shell.execute_reply.started":"2024-05-19T08:57:12.810234Z","shell.execute_reply":"2024-05-19T08:57:12.815046Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"1716108383.3486536"},"metadata":{}}]},{"cell_type":"code","source":"source_directory, sorted(os.listdir(source_directory),\n       key=lambda x : int(x.split(\"_\")[2]),\n      reverse=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:55:00.459402Z","iopub.execute_input":"2024-05-19T08:55:00.460359Z","iopub.status.idle":"2024-05-19T08:55:00.486657Z","shell.execute_reply.started":"2024-05-19T08:55:00.460323Z","shell.execute_reply":"2024-05-19T08:55:00.485746Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/checkpoints',\n ['rl_model_1000000_steps.zip',\n  'rl_model_999000_steps.zip',\n  'rl_model_998000_steps.zip',\n  'rl_model_997000_steps.zip',\n  'rl_model_996000_steps.zip',\n  'rl_model_995000_steps.zip',\n  'rl_model_994000_steps.zip',\n  'rl_model_993000_steps.zip',\n  'rl_model_992000_steps.zip',\n  'rl_model_991000_steps.zip',\n  'rl_model_990000_steps.zip',\n  'rl_model_989000_steps.zip',\n  'rl_model_988000_steps.zip',\n  'rl_model_987000_steps.zip',\n  'rl_model_986000_steps.zip',\n  'rl_model_985000_steps.zip',\n  'rl_model_984000_steps.zip',\n  'rl_model_983000_steps.zip',\n  'rl_model_982000_steps.zip',\n  'rl_model_981000_steps.zip',\n  'rl_model_980000_steps.zip',\n  'rl_model_979000_steps.zip',\n  'rl_model_978000_steps.zip',\n  'rl_model_977000_steps.zip',\n  'rl_model_976000_steps.zip',\n  'rl_model_975000_steps.zip',\n  'rl_model_974000_steps.zip',\n  'rl_model_973000_steps.zip',\n  'rl_model_972000_steps.zip',\n  'rl_model_971000_steps.zip',\n  'rl_model_970000_steps.zip',\n  'rl_model_969000_steps.zip',\n  'rl_model_968000_steps.zip',\n  'rl_model_967000_steps.zip',\n  'rl_model_966000_steps.zip',\n  'rl_model_965000_steps.zip',\n  'rl_model_964000_steps.zip',\n  'rl_model_963000_steps.zip',\n  'rl_model_962000_steps.zip',\n  'rl_model_961000_steps.zip',\n  'rl_model_960000_steps.zip',\n  'rl_model_959000_steps.zip',\n  'rl_model_958000_steps.zip',\n  'rl_model_957000_steps.zip',\n  'rl_model_956000_steps.zip',\n  'rl_model_955000_steps.zip',\n  'rl_model_954000_steps.zip',\n  'rl_model_953000_steps.zip',\n  'rl_model_952000_steps.zip',\n  'rl_model_951000_steps.zip',\n  'rl_model_950000_steps.zip',\n  'rl_model_949000_steps.zip',\n  'rl_model_948000_steps.zip',\n  'rl_model_947000_steps.zip',\n  'rl_model_946000_steps.zip',\n  'rl_model_945000_steps.zip',\n  'rl_model_944000_steps.zip',\n  'rl_model_943000_steps.zip',\n  'rl_model_942000_steps.zip',\n  'rl_model_941000_steps.zip',\n  'rl_model_940000_steps.zip',\n  'rl_model_939000_steps.zip',\n  'rl_model_938000_steps.zip',\n  'rl_model_937000_steps.zip',\n  'rl_model_936000_steps.zip',\n  'rl_model_935000_steps.zip',\n  'rl_model_934000_steps.zip',\n  'rl_model_933000_steps.zip',\n  'rl_model_932000_steps.zip',\n  'rl_model_931000_steps.zip',\n  'rl_model_930000_steps.zip',\n  'rl_model_929000_steps.zip',\n  'rl_model_928000_steps.zip',\n  'rl_model_927000_steps.zip',\n  'rl_model_926000_steps.zip',\n  'rl_model_925000_steps.zip',\n  'rl_model_924000_steps.zip',\n  'rl_model_923000_steps.zip',\n  'rl_model_922000_steps.zip',\n  'rl_model_921000_steps.zip',\n  'rl_model_920000_steps.zip',\n  'rl_model_919000_steps.zip',\n  'rl_model_918000_steps.zip',\n  'rl_model_917000_steps.zip',\n  'rl_model_916000_steps.zip',\n  'rl_model_915000_steps.zip',\n  'rl_model_914000_steps.zip',\n  'rl_model_913000_steps.zip',\n  'rl_model_912000_steps.zip',\n  'rl_model_911000_steps.zip',\n  'rl_model_910000_steps.zip',\n  'rl_model_909000_steps.zip',\n  'rl_model_908000_steps.zip',\n  'rl_model_907000_steps.zip',\n  'rl_model_906000_steps.zip',\n  'rl_model_905000_steps.zip',\n  'rl_model_904000_steps.zip',\n  'rl_model_903000_steps.zip',\n  'rl_model_902000_steps.zip',\n  'rl_model_901000_steps.zip',\n  'rl_model_900000_steps.zip',\n  'rl_model_899000_steps.zip',\n  'rl_model_898000_steps.zip',\n  'rl_model_897000_steps.zip',\n  'rl_model_896000_steps.zip',\n  'rl_model_895000_steps.zip',\n  'rl_model_894000_steps.zip',\n  'rl_model_893000_steps.zip',\n  'rl_model_892000_steps.zip',\n  'rl_model_891000_steps.zip',\n  'rl_model_890000_steps.zip',\n  'rl_model_889000_steps.zip',\n  'rl_model_888000_steps.zip',\n  'rl_model_887000_steps.zip',\n  'rl_model_886000_steps.zip',\n  'rl_model_885000_steps.zip',\n  'rl_model_884000_steps.zip',\n  'rl_model_883000_steps.zip',\n  'rl_model_882000_steps.zip',\n  'rl_model_881000_steps.zip',\n  'rl_model_880000_steps.zip',\n  'rl_model_879000_steps.zip',\n  'rl_model_878000_steps.zip',\n  'rl_model_877000_steps.zip',\n  'rl_model_876000_steps.zip',\n  'rl_model_875000_steps.zip',\n  'rl_model_874000_steps.zip',\n  'rl_model_873000_steps.zip',\n  'rl_model_872000_steps.zip',\n  'rl_model_871000_steps.zip',\n  'rl_model_870000_steps.zip',\n  'rl_model_869000_steps.zip',\n  'rl_model_868000_steps.zip',\n  'rl_model_867000_steps.zip',\n  'rl_model_866000_steps.zip',\n  'rl_model_865000_steps.zip',\n  'rl_model_864000_steps.zip',\n  'rl_model_863000_steps.zip',\n  'rl_model_862000_steps.zip',\n  'rl_model_861000_steps.zip',\n  'rl_model_860000_steps.zip',\n  'rl_model_859000_steps.zip',\n  'rl_model_858000_steps.zip',\n  'rl_model_857000_steps.zip',\n  'rl_model_856000_steps.zip',\n  'rl_model_855000_steps.zip',\n  'rl_model_854000_steps.zip',\n  'rl_model_853000_steps.zip',\n  'rl_model_852000_steps.zip',\n  'rl_model_851000_steps.zip',\n  'rl_model_850000_steps.zip',\n  'rl_model_849000_steps.zip',\n  'rl_model_848000_steps.zip',\n  'rl_model_847000_steps.zip',\n  'rl_model_846000_steps.zip',\n  'rl_model_845000_steps.zip',\n  'rl_model_844000_steps.zip',\n  'rl_model_843000_steps.zip',\n  'rl_model_842000_steps.zip',\n  'rl_model_841000_steps.zip',\n  'rl_model_840000_steps.zip',\n  'rl_model_839000_steps.zip',\n  'rl_model_838000_steps.zip',\n  'rl_model_837000_steps.zip',\n  'rl_model_836000_steps.zip',\n  'rl_model_835000_steps.zip',\n  'rl_model_834000_steps.zip',\n  'rl_model_833000_steps.zip',\n  'rl_model_832000_steps.zip',\n  'rl_model_831000_steps.zip',\n  'rl_model_830000_steps.zip',\n  'rl_model_829000_steps.zip',\n  'rl_model_828000_steps.zip',\n  'rl_model_827000_steps.zip',\n  'rl_model_826000_steps.zip',\n  'rl_model_825000_steps.zip',\n  'rl_model_824000_steps.zip',\n  'rl_model_823000_steps.zip',\n  'rl_model_822000_steps.zip',\n  'rl_model_821000_steps.zip',\n  'rl_model_820000_steps.zip',\n  'rl_model_819000_steps.zip',\n  'rl_model_818000_steps.zip',\n  'rl_model_817000_steps.zip',\n  'rl_model_816000_steps.zip',\n  'rl_model_815000_steps.zip',\n  'rl_model_814000_steps.zip',\n  'rl_model_813000_steps.zip',\n  'rl_model_812000_steps.zip',\n  'rl_model_811000_steps.zip',\n  'rl_model_810000_steps.zip',\n  'rl_model_809000_steps.zip',\n  'rl_model_808000_steps.zip',\n  'rl_model_807000_steps.zip',\n  'rl_model_806000_steps.zip',\n  'rl_model_805000_steps.zip',\n  'rl_model_804000_steps.zip',\n  'rl_model_803000_steps.zip',\n  'rl_model_802000_steps.zip',\n  'rl_model_801000_steps.zip',\n  'rl_model_800000_steps.zip',\n  'rl_model_799000_steps.zip',\n  'rl_model_798000_steps.zip',\n  'rl_model_797000_steps.zip',\n  'rl_model_796000_steps.zip',\n  'rl_model_795000_steps.zip',\n  'rl_model_794000_steps.zip',\n  'rl_model_793000_steps.zip',\n  'rl_model_792000_steps.zip',\n  'rl_model_791000_steps.zip',\n  'rl_model_790000_steps.zip',\n  'rl_model_789000_steps.zip',\n  'rl_model_788000_steps.zip',\n  'rl_model_787000_steps.zip',\n  'rl_model_786000_steps.zip',\n  'rl_model_785000_steps.zip',\n  'rl_model_784000_steps.zip',\n  'rl_model_783000_steps.zip',\n  'rl_model_782000_steps.zip',\n  'rl_model_781000_steps.zip',\n  'rl_model_780000_steps.zip',\n  'rl_model_779000_steps.zip',\n  'rl_model_778000_steps.zip',\n  'rl_model_777000_steps.zip',\n  'rl_model_776000_steps.zip',\n  'rl_model_775000_steps.zip',\n  'rl_model_774000_steps.zip',\n  'rl_model_773000_steps.zip',\n  'rl_model_772000_steps.zip',\n  'rl_model_771000_steps.zip',\n  'rl_model_770000_steps.zip',\n  'rl_model_769000_steps.zip',\n  'rl_model_768000_steps.zip',\n  'rl_model_767000_steps.zip',\n  'rl_model_766000_steps.zip',\n  'rl_model_765000_steps.zip',\n  'rl_model_764000_steps.zip',\n  'rl_model_763000_steps.zip',\n  'rl_model_762000_steps.zip',\n  'rl_model_761000_steps.zip',\n  'rl_model_760000_steps.zip',\n  'rl_model_759000_steps.zip',\n  'rl_model_758000_steps.zip',\n  'rl_model_757000_steps.zip',\n  'rl_model_756000_steps.zip',\n  'rl_model_755000_steps.zip',\n  'rl_model_754000_steps.zip',\n  'rl_model_753000_steps.zip',\n  'rl_model_752000_steps.zip',\n  'rl_model_751000_steps.zip',\n  'rl_model_750000_steps.zip',\n  'rl_model_749000_steps.zip',\n  'rl_model_748000_steps.zip',\n  'rl_model_747000_steps.zip',\n  'rl_model_746000_steps.zip',\n  'rl_model_745000_steps.zip',\n  'rl_model_744000_steps.zip',\n  'rl_model_743000_steps.zip',\n  'rl_model_742000_steps.zip',\n  'rl_model_741000_steps.zip',\n  'rl_model_740000_steps.zip',\n  'rl_model_739000_steps.zip',\n  'rl_model_738000_steps.zip',\n  'rl_model_737000_steps.zip',\n  'rl_model_736000_steps.zip',\n  'rl_model_735000_steps.zip',\n  'rl_model_734000_steps.zip',\n  'rl_model_733000_steps.zip',\n  'rl_model_732000_steps.zip',\n  'rl_model_731000_steps.zip',\n  'rl_model_730000_steps.zip',\n  'rl_model_729000_steps.zip',\n  'rl_model_728000_steps.zip',\n  'rl_model_727000_steps.zip',\n  'rl_model_726000_steps.zip',\n  'rl_model_725000_steps.zip',\n  'rl_model_724000_steps.zip',\n  'rl_model_723000_steps.zip',\n  'rl_model_722000_steps.zip',\n  'rl_model_721000_steps.zip',\n  'rl_model_720000_steps.zip',\n  'rl_model_719000_steps.zip',\n  'rl_model_718000_steps.zip',\n  'rl_model_717000_steps.zip',\n  'rl_model_716000_steps.zip',\n  'rl_model_715000_steps.zip',\n  'rl_model_714000_steps.zip',\n  'rl_model_713000_steps.zip',\n  'rl_model_712000_steps.zip',\n  'rl_model_711000_steps.zip',\n  'rl_model_710000_steps.zip',\n  'rl_model_709000_steps.zip',\n  'rl_model_708000_steps.zip',\n  'rl_model_707000_steps.zip',\n  'rl_model_706000_steps.zip',\n  'rl_model_705000_steps.zip',\n  'rl_model_704000_steps.zip',\n  'rl_model_703000_steps.zip',\n  'rl_model_702000_steps.zip',\n  'rl_model_701000_steps.zip',\n  'rl_model_700000_steps.zip',\n  'rl_model_699000_steps.zip',\n  'rl_model_698000_steps.zip',\n  'rl_model_697000_steps.zip',\n  'rl_model_696000_steps.zip',\n  'rl_model_695000_steps.zip',\n  'rl_model_694000_steps.zip',\n  'rl_model_693000_steps.zip',\n  'rl_model_692000_steps.zip',\n  'rl_model_691000_steps.zip',\n  'rl_model_690000_steps.zip',\n  'rl_model_689000_steps.zip',\n  'rl_model_688000_steps.zip',\n  'rl_model_687000_steps.zip',\n  'rl_model_686000_steps.zip',\n  'rl_model_685000_steps.zip',\n  'rl_model_684000_steps.zip',\n  'rl_model_683000_steps.zip',\n  'rl_model_682000_steps.zip',\n  'rl_model_681000_steps.zip',\n  'rl_model_680000_steps.zip',\n  'rl_model_679000_steps.zip',\n  'rl_model_678000_steps.zip',\n  'rl_model_677000_steps.zip',\n  'rl_model_676000_steps.zip',\n  'rl_model_675000_steps.zip',\n  'rl_model_674000_steps.zip',\n  'rl_model_673000_steps.zip',\n  'rl_model_672000_steps.zip',\n  'rl_model_671000_steps.zip',\n  'rl_model_670000_steps.zip',\n  'rl_model_669000_steps.zip',\n  'rl_model_668000_steps.zip',\n  'rl_model_667000_steps.zip',\n  'rl_model_666000_steps.zip',\n  'rl_model_665000_steps.zip',\n  'rl_model_664000_steps.zip',\n  'rl_model_663000_steps.zip',\n  'rl_model_662000_steps.zip',\n  'rl_model_661000_steps.zip',\n  'rl_model_660000_steps.zip',\n  'rl_model_659000_steps.zip',\n  'rl_model_658000_steps.zip',\n  'rl_model_657000_steps.zip',\n  'rl_model_656000_steps.zip',\n  'rl_model_655000_steps.zip',\n  'rl_model_654000_steps.zip',\n  'rl_model_653000_steps.zip',\n  'rl_model_652000_steps.zip',\n  'rl_model_651000_steps.zip',\n  'rl_model_650000_steps.zip',\n  'rl_model_649000_steps.zip',\n  'rl_model_648000_steps.zip',\n  'rl_model_647000_steps.zip',\n  'rl_model_646000_steps.zip',\n  'rl_model_645000_steps.zip',\n  'rl_model_644000_steps.zip',\n  'rl_model_643000_steps.zip',\n  'rl_model_642000_steps.zip',\n  'rl_model_641000_steps.zip',\n  'rl_model_640000_steps.zip',\n  'rl_model_639000_steps.zip',\n  'rl_model_638000_steps.zip',\n  'rl_model_637000_steps.zip',\n  'rl_model_636000_steps.zip',\n  'rl_model_635000_steps.zip',\n  'rl_model_634000_steps.zip',\n  'rl_model_633000_steps.zip',\n  'rl_model_632000_steps.zip',\n  'rl_model_631000_steps.zip',\n  'rl_model_630000_steps.zip',\n  'rl_model_629000_steps.zip',\n  'rl_model_628000_steps.zip',\n  'rl_model_627000_steps.zip',\n  'rl_model_626000_steps.zip',\n  'rl_model_625000_steps.zip',\n  'rl_model_624000_steps.zip',\n  'rl_model_623000_steps.zip',\n  'rl_model_622000_steps.zip',\n  'rl_model_621000_steps.zip',\n  'rl_model_620000_steps.zip',\n  'rl_model_619000_steps.zip',\n  'rl_model_618000_steps.zip',\n  'rl_model_617000_steps.zip',\n  'rl_model_616000_steps.zip',\n  'rl_model_615000_steps.zip',\n  'rl_model_614000_steps.zip',\n  'rl_model_613000_steps.zip',\n  'rl_model_612000_steps.zip',\n  'rl_model_611000_steps.zip',\n  'rl_model_610000_steps.zip',\n  'rl_model_609000_steps.zip',\n  'rl_model_608000_steps.zip',\n  'rl_model_607000_steps.zip',\n  'rl_model_606000_steps.zip',\n  'rl_model_605000_steps.zip',\n  'rl_model_604000_steps.zip',\n  'rl_model_603000_steps.zip',\n  'rl_model_602000_steps.zip',\n  'rl_model_601000_steps.zip',\n  'rl_model_600000_steps.zip',\n  'rl_model_599000_steps.zip',\n  'rl_model_598000_steps.zip',\n  'rl_model_597000_steps.zip',\n  'rl_model_596000_steps.zip',\n  'rl_model_595000_steps.zip',\n  'rl_model_594000_steps.zip',\n  'rl_model_593000_steps.zip',\n  'rl_model_592000_steps.zip',\n  'rl_model_591000_steps.zip',\n  'rl_model_590000_steps.zip',\n  'rl_model_589000_steps.zip',\n  'rl_model_588000_steps.zip',\n  'rl_model_587000_steps.zip',\n  'rl_model_586000_steps.zip',\n  'rl_model_585000_steps.zip',\n  'rl_model_584000_steps.zip',\n  'rl_model_583000_steps.zip',\n  'rl_model_582000_steps.zip',\n  'rl_model_581000_steps.zip',\n  'rl_model_580000_steps.zip',\n  'rl_model_579000_steps.zip',\n  'rl_model_578000_steps.zip',\n  'rl_model_577000_steps.zip',\n  'rl_model_576000_steps.zip',\n  'rl_model_575000_steps.zip',\n  'rl_model_574000_steps.zip',\n  'rl_model_573000_steps.zip',\n  'rl_model_572000_steps.zip',\n  'rl_model_571000_steps.zip',\n  'rl_model_570000_steps.zip',\n  'rl_model_569000_steps.zip',\n  'rl_model_568000_steps.zip',\n  'rl_model_567000_steps.zip',\n  'rl_model_566000_steps.zip',\n  'rl_model_565000_steps.zip',\n  'rl_model_564000_steps.zip',\n  'rl_model_563000_steps.zip',\n  'rl_model_562000_steps.zip',\n  'rl_model_561000_steps.zip',\n  'rl_model_560000_steps.zip',\n  'rl_model_559000_steps.zip',\n  'rl_model_558000_steps.zip',\n  'rl_model_557000_steps.zip',\n  'rl_model_556000_steps.zip',\n  'rl_model_555000_steps.zip',\n  'rl_model_554000_steps.zip',\n  'rl_model_553000_steps.zip',\n  'rl_model_552000_steps.zip',\n  'rl_model_551000_steps.zip',\n  'rl_model_550000_steps.zip',\n  'rl_model_549000_steps.zip',\n  'rl_model_548000_steps.zip',\n  'rl_model_547000_steps.zip',\n  'rl_model_546000_steps.zip',\n  'rl_model_545000_steps.zip',\n  'rl_model_544000_steps.zip',\n  'rl_model_543000_steps.zip',\n  'rl_model_542000_steps.zip',\n  'rl_model_541000_steps.zip',\n  'rl_model_540000_steps.zip',\n  'rl_model_539000_steps.zip',\n  'rl_model_538000_steps.zip',\n  'rl_model_537000_steps.zip',\n  'rl_model_536000_steps.zip',\n  'rl_model_535000_steps.zip',\n  'rl_model_534000_steps.zip',\n  'rl_model_533000_steps.zip',\n  'rl_model_532000_steps.zip',\n  'rl_model_531000_steps.zip',\n  'rl_model_530000_steps.zip',\n  'rl_model_529000_steps.zip',\n  'rl_model_528000_steps.zip',\n  'rl_model_527000_steps.zip',\n  'rl_model_526000_steps.zip',\n  'rl_model_525000_steps.zip',\n  'rl_model_524000_steps.zip',\n  'rl_model_523000_steps.zip',\n  'rl_model_522000_steps.zip',\n  'rl_model_521000_steps.zip',\n  'rl_model_520000_steps.zip',\n  'rl_model_519000_steps.zip',\n  'rl_model_518000_steps.zip',\n  'rl_model_517000_steps.zip',\n  'rl_model_516000_steps.zip',\n  'rl_model_515000_steps.zip',\n  'rl_model_514000_steps.zip',\n  'rl_model_513000_steps.zip',\n  'rl_model_512000_steps.zip',\n  'rl_model_511000_steps.zip',\n  'rl_model_510000_steps.zip',\n  'rl_model_509000_steps.zip',\n  'rl_model_508000_steps.zip',\n  'rl_model_507000_steps.zip',\n  'rl_model_506000_steps.zip',\n  'rl_model_505000_steps.zip',\n  'rl_model_504000_steps.zip',\n  'rl_model_503000_steps.zip',\n  'rl_model_502000_steps.zip',\n  'rl_model_501000_steps.zip',\n  'rl_model_500000_steps.zip',\n  'rl_model_499000_steps.zip',\n  'rl_model_498000_steps.zip',\n  'rl_model_497000_steps.zip',\n  'rl_model_496000_steps.zip',\n  'rl_model_495000_steps.zip',\n  'rl_model_494000_steps.zip',\n  'rl_model_493000_steps.zip',\n  'rl_model_492000_steps.zip',\n  'rl_model_491000_steps.zip',\n  'rl_model_490000_steps.zip',\n  'rl_model_489000_steps.zip',\n  'rl_model_488000_steps.zip',\n  'rl_model_487000_steps.zip',\n  'rl_model_486000_steps.zip',\n  'rl_model_485000_steps.zip',\n  'rl_model_484000_steps.zip',\n  'rl_model_483000_steps.zip',\n  'rl_model_482000_steps.zip',\n  'rl_model_481000_steps.zip',\n  'rl_model_480000_steps.zip',\n  'rl_model_479000_steps.zip',\n  'rl_model_478000_steps.zip',\n  'rl_model_477000_steps.zip',\n  'rl_model_476000_steps.zip',\n  'rl_model_475000_steps.zip',\n  'rl_model_474000_steps.zip',\n  'rl_model_473000_steps.zip',\n  'rl_model_472000_steps.zip',\n  'rl_model_471000_steps.zip',\n  'rl_model_470000_steps.zip',\n  'rl_model_469000_steps.zip',\n  'rl_model_468000_steps.zip',\n  'rl_model_467000_steps.zip',\n  'rl_model_466000_steps.zip',\n  'rl_model_465000_steps.zip',\n  'rl_model_464000_steps.zip',\n  'rl_model_463000_steps.zip',\n  'rl_model_462000_steps.zip',\n  'rl_model_461000_steps.zip',\n  'rl_model_460000_steps.zip',\n  'rl_model_459000_steps.zip',\n  'rl_model_458000_steps.zip',\n  'rl_model_457000_steps.zip',\n  'rl_model_456000_steps.zip',\n  'rl_model_455000_steps.zip',\n  'rl_model_454000_steps.zip',\n  'rl_model_453000_steps.zip',\n  'rl_model_452000_steps.zip',\n  'rl_model_451000_steps.zip',\n  'rl_model_450000_steps.zip',\n  'rl_model_449000_steps.zip',\n  'rl_model_448000_steps.zip',\n  'rl_model_447000_steps.zip',\n  'rl_model_446000_steps.zip',\n  'rl_model_445000_steps.zip',\n  'rl_model_444000_steps.zip',\n  'rl_model_443000_steps.zip',\n  'rl_model_442000_steps.zip',\n  'rl_model_441000_steps.zip',\n  'rl_model_440000_steps.zip',\n  'rl_model_439000_steps.zip',\n  'rl_model_438000_steps.zip',\n  'rl_model_437000_steps.zip',\n  'rl_model_436000_steps.zip',\n  'rl_model_435000_steps.zip',\n  'rl_model_434000_steps.zip',\n  'rl_model_433000_steps.zip',\n  'rl_model_432000_steps.zip',\n  'rl_model_431000_steps.zip',\n  'rl_model_430000_steps.zip',\n  'rl_model_429000_steps.zip',\n  'rl_model_428000_steps.zip',\n  'rl_model_427000_steps.zip',\n  'rl_model_426000_steps.zip',\n  'rl_model_425000_steps.zip',\n  'rl_model_424000_steps.zip',\n  'rl_model_423000_steps.zip',\n  'rl_model_422000_steps.zip',\n  'rl_model_421000_steps.zip',\n  'rl_model_420000_steps.zip',\n  'rl_model_419000_steps.zip',\n  'rl_model_418000_steps.zip',\n  'rl_model_417000_steps.zip',\n  'rl_model_416000_steps.zip',\n  'rl_model_415000_steps.zip',\n  'rl_model_414000_steps.zip',\n  'rl_model_413000_steps.zip',\n  'rl_model_412000_steps.zip',\n  'rl_model_411000_steps.zip',\n  'rl_model_410000_steps.zip',\n  'rl_model_409000_steps.zip',\n  'rl_model_408000_steps.zip',\n  'rl_model_407000_steps.zip',\n  'rl_model_406000_steps.zip',\n  'rl_model_405000_steps.zip',\n  'rl_model_404000_steps.zip',\n  'rl_model_403000_steps.zip',\n  'rl_model_402000_steps.zip',\n  'rl_model_401000_steps.zip',\n  'rl_model_400000_steps.zip',\n  'rl_model_399000_steps.zip',\n  'rl_model_398000_steps.zip',\n  'rl_model_397000_steps.zip',\n  'rl_model_396000_steps.zip',\n  'rl_model_395000_steps.zip',\n  'rl_model_394000_steps.zip',\n  'rl_model_393000_steps.zip',\n  'rl_model_392000_steps.zip',\n  'rl_model_391000_steps.zip',\n  'rl_model_390000_steps.zip',\n  'rl_model_389000_steps.zip',\n  'rl_model_388000_steps.zip',\n  'rl_model_387000_steps.zip',\n  'rl_model_386000_steps.zip',\n  'rl_model_385000_steps.zip',\n  'rl_model_384000_steps.zip',\n  'rl_model_383000_steps.zip',\n  'rl_model_382000_steps.zip',\n  'rl_model_381000_steps.zip',\n  'rl_model_380000_steps.zip',\n  'rl_model_379000_steps.zip',\n  'rl_model_378000_steps.zip',\n  'rl_model_377000_steps.zip',\n  'rl_model_376000_steps.zip',\n  'rl_model_375000_steps.zip',\n  'rl_model_374000_steps.zip',\n  'rl_model_373000_steps.zip',\n  'rl_model_372000_steps.zip',\n  'rl_model_371000_steps.zip',\n  'rl_model_370000_steps.zip',\n  'rl_model_369000_steps.zip',\n  'rl_model_368000_steps.zip',\n  'rl_model_367000_steps.zip',\n  'rl_model_366000_steps.zip',\n  'rl_model_365000_steps.zip',\n  'rl_model_364000_steps.zip',\n  'rl_model_363000_steps.zip',\n  'rl_model_362000_steps.zip',\n  'rl_model_361000_steps.zip',\n  'rl_model_360000_steps.zip',\n  'rl_model_359000_steps.zip',\n  'rl_model_358000_steps.zip',\n  'rl_model_357000_steps.zip',\n  'rl_model_356000_steps.zip',\n  'rl_model_355000_steps.zip',\n  'rl_model_354000_steps.zip',\n  'rl_model_353000_steps.zip',\n  'rl_model_352000_steps.zip',\n  'rl_model_351000_steps.zip',\n  'rl_model_350000_steps.zip',\n  'rl_model_349000_steps.zip',\n  'rl_model_348000_steps.zip',\n  'rl_model_347000_steps.zip',\n  'rl_model_346000_steps.zip',\n  'rl_model_345000_steps.zip',\n  'rl_model_344000_steps.zip',\n  'rl_model_343000_steps.zip',\n  'rl_model_342000_steps.zip',\n  'rl_model_341000_steps.zip',\n  'rl_model_340000_steps.zip',\n  'rl_model_339000_steps.zip',\n  'rl_model_338000_steps.zip',\n  'rl_model_337000_steps.zip',\n  'rl_model_336000_steps.zip',\n  'rl_model_335000_steps.zip',\n  'rl_model_334000_steps.zip',\n  'rl_model_333000_steps.zip',\n  'rl_model_332000_steps.zip',\n  'rl_model_331000_steps.zip',\n  'rl_model_330000_steps.zip',\n  'rl_model_329000_steps.zip',\n  'rl_model_328000_steps.zip',\n  'rl_model_327000_steps.zip',\n  'rl_model_326000_steps.zip',\n  'rl_model_325000_steps.zip',\n  'rl_model_324000_steps.zip',\n  'rl_model_323000_steps.zip',\n  'rl_model_322000_steps.zip',\n  'rl_model_321000_steps.zip',\n  'rl_model_320000_steps.zip',\n  'rl_model_319000_steps.zip',\n  'rl_model_318000_steps.zip',\n  'rl_model_317000_steps.zip',\n  'rl_model_316000_steps.zip',\n  'rl_model_315000_steps.zip',\n  'rl_model_314000_steps.zip',\n  'rl_model_313000_steps.zip',\n  'rl_model_312000_steps.zip',\n  'rl_model_311000_steps.zip',\n  'rl_model_310000_steps.zip',\n  'rl_model_309000_steps.zip',\n  'rl_model_308000_steps.zip',\n  'rl_model_307000_steps.zip',\n  'rl_model_306000_steps.zip',\n  'rl_model_305000_steps.zip',\n  'rl_model_304000_steps.zip',\n  'rl_model_303000_steps.zip',\n  'rl_model_302000_steps.zip',\n  'rl_model_301000_steps.zip',\n  'rl_model_300000_steps.zip',\n  'rl_model_299000_steps.zip',\n  'rl_model_298000_steps.zip',\n  'rl_model_297000_steps.zip',\n  'rl_model_296000_steps.zip',\n  'rl_model_295000_steps.zip',\n  'rl_model_294000_steps.zip',\n  'rl_model_293000_steps.zip',\n  'rl_model_292000_steps.zip',\n  'rl_model_291000_steps.zip',\n  'rl_model_290000_steps.zip',\n  'rl_model_289000_steps.zip',\n  'rl_model_288000_steps.zip',\n  'rl_model_287000_steps.zip',\n  'rl_model_286000_steps.zip',\n  'rl_model_285000_steps.zip',\n  'rl_model_284000_steps.zip',\n  'rl_model_283000_steps.zip',\n  'rl_model_282000_steps.zip',\n  'rl_model_281000_steps.zip',\n  'rl_model_280000_steps.zip',\n  'rl_model_279000_steps.zip',\n  'rl_model_278000_steps.zip',\n  'rl_model_277000_steps.zip',\n  'rl_model_276000_steps.zip',\n  'rl_model_275000_steps.zip',\n  'rl_model_274000_steps.zip',\n  'rl_model_273000_steps.zip',\n  'rl_model_272000_steps.zip',\n  'rl_model_271000_steps.zip',\n  'rl_model_270000_steps.zip',\n  'rl_model_269000_steps.zip',\n  'rl_model_268000_steps.zip',\n  'rl_model_267000_steps.zip',\n  'rl_model_266000_steps.zip',\n  'rl_model_265000_steps.zip',\n  'rl_model_264000_steps.zip',\n  'rl_model_263000_steps.zip',\n  'rl_model_262000_steps.zip',\n  'rl_model_261000_steps.zip',\n  'rl_model_260000_steps.zip',\n  'rl_model_259000_steps.zip',\n  'rl_model_258000_steps.zip',\n  'rl_model_257000_steps.zip',\n  'rl_model_256000_steps.zip',\n  'rl_model_255000_steps.zip',\n  'rl_model_254000_steps.zip',\n  'rl_model_253000_steps.zip',\n  'rl_model_252000_steps.zip',\n  'rl_model_251000_steps.zip',\n  'rl_model_250000_steps.zip',\n  'rl_model_249000_steps.zip',\n  'rl_model_248000_steps.zip',\n  'rl_model_247000_steps.zip',\n  'rl_model_246000_steps.zip',\n  'rl_model_245000_steps.zip',\n  'rl_model_244000_steps.zip',\n  'rl_model_243000_steps.zip',\n  'rl_model_242000_steps.zip',\n  'rl_model_241000_steps.zip',\n  'rl_model_240000_steps.zip',\n  'rl_model_239000_steps.zip',\n  'rl_model_238000_steps.zip',\n  'rl_model_237000_steps.zip',\n  'rl_model_236000_steps.zip',\n  'rl_model_235000_steps.zip',\n  'rl_model_234000_steps.zip',\n  'rl_model_233000_steps.zip',\n  'rl_model_232000_steps.zip',\n  'rl_model_231000_steps.zip',\n  'rl_model_230000_steps.zip',\n  'rl_model_229000_steps.zip',\n  'rl_model_228000_steps.zip',\n  'rl_model_227000_steps.zip',\n  'rl_model_226000_steps.zip',\n  'rl_model_225000_steps.zip',\n  'rl_model_224000_steps.zip',\n  'rl_model_223000_steps.zip',\n  'rl_model_222000_steps.zip',\n  'rl_model_221000_steps.zip',\n  'rl_model_220000_steps.zip',\n  'rl_model_219000_steps.zip',\n  'rl_model_218000_steps.zip',\n  'rl_model_217000_steps.zip',\n  'rl_model_216000_steps.zip',\n  'rl_model_215000_steps.zip',\n  'rl_model_214000_steps.zip',\n  'rl_model_213000_steps.zip',\n  'rl_model_212000_steps.zip',\n  'rl_model_211000_steps.zip',\n  'rl_model_210000_steps.zip',\n  'rl_model_209000_steps.zip',\n  'rl_model_208000_steps.zip',\n  'rl_model_207000_steps.zip',\n  'rl_model_206000_steps.zip',\n  'rl_model_205000_steps.zip',\n  'rl_model_204000_steps.zip',\n  'rl_model_203000_steps.zip',\n  'rl_model_202000_steps.zip',\n  'rl_model_201000_steps.zip',\n  'rl_model_200000_steps.zip',\n  'rl_model_199000_steps.zip',\n  'rl_model_198000_steps.zip',\n  'rl_model_197000_steps.zip',\n  'rl_model_196000_steps.zip',\n  'rl_model_195000_steps.zip',\n  'rl_model_194000_steps.zip',\n  'rl_model_193000_steps.zip',\n  'rl_model_192000_steps.zip',\n  'rl_model_191000_steps.zip',\n  'rl_model_190000_steps.zip',\n  'rl_model_189000_steps.zip',\n  'rl_model_188000_steps.zip',\n  'rl_model_187000_steps.zip',\n  'rl_model_186000_steps.zip',\n  'rl_model_185000_steps.zip',\n  'rl_model_184000_steps.zip',\n  'rl_model_183000_steps.zip',\n  'rl_model_182000_steps.zip',\n  'rl_model_181000_steps.zip',\n  'rl_model_180000_steps.zip',\n  'rl_model_179000_steps.zip',\n  'rl_model_178000_steps.zip',\n  'rl_model_177000_steps.zip',\n  'rl_model_176000_steps.zip',\n  'rl_model_175000_steps.zip',\n  'rl_model_174000_steps.zip',\n  'rl_model_173000_steps.zip',\n  'rl_model_172000_steps.zip',\n  'rl_model_171000_steps.zip',\n  'rl_model_170000_steps.zip',\n  'rl_model_169000_steps.zip',\n  'rl_model_168000_steps.zip',\n  'rl_model_167000_steps.zip',\n  'rl_model_166000_steps.zip',\n  'rl_model_165000_steps.zip',\n  'rl_model_164000_steps.zip',\n  'rl_model_163000_steps.zip',\n  'rl_model_162000_steps.zip',\n  'rl_model_161000_steps.zip',\n  'rl_model_160000_steps.zip',\n  'rl_model_159000_steps.zip',\n  'rl_model_158000_steps.zip',\n  'rl_model_157000_steps.zip',\n  'rl_model_156000_steps.zip',\n  'rl_model_155000_steps.zip',\n  'rl_model_154000_steps.zip',\n  'rl_model_153000_steps.zip',\n  'rl_model_152000_steps.zip',\n  'rl_model_151000_steps.zip',\n  'rl_model_150000_steps.zip',\n  'rl_model_149000_steps.zip',\n  'rl_model_148000_steps.zip',\n  'rl_model_147000_steps.zip',\n  'rl_model_146000_steps.zip',\n  'rl_model_145000_steps.zip',\n  'rl_model_144000_steps.zip',\n  'rl_model_143000_steps.zip',\n  'rl_model_142000_steps.zip',\n  'rl_model_141000_steps.zip',\n  'rl_model_140000_steps.zip',\n  'rl_model_139000_steps.zip',\n  'rl_model_138000_steps.zip',\n  'rl_model_137000_steps.zip',\n  'rl_model_136000_steps.zip',\n  'rl_model_135000_steps.zip',\n  'rl_model_134000_steps.zip',\n  'rl_model_133000_steps.zip',\n  'rl_model_132000_steps.zip',\n  'rl_model_131000_steps.zip',\n  'rl_model_130000_steps.zip',\n  'rl_model_129000_steps.zip',\n  'rl_model_128000_steps.zip',\n  'rl_model_127000_steps.zip',\n  'rl_model_126000_steps.zip',\n  'rl_model_125000_steps.zip',\n  'rl_model_124000_steps.zip',\n  'rl_model_123000_steps.zip',\n  'rl_model_122000_steps.zip',\n  'rl_model_121000_steps.zip',\n  'rl_model_120000_steps.zip',\n  'rl_model_119000_steps.zip',\n  'rl_model_118000_steps.zip',\n  'rl_model_117000_steps.zip',\n  'rl_model_116000_steps.zip',\n  'rl_model_115000_steps.zip',\n  'rl_model_114000_steps.zip',\n  'rl_model_113000_steps.zip',\n  'rl_model_112000_steps.zip',\n  'rl_model_111000_steps.zip',\n  'rl_model_110000_steps.zip',\n  'rl_model_109000_steps.zip',\n  'rl_model_108000_steps.zip',\n  'rl_model_107000_steps.zip',\n  'rl_model_106000_steps.zip',\n  'rl_model_105000_steps.zip',\n  'rl_model_104000_steps.zip',\n  'rl_model_103000_steps.zip',\n  'rl_model_102000_steps.zip',\n  'rl_model_101000_steps.zip',\n  'rl_model_100000_steps.zip',\n  'rl_model_99000_steps.zip',\n  'rl_model_98000_steps.zip',\n  'rl_model_97000_steps.zip',\n  'rl_model_96000_steps.zip',\n  'rl_model_95000_steps.zip',\n  'rl_model_94000_steps.zip',\n  'rl_model_93000_steps.zip',\n  'rl_model_92000_steps.zip',\n  'rl_model_91000_steps.zip',\n  'rl_model_90000_steps.zip',\n  'rl_model_89000_steps.zip',\n  'rl_model_88000_steps.zip',\n  'rl_model_87000_steps.zip',\n  'rl_model_86000_steps.zip',\n  'rl_model_85000_steps.zip',\n  'rl_model_84000_steps.zip',\n  'rl_model_83000_steps.zip',\n  'rl_model_82000_steps.zip',\n  'rl_model_81000_steps.zip',\n  'rl_model_80000_steps.zip',\n  'rl_model_79000_steps.zip',\n  'rl_model_78000_steps.zip',\n  'rl_model_77000_steps.zip',\n  'rl_model_76000_steps.zip',\n  'rl_model_75000_steps.zip',\n  'rl_model_74000_steps.zip',\n  'rl_model_73000_steps.zip',\n  'rl_model_72000_steps.zip',\n  'rl_model_71000_steps.zip',\n  'rl_model_70000_steps.zip',\n  'rl_model_69000_steps.zip',\n  'rl_model_68000_steps.zip',\n  'rl_model_67000_steps.zip',\n  'rl_model_66000_steps.zip',\n  'rl_model_65000_steps.zip',\n  'rl_model_64000_steps.zip',\n  'rl_model_63000_steps.zip',\n  'rl_model_62000_steps.zip',\n  'rl_model_61000_steps.zip',\n  'rl_model_60000_steps.zip',\n  'rl_model_59000_steps.zip',\n  'rl_model_58000_steps.zip',\n  'rl_model_57000_steps.zip',\n  'rl_model_56000_steps.zip',\n  'rl_model_55000_steps.zip',\n  'rl_model_54000_steps.zip',\n  'rl_model_53000_steps.zip',\n  'rl_model_52000_steps.zip',\n  'rl_model_51000_steps.zip',\n  'rl_model_50000_steps.zip',\n  'rl_model_49000_steps.zip',\n  'rl_model_48000_steps.zip',\n  'rl_model_47000_steps.zip',\n  'rl_model_46000_steps.zip',\n  'rl_model_45000_steps.zip',\n  'rl_model_44000_steps.zip',\n  'rl_model_43000_steps.zip',\n  'rl_model_42000_steps.zip',\n  'rl_model_41000_steps.zip',\n  'rl_model_40000_steps.zip',\n  'rl_model_39000_steps.zip',\n  'rl_model_38000_steps.zip',\n  'rl_model_37000_steps.zip',\n  'rl_model_36000_steps.zip',\n  'rl_model_35000_steps.zip',\n  'rl_model_34000_steps.zip',\n  'rl_model_33000_steps.zip',\n  'rl_model_32000_steps.zip',\n  'rl_model_31000_steps.zip',\n  'rl_model_30000_steps.zip',\n  'rl_model_29000_steps.zip',\n  'rl_model_28000_steps.zip',\n  'rl_model_27000_steps.zip',\n  'rl_model_26000_steps.zip',\n  'rl_model_25000_steps.zip',\n  'rl_model_24000_steps.zip',\n  'rl_model_23000_steps.zip',\n  'rl_model_22000_steps.zip',\n  'rl_model_21000_steps.zip',\n  'rl_model_20000_steps.zip',\n  'rl_model_19000_steps.zip',\n  'rl_model_18000_steps.zip',\n  'rl_model_17000_steps.zip',\n  'rl_model_16000_steps.zip',\n  'rl_model_15000_steps.zip',\n  'rl_model_14000_steps.zip',\n  'rl_model_13000_steps.zip',\n  'rl_model_12000_steps.zip',\n  'rl_model_11000_steps.zip',\n  'rl_model_10000_steps.zip',\n  'rl_model_9000_steps.zip',\n  'rl_model_8000_steps.zip',\n  'rl_model_7000_steps.zip',\n  'rl_model_6000_steps.zip',\n  'rl_model_5000_steps.zip',\n  'rl_model_4000_steps.zip',\n  'rl_model_3000_steps.zip',\n  'rl_model_2000_steps.zip',\n  'rl_model_1000_steps.zip'])"},"metadata":{}}]}]}